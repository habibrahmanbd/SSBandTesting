[{"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "bb6e28592e1bfeaf1e9648d51a67b469af1472b1", "fixCommitParentSHA1": "1619d8d62a7fb925dfe40d6bb06ed2c145e285ee", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java\nindex b7f264b..fb2aa0e 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java\n@@ -368,9 +368,8 @@\n           public void run() {\n             try {\n               onTimerEvent();\n-            }\n-            catch (Exception e) {\n-              LOG.warn(e);\n+            } catch (Exception e) {\n+              LOG.warn(\"Error invoking metrics timer\", e);\n             }\n           }\n         }, millis, millis);\n", "projectName": "apache.hadoop", "bugLineNum": 373, "bugNodeStartChar": 13080, "bugNodeLength": 11, "fixLineNum": 372, "fixNodeStartChar": 13068, "fixNodeLength": 43, "sourceBeforeFix": "LOG.warn(e)", "sourceAfterFix": "LOG.warn(\"Error invoking metrics timer\",e)"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "3b9698ecac75326d0cd88929189eaed782b9b8b2", "fixCommitParentSHA1": "80697e4f324948ec32b4cad3faccba55287be652", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java\nindex e06ec1c..a777d7e 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java\n@@ -278,7 +278,7 @@\n       + \"intermediate-data-encryption.enable\";\n \n   @Private\n-  public static final Boolean DEFAULT_YARN_INTERMEDIATE_DATA_ENCRYPTION = false;\n+  public static final boolean DEFAULT_YARN_INTERMEDIATE_DATA_ENCRYPTION = false;\n \n   /** The address of the RM admin interface.*/\n   public static final String RM_ADMIN_ADDRESS = \n@@ -729,7 +729,7 @@\n \n   public static final String RM_PROXY_USER_PRIVILEGES_ENABLED = RM_PREFIX\n       + \"proxy-user-privileges.enabled\";\n-  public static boolean DEFAULT_RM_PROXY_USER_PRIVILEGES_ENABLED = false;\n+  public static final boolean DEFAULT_RM_PROXY_USER_PRIVILEGES_ENABLED = false;\n \n   /**\n    * How many diagnostics/failure messages can be saved in RM for\n", "projectName": "apache.hadoop", "bugLineNum": 732, "bugNodeStartChar": 30597, "bugNodeLength": 71, "fixLineNum": 732, "fixNodeStartChar": 30597, "fixNodeLength": 77, "sourceBeforeFix": "9", "sourceAfterFix": "25"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "cc671349bc1c56d597aedd244d939dc248353d8c", "fixCommitParentSHA1": "b9865cbf3d435903835afa23c74c5e2ecfbba1bc", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java\nindex dd2ab25..81be813 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java\n@@ -359,7 +359,7 @@\n         finalReport.setApplicationId(appId);\n         finalReport.setLogAggregationStatus(renameTemporaryLogFileFailed\n             ? LogAggregationStatus.FAILED : LogAggregationStatus.SUCCEEDED);\n-        this.context.getLogAggregationStatusForApps().add(report);\n+        this.context.getLogAggregationStatusForApps().add(finalReport);\n       }\n     } finally {\n       if (writer != null) {\n", "projectName": "apache.hadoop", "bugLineNum": 362, "bugNodeStartChar": 15923, "bugNodeLength": 57, "fixLineNum": 362, "fixNodeStartChar": 15923, "fixNodeLength": 62, "sourceBeforeFix": "this.context.getLogAggregationStatusForApps().add(report)", "sourceAfterFix": "this.context.getLogAggregationStatusForApps().add(finalReport)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "453488cff2a54f32c573589f1cfc47bbf30ebe09", "fixCommitParentSHA1": "015b30c3ab0231f316f318417c69c869fc10f9f0", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java\nindex 130cfd4..c209873 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java\n@@ -200,7 +200,8 @@\n         this.configurationProvider.getConfigurationInputStream(this.conf,\n             YarnConfiguration.CORE_SITE_CONFIGURATION_FILE);\n     if (coreSiteXMLInputStream != null) {\n-      this.conf.addResource(coreSiteXMLInputStream);\n+      this.conf.addResource(coreSiteXMLInputStream,\n+          YarnConfiguration.CORE_SITE_CONFIGURATION_FILE);\n     }\n \n     // Do refreshUserToGroupsMappings with loaded core-site.xml\n@@ -218,7 +219,8 @@\n         this.configurationProvider.getConfigurationInputStream(this.conf,\n             YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n     if (yarnSiteXMLInputStream != null) {\n-      this.conf.addResource(yarnSiteXMLInputStream);\n+      this.conf.addResource(yarnSiteXMLInputStream,\n+          YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n     }\n \n     validateConfigs(this.conf);\n", "projectName": "apache.hadoop", "bugLineNum": 203, "bugNodeStartChar": 9350, "bugNodeLength": 45, "fixLineNum": 203, "fixNodeStartChar": 9350, "fixNodeLength": 103, "sourceBeforeFix": "this.conf.addResource(coreSiteXMLInputStream)", "sourceAfterFix": "this.conf.addResource(coreSiteXMLInputStream,YarnConfiguration.CORE_SITE_CONFIGURATION_FILE)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "453488cff2a54f32c573589f1cfc47bbf30ebe09", "fixCommitParentSHA1": "015b30c3ab0231f316f318417c69c869fc10f9f0", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java\nindex 130cfd4..c209873 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java\n@@ -200,7 +200,8 @@\n         this.configurationProvider.getConfigurationInputStream(this.conf,\n             YarnConfiguration.CORE_SITE_CONFIGURATION_FILE);\n     if (coreSiteXMLInputStream != null) {\n-      this.conf.addResource(coreSiteXMLInputStream);\n+      this.conf.addResource(coreSiteXMLInputStream,\n+          YarnConfiguration.CORE_SITE_CONFIGURATION_FILE);\n     }\n \n     // Do refreshUserToGroupsMappings with loaded core-site.xml\n@@ -218,7 +219,8 @@\n         this.configurationProvider.getConfigurationInputStream(this.conf,\n             YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n     if (yarnSiteXMLInputStream != null) {\n-      this.conf.addResource(yarnSiteXMLInputStream);\n+      this.conf.addResource(yarnSiteXMLInputStream,\n+          YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n     }\n \n     validateConfigs(this.conf);\n", "projectName": "apache.hadoop", "bugLineNum": 221, "bugNodeStartChar": 10096, "bugNodeLength": 45, "fixLineNum": 221, "fixNodeStartChar": 10096, "fixNodeLength": 103, "sourceBeforeFix": "this.conf.addResource(yarnSiteXMLInputStream)", "sourceAfterFix": "this.conf.addResource(yarnSiteXMLInputStream,YarnConfiguration.YARN_SITE_CONFIGURATION_FILE)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "dce2381dc4a877fcbfb869f115152ecd44a92173", "fixCommitParentSHA1": "d0fff5d3282a6a823a29b2ed651adbe6cec3caff", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/RMNodeLabelsManager.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/RMNodeLabelsManager.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/RMNodeLabelsManager.java\nindex 25e5bc09..696b99b 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/RMNodeLabelsManager.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/RMNodeLabelsManager.java\n@@ -46,11 +46,11 @@\n \n public class RMNodeLabelsManager extends CommonNodeLabelsManager {\n   protected static class Queue {\n-    protected Set<String> acccessibleNodeLabels;\n+    protected Set<String> accessibleNodeLabels;\n     protected Resource resource;\n \n     protected Queue() {\n-      acccessibleNodeLabels =\n+      accessibleNodeLabels =\n           Collections.newSetFromMap(new ConcurrentHashMap<String, Boolean>());\n       resource = Resource.newInstance(0, 0);\n     }\n@@ -98,7 +98,7 @@\n       // check if any queue contains this label\n       for (Entry<String, Queue> entry : queueCollections.entrySet()) {\n         String queueName = entry.getKey();\n-        Set<String> queueLabels = entry.getValue().acccessibleNodeLabels;\n+        Set<String> queueLabels = entry.getValue().accessibleNodeLabels;\n         if (queueLabels.contains(label)) {\n           throw new IOException(\"Cannot remove label=\" + label\n               + \", because queue=\" + queueName + \" is using this label. \"\n@@ -275,7 +275,7 @@\n           continue;\n         }\n \n-        q.acccessibleNodeLabels.addAll(labels);\n+        q.accessibleNodeLabels.addAll(labels);\n         for (Host host : nodeCollections.values()) {\n           for (Entry<NodeId, Node> nentry : host.nms.entrySet()) {\n             NodeId nodeId = nentry.getKey();\n@@ -468,7 +468,7 @@\n     }\n \n     for (String label : nodeLabels) {\n-      if (q.acccessibleNodeLabels.contains(label)) {\n+      if (q.accessibleNodeLabels.contains(label)) {\n         return true;\n       }\n     }\n", "projectName": "apache.hadoop", "bugLineNum": 101, "bugNodeStartChar": 3735, "bugNodeLength": 38, "fixLineNum": 101, "fixNodeStartChar": 3735, "fixNodeLength": 37, "sourceBeforeFix": "entry.getValue().acccessibleNodeLabels", "sourceAfterFix": "entry.getValue().accessibleNodeLabels"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "97244b143ca72cb9dcd86b428f9408928985b545", "fixCommitParentSHA1": "ac32fa187cf37e5a51fd579e052105662ab3c411", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestLog4jWarningErrorMetricsAppender.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestLog4jWarningErrorMetricsAppender.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestLog4jWarningErrorMetricsAppender.java\nindex 61d4c4c..e788e80 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestLog4jWarningErrorMetricsAppender.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestLog4jWarningErrorMetricsAppender.java\n@@ -84,7 +84,7 @@\n     Assert.assertEquals(1, appender.getErrorCounts(cutoff).get(0).longValue());\n     Assert.assertEquals(1, appender.getErrorMessagesAndCounts(cutoff).get(0)\n       .size());\n-    Thread.sleep(2000);\n+    Thread.sleep(3000);\n     Assert.assertEquals(1, appender.getErrorCounts(cutoff).size());\n     Assert.assertEquals(0, appender.getErrorCounts(cutoff).get(0).longValue());\n     Assert.assertEquals(0, appender.getErrorMessagesAndCounts(cutoff).get(0)\n", "projectName": "apache.hadoop", "bugLineNum": 87, "bugNodeStartChar": 2785, "bugNodeLength": 18, "fixLineNum": 87, "fixNodeStartChar": 2785, "fixNodeLength": 18, "sourceBeforeFix": "Thread.sleep(2000)", "sourceAfterFix": "Thread.sleep(3000)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "9cbe9bcbaa33825a866bdc8a9bf92f270723d423", "fixCommitParentSHA1": "187e081d5a8afe1ddfe5d7b5e7de7a94512aa53e", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java\nindex 8eb00f4..787422b 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java\n@@ -248,7 +248,7 @@\n \n       } catch (IOException e1) {\n         LOG.error(\"Cannot create writer for app \" + this.applicationId\n-            + \". Skip log upload this time. \");\n+            + \". Skip log upload this time. \", e1);\n         return;\n       }\n \n@@ -549,7 +549,7 @@\n         writer.append(logKey, logValue);\n       } catch (Exception e) {\n         LOG.error(\"Couldn't upload logs for \" + containerId\n-            + \". Skipping this container.\");\n+            + \". Skipping this container.\", e);\n         return new HashSet<Path>();\n       }\n       this.uploadedFileMeta.addAll(logValue\n", "projectName": "apache.hadoop", "bugLineNum": 250, "bugNodeStartChar": 11007, "bugNodeLength": 109, "fixLineNum": 250, "fixNodeStartChar": 11007, "fixNodeLength": 113, "sourceBeforeFix": "LOG.error(\"Cannot create writer for app \" + this.applicationId + \". Skip log upload this time. \")", "sourceAfterFix": "LOG.error(\"Cannot create writer for app \" + this.applicationId + \". Skip log upload this time. \",e1)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "9cbe9bcbaa33825a866bdc8a9bf92f270723d423", "fixCommitParentSHA1": "187e081d5a8afe1ddfe5d7b5e7de7a94512aa53e", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java\nindex 8eb00f4..787422b 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java\n@@ -248,7 +248,7 @@\n \n       } catch (IOException e1) {\n         LOG.error(\"Cannot create writer for app \" + this.applicationId\n-            + \". Skip log upload this time. \");\n+            + \". Skip log upload this time. \", e1);\n         return;\n       }\n \n@@ -549,7 +549,7 @@\n         writer.append(logKey, logValue);\n       } catch (Exception e) {\n         LOG.error(\"Couldn't upload logs for \" + containerId\n-            + \". Skipping this container.\");\n+            + \". Skipping this container.\", e);\n         return new HashSet<Path>();\n       }\n       this.uploadedFileMeta.addAll(logValue\n", "projectName": "apache.hadoop", "bugLineNum": 551, "bugNodeStartChar": 21178, "bugNodeLength": 95, "fixLineNum": 551, "fixNodeStartChar": 21178, "fixNodeLength": 98, "sourceBeforeFix": "LOG.error(\"Couldn't upload logs for \" + containerId + \". Skipping this container.\")", "sourceAfterFix": "LOG.error(\"Couldn't upload logs for \" + containerId + \". Skipping this container.\",e)"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "f59b698fc99211c010d569e7f71555183dfc29f1", "fixCommitParentSHA1": "ad4b243586e9034da725f8695efdcf5f1b0ce6b0", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java\nindex 4507e35..b7660e5 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java\n@@ -387,7 +387,7 @@\n    * Requests an immediate publish of all metrics from sources to sinks.\n    */\n   @Override\n-  public void publishMetricsNow() {\n+  public synchronized void publishMetricsNow() {\n     if (sinks.size() > 0) {\n       publishMetrics(sampleMetrics(), true);\n     }    \n", "projectName": "apache.hadoop", "bugLineNum": 386, "bugNodeStartChar": 13332, "bugNodeLength": 217, "fixLineNum": 386, "fixNodeStartChar": 13332, "fixNodeLength": 230, "sourceBeforeFix": "1", "sourceAfterFix": "33"}, {"bugType": "CHANGE_CALLER_IN_FUNCTION_CALL", "fixCommitSHA1": "f59b698fc99211c010d569e7f71555183dfc29f1", "fixCommitParentSHA1": "ad4b243586e9034da725f8695efdcf5f1b0ce6b0", "bugFilePath": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSystemImpl.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSystemImpl.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSystemImpl.java\nindex d59e80b..4c2ebc8 100644\n--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSystemImpl.java\n+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSystemImpl.java\n@@ -190,7 +190,7 @@\n       threads[i] = new Thread(new Runnable() {\n         private boolean safeAwait(int mySource, CyclicBarrier barrier) {\n           try {\n-            barrier1.await(2, TimeUnit.SECONDS);\n+            barrier.await(2, TimeUnit.SECONDS);\n           } catch (InterruptedException e) {\n             results[mySource] = \"Interrupted\";\n             return false;\n", "projectName": "apache.hadoop", "bugLineNum": 193, "bugNodeStartChar": 7510, "bugNodeLength": 35, "fixLineNum": 193, "fixNodeStartChar": 7510, "fixNodeLength": 34, "sourceBeforeFix": "barrier1.await(2,TimeUnit.SECONDS)", "sourceAfterFix": "barrier.await(2,TimeUnit.SECONDS)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "f59b698fc99211c010d569e7f71555183dfc29f1", "fixCommitParentSHA1": "ad4b243586e9034da725f8695efdcf5f1b0ce6b0", "bugFilePath": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSystemImpl.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSystemImpl.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSystemImpl.java\nindex d59e80b..4c2ebc8 100644\n--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSystemImpl.java\n+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/impl/TestMetricsSystemImpl.java\n@@ -190,7 +190,7 @@\n       threads[i] = new Thread(new Runnable() {\n         private boolean safeAwait(int mySource, CyclicBarrier barrier) {\n           try {\n-            barrier1.await(2, TimeUnit.SECONDS);\n+            barrier.await(2, TimeUnit.SECONDS);\n           } catch (InterruptedException e) {\n             results[mySource] = \"Interrupted\";\n             return false;\n", "projectName": "apache.hadoop", "bugLineNum": 193, "bugNodeStartChar": 7510, "bugNodeLength": 35, "fixLineNum": 193, "fixNodeStartChar": 7510, "fixNodeLength": 34, "sourceBeforeFix": "barrier1.await(2,TimeUnit.SECONDS)", "sourceAfterFix": "barrier.await(2,TimeUnit.SECONDS)"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "d3b26454e453953d2eb221b62d1ff3632da9e8a6", "fixCommitParentSHA1": "adf49e4b9440fec7267817476cac83d6a5dfb98f", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java\nindex bf720ae..ad2b5d4 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java\n@@ -189,8 +189,8 @@\n     }\n   }\n \n-  protected void containerLaunchedOnNode(ContainerId containerId,\n-                                         SchedulerNode node) {\n+  protected synchronized void containerLaunchedOnNode(\n+      ContainerId containerId, SchedulerNode node) {\n     // Get the application for the finished container\n     SchedulerApplicationAttempt application = getCurrentAttemptForContainer\n         (containerId);\n", "projectName": "apache.hadoop", "bugLineNum": 192, "bugNodeStartChar": 7833, "bugNodeLength": 721, "fixLineNum": 192, "fixNodeStartChar": 7833, "fixNodeLength": 700, "sourceBeforeFix": "4", "sourceAfterFix": "36"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "d3b26454e453953d2eb221b62d1ff3632da9e8a6", "fixCommitParentSHA1": "adf49e4b9440fec7267817476cac83d6a5dfb98f", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java\nindex 5e6a352..e622d3a 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java\n@@ -249,7 +249,8 @@\n   }\n \n   @Override\n-  public RMContainerTokenSecretManager getContainerTokenSecretManager() {\n+  public synchronized RMContainerTokenSecretManager \n+  getContainerTokenSecretManager() {\n     return this.rmContext.getContainerTokenSecretManager();\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 251, "bugNodeStartChar": 11415, "bugNodeLength": 147, "fixLineNum": 251, "fixNodeStartChar": 11415, "fixNodeLength": 163, "sourceBeforeFix": "1", "sourceAfterFix": "33"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "d3b26454e453953d2eb221b62d1ff3632da9e8a6", "fixCommitParentSHA1": "adf49e4b9440fec7267817476cac83d6a5dfb98f", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java\nindex 124da99..2f0857e 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java\n@@ -1232,7 +1232,7 @@\n     }\n   }\n \n-  private String resolveReservationQueueName(String queueName,\n+  private synchronized String resolveReservationQueueName(String queueName,\n       ApplicationId applicationId, ReservationId reservationID) {\n     FSQueue queue = queueMgr.getQueue(queueName);\n     if ((queue == null) || !allocConf.isReservable(queue.getQueueName())) {\n", "projectName": "apache.hadoop", "bugLineNum": 1235, "bugNodeStartChar": 48903, "bugNodeLength": 1591, "fixLineNum": 1235, "fixNodeStartChar": 48903, "fixNodeLength": 1604, "sourceBeforeFix": "2", "sourceAfterFix": "34"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "31753da961740937fe2f6d86432c2d57827bf975", "fixCommitParentSHA1": "0a5b28605ffbadb0178467b2d16c91684ddef6bd", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestNMClient.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestNMClient.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestNMClient.java\nindex 88dbf81..0d4a271 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestNMClient.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestNMClient.java\n@@ -344,10 +344,11 @@\n         // getContainerStatus can be called after stopContainer\n         try {\n           // O is possible if CLEANUP_CONTAINER is executed too late\n-          // 137 is possible if the container is not terminated but killed\n+          // -105 is possible if the container is not terminated but killed\n           testGetContainerStatus(container, i, ContainerState.COMPLETE,\n               \"Container killed by the ApplicationMaster.\", Arrays.asList(\n-                  new Integer[] {ContainerExitStatus.KILLED_BY_APPMASTER}));\n+                  new Integer[] {ContainerExitStatus.KILLED_BY_APPMASTER,\n+                  ContainerExitStatus.SUCCESS}));\n         } catch (YarnException e) {\n           // The exception is possible because, after the container is stopped,\n           // it may be removed from NM's context.\n@@ -383,7 +384,10 @@\n           assertEquals(container.getId(), status.getContainerId());\n           assertTrue(\"\" + index + \": \" + status.getDiagnostics(),\n               status.getDiagnostics().contains(diagnostics));\n-          assertTrue(exitStatuses.contains(status.getExitStatus()));\n+          \n+          assertTrue(\"Exit Statuses are supposed to be in: \" + exitStatuses +\n+              \", but the actual exit status code is: \" + status.getExitStatus(),\n+              exitStatuses.contains(status.getExitStatus()));\n           break;\n         }\n         Thread.sleep(100);\n", "projectName": "apache.hadoop", "bugLineNum": 386, "bugNodeStartChar": 14985, "bugNodeLength": 57, "fixLineNum": 387, "fixNodeStartChar": 14996, "fixNodeLength": 209, "sourceBeforeFix": "assertTrue(exitStatuses.contains(status.getExitStatus()))", "sourceAfterFix": "assertTrue(\"Exit Statuses are supposed to be in: \" + exitStatuses + \", but the actual exit status code is: \"+ status.getExitStatus(),exitStatuses.contains(status.getExitStatus()))"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "22f72c79462c424e47610470fd48e871887ac326", "fixCommitParentSHA1": "fe38ed2ee7bd3be5c502d217f13d848c4ca0d9c3", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java\nindex b506710..c02603d 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java\n@@ -213,7 +213,7 @@\n \n   @Override\n   public String toString() {\n-    return volumes.toString();\n+    return Arrays.toString(volumes.get());\n   }\n \n   /**\n", "projectName": "apache.hadoop", "bugLineNum": 216, "bugNodeStartChar": 7136, "bugNodeLength": 18, "fixLineNum": 216, "fixNodeStartChar": 7136, "fixNodeLength": 30, "sourceBeforeFix": "volumes.toString()", "sourceAfterFix": "Arrays.toString(volumes.get())"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "8018d1af58330b6d21c858fa5ebfee6a66cd10b3", "fixCommitParentSHA1": "915176c4e769f510f835f1ad182bc62586a01e0b", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 3875125..0aa3c85 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -298,7 +298,7 @@\n         }\n       } catch (IOException e) {\n         out.println(\"Cannot list keys for KeyProvider: \" + provider\n-            + \": \" + e.getMessage());\n+            + \": \" + e.toString());\n         throw e;\n       }\n     }\n@@ -350,12 +350,12 @@\n           printProviderWritten();\n         } catch (NoSuchAlgorithmException e) {\n           out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-              + provider);\n+              + provider + \". \" + e.toString());\n           throw e;\n         }\n       } catch (IOException e1) {\n         out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-            + provider);\n+            + provider + \". \" + e1.toString());\n         throw e1;\n       }\n     }\n@@ -422,7 +422,7 @@\n           out.println(keyName + \" has been successfully deleted.\");\n           printProviderWritten();\n         } catch (IOException e) {\n-          out.println(keyName + \" has not been deleted.\");\n+          out.println(keyName + \" has not been deleted. \" + e.toString());\n           throw e;\n         }\n       }\n@@ -484,13 +484,13 @@\n             + options.toString() + \".\");\n         printProviderWritten();\n       } catch (InvalidParameterException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (IOException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (NoSuchAlgorithmException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       }\n     }\n", "projectName": "apache.hadoop", "bugLineNum": 301, "bugNodeStartChar": 10106, "bugNodeLength": 14, "fixLineNum": 301, "fixNodeStartChar": 10106, "fixNodeLength": 12, "sourceBeforeFix": "e.getMessage()", "sourceAfterFix": "e.toString()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "8018d1af58330b6d21c858fa5ebfee6a66cd10b3", "fixCommitParentSHA1": "915176c4e769f510f835f1ad182bc62586a01e0b", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 3875125..0aa3c85 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -298,7 +298,7 @@\n         }\n       } catch (IOException e) {\n         out.println(\"Cannot list keys for KeyProvider: \" + provider\n-            + \": \" + e.getMessage());\n+            + \": \" + e.toString());\n         throw e;\n       }\n     }\n@@ -350,12 +350,12 @@\n           printProviderWritten();\n         } catch (NoSuchAlgorithmException e) {\n           out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-              + provider);\n+              + provider + \". \" + e.toString());\n           throw e;\n         }\n       } catch (IOException e1) {\n         out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-            + provider);\n+            + provider + \". \" + e1.toString());\n         throw e1;\n       }\n     }\n@@ -422,7 +422,7 @@\n           out.println(keyName + \" has been successfully deleted.\");\n           printProviderWritten();\n         } catch (IOException e) {\n-          out.println(keyName + \" has not been deleted.\");\n+          out.println(keyName + \" has not been deleted. \" + e.toString());\n           throw e;\n         }\n       }\n@@ -484,13 +484,13 @@\n             + options.toString() + \".\");\n         printProviderWritten();\n       } catch (InvalidParameterException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (IOException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (NoSuchAlgorithmException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       }\n     }\n", "projectName": "apache.hadoop", "bugLineNum": 301, "bugNodeStartChar": 10106, "bugNodeLength": 14, "fixLineNum": 301, "fixNodeStartChar": 10106, "fixNodeLength": 12, "sourceBeforeFix": "e.getMessage()", "sourceAfterFix": "e.toString()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "8018d1af58330b6d21c858fa5ebfee6a66cd10b3", "fixCommitParentSHA1": "915176c4e769f510f835f1ad182bc62586a01e0b", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 3875125..0aa3c85 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -298,7 +298,7 @@\n         }\n       } catch (IOException e) {\n         out.println(\"Cannot list keys for KeyProvider: \" + provider\n-            + \": \" + e.getMessage());\n+            + \": \" + e.toString());\n         throw e;\n       }\n     }\n@@ -350,12 +350,12 @@\n           printProviderWritten();\n         } catch (NoSuchAlgorithmException e) {\n           out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-              + provider);\n+              + provider + \". \" + e.toString());\n           throw e;\n         }\n       } catch (IOException e1) {\n         out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-            + provider);\n+            + provider + \". \" + e1.toString());\n         throw e1;\n       }\n     }\n@@ -422,7 +422,7 @@\n           out.println(keyName + \" has been successfully deleted.\");\n           printProviderWritten();\n         } catch (IOException e) {\n-          out.println(keyName + \" has not been deleted.\");\n+          out.println(keyName + \" has not been deleted. \" + e.toString());\n           throw e;\n         }\n       }\n@@ -484,13 +484,13 @@\n             + options.toString() + \".\");\n         printProviderWritten();\n       } catch (InvalidParameterException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (IOException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (NoSuchAlgorithmException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       }\n     }\n", "projectName": "apache.hadoop", "bugLineNum": 487, "bugNodeStartChar": 16276, "bugNodeLength": 14, "fixLineNum": 487, "fixNodeStartChar": 16276, "fixNodeLength": 12, "sourceBeforeFix": "e.getMessage()", "sourceAfterFix": "e.toString()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "8018d1af58330b6d21c858fa5ebfee6a66cd10b3", "fixCommitParentSHA1": "915176c4e769f510f835f1ad182bc62586a01e0b", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 3875125..0aa3c85 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -298,7 +298,7 @@\n         }\n       } catch (IOException e) {\n         out.println(\"Cannot list keys for KeyProvider: \" + provider\n-            + \": \" + e.getMessage());\n+            + \": \" + e.toString());\n         throw e;\n       }\n     }\n@@ -350,12 +350,12 @@\n           printProviderWritten();\n         } catch (NoSuchAlgorithmException e) {\n           out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-              + provider);\n+              + provider + \". \" + e.toString());\n           throw e;\n         }\n       } catch (IOException e1) {\n         out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-            + provider);\n+            + provider + \". \" + e1.toString());\n         throw e1;\n       }\n     }\n@@ -422,7 +422,7 @@\n           out.println(keyName + \" has been successfully deleted.\");\n           printProviderWritten();\n         } catch (IOException e) {\n-          out.println(keyName + \" has not been deleted.\");\n+          out.println(keyName + \" has not been deleted. \" + e.toString());\n           throw e;\n         }\n       }\n@@ -484,13 +484,13 @@\n             + options.toString() + \".\");\n         printProviderWritten();\n       } catch (InvalidParameterException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (IOException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (NoSuchAlgorithmException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       }\n     }\n", "projectName": "apache.hadoop", "bugLineNum": 487, "bugNodeStartChar": 16276, "bugNodeLength": 14, "fixLineNum": 487, "fixNodeStartChar": 16276, "fixNodeLength": 12, "sourceBeforeFix": "e.getMessage()", "sourceAfterFix": "e.toString()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "8018d1af58330b6d21c858fa5ebfee6a66cd10b3", "fixCommitParentSHA1": "915176c4e769f510f835f1ad182bc62586a01e0b", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 3875125..0aa3c85 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -298,7 +298,7 @@\n         }\n       } catch (IOException e) {\n         out.println(\"Cannot list keys for KeyProvider: \" + provider\n-            + \": \" + e.getMessage());\n+            + \": \" + e.toString());\n         throw e;\n       }\n     }\n@@ -350,12 +350,12 @@\n           printProviderWritten();\n         } catch (NoSuchAlgorithmException e) {\n           out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-              + provider);\n+              + provider + \". \" + e.toString());\n           throw e;\n         }\n       } catch (IOException e1) {\n         out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-            + provider);\n+            + provider + \". \" + e1.toString());\n         throw e1;\n       }\n     }\n@@ -422,7 +422,7 @@\n           out.println(keyName + \" has been successfully deleted.\");\n           printProviderWritten();\n         } catch (IOException e) {\n-          out.println(keyName + \" has not been deleted.\");\n+          out.println(keyName + \" has not been deleted. \" + e.toString());\n           throw e;\n         }\n       }\n@@ -484,13 +484,13 @@\n             + options.toString() + \".\");\n         printProviderWritten();\n       } catch (InvalidParameterException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (IOException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (NoSuchAlgorithmException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       }\n     }\n", "projectName": "apache.hadoop", "bugLineNum": 490, "bugNodeStartChar": 16400, "bugNodeLength": 14, "fixLineNum": 490, "fixNodeStartChar": 16400, "fixNodeLength": 12, "sourceBeforeFix": "e.getMessage()", "sourceAfterFix": "e.toString()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "8018d1af58330b6d21c858fa5ebfee6a66cd10b3", "fixCommitParentSHA1": "915176c4e769f510f835f1ad182bc62586a01e0b", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 3875125..0aa3c85 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -298,7 +298,7 @@\n         }\n       } catch (IOException e) {\n         out.println(\"Cannot list keys for KeyProvider: \" + provider\n-            + \": \" + e.getMessage());\n+            + \": \" + e.toString());\n         throw e;\n       }\n     }\n@@ -350,12 +350,12 @@\n           printProviderWritten();\n         } catch (NoSuchAlgorithmException e) {\n           out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-              + provider);\n+              + provider + \". \" + e.toString());\n           throw e;\n         }\n       } catch (IOException e1) {\n         out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-            + provider);\n+            + provider + \". \" + e1.toString());\n         throw e1;\n       }\n     }\n@@ -422,7 +422,7 @@\n           out.println(keyName + \" has been successfully deleted.\");\n           printProviderWritten();\n         } catch (IOException e) {\n-          out.println(keyName + \" has not been deleted.\");\n+          out.println(keyName + \" has not been deleted. \" + e.toString());\n           throw e;\n         }\n       }\n@@ -484,13 +484,13 @@\n             + options.toString() + \".\");\n         printProviderWritten();\n       } catch (InvalidParameterException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (IOException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (NoSuchAlgorithmException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       }\n     }\n", "projectName": "apache.hadoop", "bugLineNum": 490, "bugNodeStartChar": 16400, "bugNodeLength": 14, "fixLineNum": 490, "fixNodeStartChar": 16400, "fixNodeLength": 12, "sourceBeforeFix": "e.getMessage()", "sourceAfterFix": "e.toString()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "8018d1af58330b6d21c858fa5ebfee6a66cd10b3", "fixCommitParentSHA1": "915176c4e769f510f835f1ad182bc62586a01e0b", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 3875125..0aa3c85 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -298,7 +298,7 @@\n         }\n       } catch (IOException e) {\n         out.println(\"Cannot list keys for KeyProvider: \" + provider\n-            + \": \" + e.getMessage());\n+            + \": \" + e.toString());\n         throw e;\n       }\n     }\n@@ -350,12 +350,12 @@\n           printProviderWritten();\n         } catch (NoSuchAlgorithmException e) {\n           out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-              + provider);\n+              + provider + \". \" + e.toString());\n           throw e;\n         }\n       } catch (IOException e1) {\n         out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-            + provider);\n+            + provider + \". \" + e1.toString());\n         throw e1;\n       }\n     }\n@@ -422,7 +422,7 @@\n           out.println(keyName + \" has been successfully deleted.\");\n           printProviderWritten();\n         } catch (IOException e) {\n-          out.println(keyName + \" has not been deleted.\");\n+          out.println(keyName + \" has not been deleted. \" + e.toString());\n           throw e;\n         }\n       }\n@@ -484,13 +484,13 @@\n             + options.toString() + \".\");\n         printProviderWritten();\n       } catch (InvalidParameterException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (IOException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (NoSuchAlgorithmException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       }\n     }\n", "projectName": "apache.hadoop", "bugLineNum": 493, "bugNodeStartChar": 16537, "bugNodeLength": 14, "fixLineNum": 493, "fixNodeStartChar": 16537, "fixNodeLength": 12, "sourceBeforeFix": "e.getMessage()", "sourceAfterFix": "e.toString()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "8018d1af58330b6d21c858fa5ebfee6a66cd10b3", "fixCommitParentSHA1": "915176c4e769f510f835f1ad182bc62586a01e0b", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 3875125..0aa3c85 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -298,7 +298,7 @@\n         }\n       } catch (IOException e) {\n         out.println(\"Cannot list keys for KeyProvider: \" + provider\n-            + \": \" + e.getMessage());\n+            + \": \" + e.toString());\n         throw e;\n       }\n     }\n@@ -350,12 +350,12 @@\n           printProviderWritten();\n         } catch (NoSuchAlgorithmException e) {\n           out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-              + provider);\n+              + provider + \". \" + e.toString());\n           throw e;\n         }\n       } catch (IOException e1) {\n         out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-            + provider);\n+            + provider + \". \" + e1.toString());\n         throw e1;\n       }\n     }\n@@ -422,7 +422,7 @@\n           out.println(keyName + \" has been successfully deleted.\");\n           printProviderWritten();\n         } catch (IOException e) {\n-          out.println(keyName + \" has not been deleted.\");\n+          out.println(keyName + \" has not been deleted. \" + e.toString());\n           throw e;\n         }\n       }\n@@ -484,13 +484,13 @@\n             + options.toString() + \".\");\n         printProviderWritten();\n       } catch (InvalidParameterException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (IOException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       } catch (NoSuchAlgorithmException e) {\n-        out.println(keyName + \" has not been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.toString());\n         throw e;\n       }\n     }\n", "projectName": "apache.hadoop", "bugLineNum": 493, "bugNodeStartChar": 16537, "bugNodeLength": 14, "fixLineNum": 493, "fixNodeStartChar": 16537, "fixNodeLength": 12, "sourceBeforeFix": "e.getMessage()", "sourceAfterFix": "e.toString()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "1f6258f535f114b88d9be038ffbcf23f7621dba3", "fixCommitParentSHA1": "a4bb56b93b9e1cf9a9f9cca787f0a821a1bdea26", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java\nindex a1f766d..9bf95eb 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java\n@@ -161,7 +161,7 @@\n         // DataNode can run out of memory if there is too many transfers.\n         // Log the event, Sleep for 30 seconds, other transfers may complete by\n         // then.\n-        LOG.warn(\"DataNode is out of memory. Will retry in 30 seconds.\", ie);\n+        LOG.error(\"DataNode is out of memory. Will retry in 30 seconds.\", ie);\n         try {\n           Thread.sleep(30 * 1000);\n         } catch (InterruptedException e) {\n", "projectName": "apache.hadoop", "bugLineNum": 164, "bugNodeStartChar": 6072, "bugNodeLength": 68, "fixLineNum": 164, "fixNodeStartChar": 6072, "fixNodeLength": 69, "sourceBeforeFix": "LOG.warn(\"DataNode is out of memory. Will retry in 30 seconds.\",ie)", "sourceAfterFix": "LOG.error(\"DataNode is out of memory. Will retry in 30 seconds.\",ie)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "1f6258f535f114b88d9be038ffbcf23f7621dba3", "fixCommitParentSHA1": "a4bb56b93b9e1cf9a9f9cca787f0a821a1bdea26", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java\nindex a1f766d..9bf95eb 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java\n@@ -161,7 +161,7 @@\n         // DataNode can run out of memory if there is too many transfers.\n         // Log the event, Sleep for 30 seconds, other transfers may complete by\n         // then.\n-        LOG.warn(\"DataNode is out of memory. Will retry in 30 seconds.\", ie);\n+        LOG.error(\"DataNode is out of memory. Will retry in 30 seconds.\", ie);\n         try {\n           Thread.sleep(30 * 1000);\n         } catch (InterruptedException e) {\n", "projectName": "apache.hadoop", "bugLineNum": 164, "bugNodeStartChar": 6072, "bugNodeLength": 68, "fixLineNum": 164, "fixNodeStartChar": 6072, "fixNodeLength": 69, "sourceBeforeFix": "LOG.warn(\"DataNode is out of memory. Will retry in 30 seconds.\",ie)", "sourceAfterFix": "LOG.error(\"DataNode is out of memory. Will retry in 30 seconds.\",ie)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "5f86ce7ea99b1bbb661b683bf2ffcac69d9f17ec", "fixCommitParentSHA1": "14edbc9419bb4cc588d90d218bfe6c2d61a4e957", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataDirs.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataDirs.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataDirs.java\nindex c0b4f9a..94af015 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataDirs.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataDirs.java\n@@ -51,7 +51,7 @@\n     String locations1 = \"[disk]/dir0,[DISK]/dir1,[sSd]/dir2,[disK]/dir3,[ram_disk]/dir4\";\n     conf.set(DFS_DATANODE_DATA_DIR_KEY, locations1);\n     locations = DataNode.getStorageLocations(conf);\n-    assertThat(locations.size(), is(4));\n+    assertThat(locations.size(), is(5));\n     assertThat(locations.get(0).getStorageType(), is(StorageType.DISK));\n     assertThat(locations.get(0).getUri(), is(dir0.toURI()));\n     assertThat(locations.get(1).getStorageType(), is(StorageType.DISK));\n", "projectName": "apache.hadoop", "bugLineNum": 54, "bugNodeStartChar": 2060, "bugNodeLength": 5, "fixLineNum": 54, "fixNodeStartChar": 2060, "fixNodeLength": 5, "sourceBeforeFix": "is(4)", "sourceAfterFix": "is(5)"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 5b83e38..dd31909 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -57,6 +57,16 @@\n \n   private boolean userSuppliedProvider = false;\n \n+  /**\n+   * Primary entry point for the KeyShell; called via main().\n+   *\n+   * @param args Command line arguments.\n+   * @return 0 on success and 1 on failure.  This value is passed back to\n+   * the unix shell, so we must follow shell return code conventions:\n+   * the return code is an unsigned character, and 0 means success, and\n+   * small positive integers mean failure.\n+   * @throws Exception\n+   */\n   @Override\n   public int run(String[] args) throws Exception {\n     int exitCode = 0;\n@@ -68,11 +78,11 @@\n       if (command.validate()) {\n           command.execute();\n       } else {\n-        exitCode = -1;\n+        exitCode = 1;\n       }\n     } catch (Exception e) {\n       e.printStackTrace(err);\n-      return -1;\n+      return 1;\n     }\n     return exitCode;\n   }\n@@ -86,8 +96,8 @@\n    * % hadoop key list [-provider providerPath]\n    * % hadoop key delete keyName [--provider providerPath] [-i]\n    * </pre>\n-   * @param args\n-   * @return\n+   * @param args Command line arguments.\n+   * @return 0 on success, 1 on failure.\n    * @throws IOException\n    */\n   private int init(String[] args) throws IOException {\n@@ -105,7 +115,7 @@\n         command = new CreateCommand(keyName, options);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n         String keyName = \"--help\";\n@@ -116,7 +126,7 @@\n         command = new DeleteCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n         String keyName = \"--help\";\n@@ -127,7 +137,7 @@\n         command = new RollCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n@@ -145,13 +155,13 @@\n           out.println(\"\\nAttributes must be in attribute=value form, \" +\n                   \"or quoted\\nlike \\\"attribute = value\\\"\\n\");\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         if (attributes.containsKey(attr)) {\n           out.println(\"\\nEach attribute must correspond to only one value:\\n\" +\n                   \"atttribute \\\"\" + attr + \"\\\" was repeated\\n\" );\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         attributes.put(attr, val);\n       } else if (\"--provider\".equals(args[i]) && moreTokens) {\n@@ -163,17 +173,17 @@\n         interactive = true;\n       } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n-        return -1;\n+        return 1;\n       } else {\n         printKeyShellUsage();\n         ToolRunner.printGenericCommandUsage(System.err);\n-        return -1;\n+        return 1;\n       }\n     }\n \n     if (command == null) {\n       printKeyShellUsage();\n-      return -1;\n+      return 1;\n     }\n \n     if (!attributes.isEmpty()) {\n@@ -491,10 +501,11 @@\n   }\n \n   /**\n-   * Main program.\n+   * main() entry point for the KeyShell.  While strictly speaking the\n+   * return is void, it will System.exit() with a return code: 0 is for\n+   * success and 1 for failure.\n    *\n-   * @param args\n-   *          Command line arguments\n+   * @param args Command line arguments.\n    * @throws Exception\n    */\n   public static void main(String[] args) throws Exception {\n", "projectName": "apache.hadoop", "bugLineNum": 71, "bugNodeStartChar": 2484, "bugNodeLength": 2, "fixLineNum": 71, "fixNodeStartChar": 2484, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 5b83e38..dd31909 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -57,6 +57,16 @@\n \n   private boolean userSuppliedProvider = false;\n \n+  /**\n+   * Primary entry point for the KeyShell; called via main().\n+   *\n+   * @param args Command line arguments.\n+   * @return 0 on success and 1 on failure.  This value is passed back to\n+   * the unix shell, so we must follow shell return code conventions:\n+   * the return code is an unsigned character, and 0 means success, and\n+   * small positive integers mean failure.\n+   * @throws Exception\n+   */\n   @Override\n   public int run(String[] args) throws Exception {\n     int exitCode = 0;\n@@ -68,11 +78,11 @@\n       if (command.validate()) {\n           command.execute();\n       } else {\n-        exitCode = -1;\n+        exitCode = 1;\n       }\n     } catch (Exception e) {\n       e.printStackTrace(err);\n-      return -1;\n+      return 1;\n     }\n     return exitCode;\n   }\n@@ -86,8 +96,8 @@\n    * % hadoop key list [-provider providerPath]\n    * % hadoop key delete keyName [--provider providerPath] [-i]\n    * </pre>\n-   * @param args\n-   * @return\n+   * @param args Command line arguments.\n+   * @return 0 on success, 1 on failure.\n    * @throws IOException\n    */\n   private int init(String[] args) throws IOException {\n@@ -105,7 +115,7 @@\n         command = new CreateCommand(keyName, options);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n         String keyName = \"--help\";\n@@ -116,7 +126,7 @@\n         command = new DeleteCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n         String keyName = \"--help\";\n@@ -127,7 +137,7 @@\n         command = new RollCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n@@ -145,13 +155,13 @@\n           out.println(\"\\nAttributes must be in attribute=value form, \" +\n                   \"or quoted\\nlike \\\"attribute = value\\\"\\n\");\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         if (attributes.containsKey(attr)) {\n           out.println(\"\\nEach attribute must correspond to only one value:\\n\" +\n                   \"atttribute \\\"\" + attr + \"\\\" was repeated\\n\" );\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         attributes.put(attr, val);\n       } else if (\"--provider\".equals(args[i]) && moreTokens) {\n@@ -163,17 +173,17 @@\n         interactive = true;\n       } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n-        return -1;\n+        return 1;\n       } else {\n         printKeyShellUsage();\n         ToolRunner.printGenericCommandUsage(System.err);\n-        return -1;\n+        return 1;\n       }\n     }\n \n     if (command == null) {\n       printKeyShellUsage();\n-      return -1;\n+      return 1;\n     }\n \n     if (!attributes.isEmpty()) {\n@@ -491,10 +501,11 @@\n   }\n \n   /**\n-   * Main program.\n+   * main() entry point for the KeyShell.  While strictly speaking the\n+   * return is void, it will System.exit() with a return code: 0 is for\n+   * success and 1 for failure.\n    *\n-   * @param args\n-   *          Command line arguments\n+   * @param args Command line arguments.\n    * @throws Exception\n    */\n   public static void main(String[] args) throws Exception {\n", "projectName": "apache.hadoop", "bugLineNum": 75, "bugNodeStartChar": 2567, "bugNodeLength": 2, "fixLineNum": 75, "fixNodeStartChar": 2567, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 5b83e38..dd31909 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -57,6 +57,16 @@\n \n   private boolean userSuppliedProvider = false;\n \n+  /**\n+   * Primary entry point for the KeyShell; called via main().\n+   *\n+   * @param args Command line arguments.\n+   * @return 0 on success and 1 on failure.  This value is passed back to\n+   * the unix shell, so we must follow shell return code conventions:\n+   * the return code is an unsigned character, and 0 means success, and\n+   * small positive integers mean failure.\n+   * @throws Exception\n+   */\n   @Override\n   public int run(String[] args) throws Exception {\n     int exitCode = 0;\n@@ -68,11 +78,11 @@\n       if (command.validate()) {\n           command.execute();\n       } else {\n-        exitCode = -1;\n+        exitCode = 1;\n       }\n     } catch (Exception e) {\n       e.printStackTrace(err);\n-      return -1;\n+      return 1;\n     }\n     return exitCode;\n   }\n@@ -86,8 +96,8 @@\n    * % hadoop key list [-provider providerPath]\n    * % hadoop key delete keyName [--provider providerPath] [-i]\n    * </pre>\n-   * @param args\n-   * @return\n+   * @param args Command line arguments.\n+   * @return 0 on success, 1 on failure.\n    * @throws IOException\n    */\n   private int init(String[] args) throws IOException {\n@@ -105,7 +115,7 @@\n         command = new CreateCommand(keyName, options);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n         String keyName = \"--help\";\n@@ -116,7 +126,7 @@\n         command = new DeleteCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n         String keyName = \"--help\";\n@@ -127,7 +137,7 @@\n         command = new RollCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n@@ -145,13 +155,13 @@\n           out.println(\"\\nAttributes must be in attribute=value form, \" +\n                   \"or quoted\\nlike \\\"attribute = value\\\"\\n\");\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         if (attributes.containsKey(attr)) {\n           out.println(\"\\nEach attribute must correspond to only one value:\\n\" +\n                   \"atttribute \\\"\" + attr + \"\\\" was repeated\\n\" );\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         attributes.put(attr, val);\n       } else if (\"--provider\".equals(args[i]) && moreTokens) {\n@@ -163,17 +173,17 @@\n         interactive = true;\n       } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n-        return -1;\n+        return 1;\n       } else {\n         printKeyShellUsage();\n         ToolRunner.printGenericCommandUsage(System.err);\n-        return -1;\n+        return 1;\n       }\n     }\n \n     if (command == null) {\n       printKeyShellUsage();\n-      return -1;\n+      return 1;\n     }\n \n     if (!attributes.isEmpty()) {\n@@ -491,10 +501,11 @@\n   }\n \n   /**\n-   * Main program.\n+   * main() entry point for the KeyShell.  While strictly speaking the\n+   * return is void, it will System.exit() with a return code: 0 is for\n+   * success and 1 for failure.\n    *\n-   * @param args\n-   *          Command line arguments\n+   * @param args Command line arguments.\n    * @throws Exception\n    */\n   public static void main(String[] args) throws Exception {\n", "projectName": "apache.hadoop", "bugLineNum": 108, "bugNodeStartChar": 3617, "bugNodeLength": 2, "fixLineNum": 108, "fixNodeStartChar": 3617, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 5b83e38..dd31909 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -57,6 +57,16 @@\n \n   private boolean userSuppliedProvider = false;\n \n+  /**\n+   * Primary entry point for the KeyShell; called via main().\n+   *\n+   * @param args Command line arguments.\n+   * @return 0 on success and 1 on failure.  This value is passed back to\n+   * the unix shell, so we must follow shell return code conventions:\n+   * the return code is an unsigned character, and 0 means success, and\n+   * small positive integers mean failure.\n+   * @throws Exception\n+   */\n   @Override\n   public int run(String[] args) throws Exception {\n     int exitCode = 0;\n@@ -68,11 +78,11 @@\n       if (command.validate()) {\n           command.execute();\n       } else {\n-        exitCode = -1;\n+        exitCode = 1;\n       }\n     } catch (Exception e) {\n       e.printStackTrace(err);\n-      return -1;\n+      return 1;\n     }\n     return exitCode;\n   }\n@@ -86,8 +96,8 @@\n    * % hadoop key list [-provider providerPath]\n    * % hadoop key delete keyName [--provider providerPath] [-i]\n    * </pre>\n-   * @param args\n-   * @return\n+   * @param args Command line arguments.\n+   * @return 0 on success, 1 on failure.\n    * @throws IOException\n    */\n   private int init(String[] args) throws IOException {\n@@ -105,7 +115,7 @@\n         command = new CreateCommand(keyName, options);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n         String keyName = \"--help\";\n@@ -116,7 +126,7 @@\n         command = new DeleteCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n         String keyName = \"--help\";\n@@ -127,7 +137,7 @@\n         command = new RollCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n@@ -145,13 +155,13 @@\n           out.println(\"\\nAttributes must be in attribute=value form, \" +\n                   \"or quoted\\nlike \\\"attribute = value\\\"\\n\");\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         if (attributes.containsKey(attr)) {\n           out.println(\"\\nEach attribute must correspond to only one value:\\n\" +\n                   \"atttribute \\\"\" + attr + \"\\\" was repeated\\n\" );\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         attributes.put(attr, val);\n       } else if (\"--provider\".equals(args[i]) && moreTokens) {\n@@ -163,17 +173,17 @@\n         interactive = true;\n       } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n-        return -1;\n+        return 1;\n       } else {\n         printKeyShellUsage();\n         ToolRunner.printGenericCommandUsage(System.err);\n-        return -1;\n+        return 1;\n       }\n     }\n \n     if (command == null) {\n       printKeyShellUsage();\n-      return -1;\n+      return 1;\n     }\n \n     if (!attributes.isEmpty()) {\n@@ -491,10 +501,11 @@\n   }\n \n   /**\n-   * Main program.\n+   * main() entry point for the KeyShell.  While strictly speaking the\n+   * return is void, it will System.exit() with a return code: 0 is for\n+   * success and 1 for failure.\n    *\n-   * @param args\n-   *          Command line arguments\n+   * @param args Command line arguments.\n    * @throws Exception\n    */\n   public static void main(String[] args) throws Exception {\n", "projectName": "apache.hadoop", "bugLineNum": 119, "bugNodeStartChar": 3914, "bugNodeLength": 2, "fixLineNum": 119, "fixNodeStartChar": 3914, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 5b83e38..dd31909 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -57,6 +57,16 @@\n \n   private boolean userSuppliedProvider = false;\n \n+  /**\n+   * Primary entry point for the KeyShell; called via main().\n+   *\n+   * @param args Command line arguments.\n+   * @return 0 on success and 1 on failure.  This value is passed back to\n+   * the unix shell, so we must follow shell return code conventions:\n+   * the return code is an unsigned character, and 0 means success, and\n+   * small positive integers mean failure.\n+   * @throws Exception\n+   */\n   @Override\n   public int run(String[] args) throws Exception {\n     int exitCode = 0;\n@@ -68,11 +78,11 @@\n       if (command.validate()) {\n           command.execute();\n       } else {\n-        exitCode = -1;\n+        exitCode = 1;\n       }\n     } catch (Exception e) {\n       e.printStackTrace(err);\n-      return -1;\n+      return 1;\n     }\n     return exitCode;\n   }\n@@ -86,8 +96,8 @@\n    * % hadoop key list [-provider providerPath]\n    * % hadoop key delete keyName [--provider providerPath] [-i]\n    * </pre>\n-   * @param args\n-   * @return\n+   * @param args Command line arguments.\n+   * @return 0 on success, 1 on failure.\n    * @throws IOException\n    */\n   private int init(String[] args) throws IOException {\n@@ -105,7 +115,7 @@\n         command = new CreateCommand(keyName, options);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n         String keyName = \"--help\";\n@@ -116,7 +126,7 @@\n         command = new DeleteCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n         String keyName = \"--help\";\n@@ -127,7 +137,7 @@\n         command = new RollCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n@@ -145,13 +155,13 @@\n           out.println(\"\\nAttributes must be in attribute=value form, \" +\n                   \"or quoted\\nlike \\\"attribute = value\\\"\\n\");\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         if (attributes.containsKey(attr)) {\n           out.println(\"\\nEach attribute must correspond to only one value:\\n\" +\n                   \"atttribute \\\"\" + attr + \"\\\" was repeated\\n\" );\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         attributes.put(attr, val);\n       } else if (\"--provider\".equals(args[i]) && moreTokens) {\n@@ -163,17 +173,17 @@\n         interactive = true;\n       } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n-        return -1;\n+        return 1;\n       } else {\n         printKeyShellUsage();\n         ToolRunner.printGenericCommandUsage(System.err);\n-        return -1;\n+        return 1;\n       }\n     }\n \n     if (command == null) {\n       printKeyShellUsage();\n-      return -1;\n+      return 1;\n     }\n \n     if (!attributes.isEmpty()) {\n@@ -491,10 +501,11 @@\n   }\n \n   /**\n-   * Main program.\n+   * main() entry point for the KeyShell.  While strictly speaking the\n+   * return is void, it will System.exit() with a return code: 0 is for\n+   * success and 1 for failure.\n    *\n-   * @param args\n-   *          Command line arguments\n+   * @param args Command line arguments.\n    * @throws Exception\n    */\n   public static void main(String[] args) throws Exception {\n", "projectName": "apache.hadoop", "bugLineNum": 130, "bugNodeStartChar": 4207, "bugNodeLength": 2, "fixLineNum": 130, "fixNodeStartChar": 4207, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 5b83e38..dd31909 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -57,6 +57,16 @@\n \n   private boolean userSuppliedProvider = false;\n \n+  /**\n+   * Primary entry point for the KeyShell; called via main().\n+   *\n+   * @param args Command line arguments.\n+   * @return 0 on success and 1 on failure.  This value is passed back to\n+   * the unix shell, so we must follow shell return code conventions:\n+   * the return code is an unsigned character, and 0 means success, and\n+   * small positive integers mean failure.\n+   * @throws Exception\n+   */\n   @Override\n   public int run(String[] args) throws Exception {\n     int exitCode = 0;\n@@ -68,11 +78,11 @@\n       if (command.validate()) {\n           command.execute();\n       } else {\n-        exitCode = -1;\n+        exitCode = 1;\n       }\n     } catch (Exception e) {\n       e.printStackTrace(err);\n-      return -1;\n+      return 1;\n     }\n     return exitCode;\n   }\n@@ -86,8 +96,8 @@\n    * % hadoop key list [-provider providerPath]\n    * % hadoop key delete keyName [--provider providerPath] [-i]\n    * </pre>\n-   * @param args\n-   * @return\n+   * @param args Command line arguments.\n+   * @return 0 on success, 1 on failure.\n    * @throws IOException\n    */\n   private int init(String[] args) throws IOException {\n@@ -105,7 +115,7 @@\n         command = new CreateCommand(keyName, options);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n         String keyName = \"--help\";\n@@ -116,7 +126,7 @@\n         command = new DeleteCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n         String keyName = \"--help\";\n@@ -127,7 +137,7 @@\n         command = new RollCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n@@ -145,13 +155,13 @@\n           out.println(\"\\nAttributes must be in attribute=value form, \" +\n                   \"or quoted\\nlike \\\"attribute = value\\\"\\n\");\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         if (attributes.containsKey(attr)) {\n           out.println(\"\\nEach attribute must correspond to only one value:\\n\" +\n                   \"atttribute \\\"\" + attr + \"\\\" was repeated\\n\" );\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         attributes.put(attr, val);\n       } else if (\"--provider\".equals(args[i]) && moreTokens) {\n@@ -163,17 +173,17 @@\n         interactive = true;\n       } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n-        return -1;\n+        return 1;\n       } else {\n         printKeyShellUsage();\n         ToolRunner.printGenericCommandUsage(System.err);\n-        return -1;\n+        return 1;\n       }\n     }\n \n     if (command == null) {\n       printKeyShellUsage();\n-      return -1;\n+      return 1;\n     }\n \n     if (!attributes.isEmpty()) {\n@@ -491,10 +501,11 @@\n   }\n \n   /**\n-   * Main program.\n+   * main() entry point for the KeyShell.  While strictly speaking the\n+   * return is void, it will System.exit() with a return code: 0 is for\n+   * success and 1 for failure.\n    *\n-   * @param args\n-   *          Command line arguments\n+   * @param args Command line arguments.\n    * @throws Exception\n    */\n   public static void main(String[] args) throws Exception {\n", "projectName": "apache.hadoop", "bugLineNum": 148, "bugNodeStartChar": 5068, "bugNodeLength": 2, "fixLineNum": 148, "fixNodeStartChar": 5068, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 5b83e38..dd31909 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -57,6 +57,16 @@\n \n   private boolean userSuppliedProvider = false;\n \n+  /**\n+   * Primary entry point for the KeyShell; called via main().\n+   *\n+   * @param args Command line arguments.\n+   * @return 0 on success and 1 on failure.  This value is passed back to\n+   * the unix shell, so we must follow shell return code conventions:\n+   * the return code is an unsigned character, and 0 means success, and\n+   * small positive integers mean failure.\n+   * @throws Exception\n+   */\n   @Override\n   public int run(String[] args) throws Exception {\n     int exitCode = 0;\n@@ -68,11 +78,11 @@\n       if (command.validate()) {\n           command.execute();\n       } else {\n-        exitCode = -1;\n+        exitCode = 1;\n       }\n     } catch (Exception e) {\n       e.printStackTrace(err);\n-      return -1;\n+      return 1;\n     }\n     return exitCode;\n   }\n@@ -86,8 +96,8 @@\n    * % hadoop key list [-provider providerPath]\n    * % hadoop key delete keyName [--provider providerPath] [-i]\n    * </pre>\n-   * @param args\n-   * @return\n+   * @param args Command line arguments.\n+   * @return 0 on success, 1 on failure.\n    * @throws IOException\n    */\n   private int init(String[] args) throws IOException {\n@@ -105,7 +115,7 @@\n         command = new CreateCommand(keyName, options);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n         String keyName = \"--help\";\n@@ -116,7 +126,7 @@\n         command = new DeleteCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n         String keyName = \"--help\";\n@@ -127,7 +137,7 @@\n         command = new RollCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n@@ -145,13 +155,13 @@\n           out.println(\"\\nAttributes must be in attribute=value form, \" +\n                   \"or quoted\\nlike \\\"attribute = value\\\"\\n\");\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         if (attributes.containsKey(attr)) {\n           out.println(\"\\nEach attribute must correspond to only one value:\\n\" +\n                   \"atttribute \\\"\" + attr + \"\\\" was repeated\\n\" );\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         attributes.put(attr, val);\n       } else if (\"--provider\".equals(args[i]) && moreTokens) {\n@@ -163,17 +173,17 @@\n         interactive = true;\n       } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n-        return -1;\n+        return 1;\n       } else {\n         printKeyShellUsage();\n         ToolRunner.printGenericCommandUsage(System.err);\n-        return -1;\n+        return 1;\n       }\n     }\n \n     if (command == null) {\n       printKeyShellUsage();\n-      return -1;\n+      return 1;\n     }\n \n     if (!attributes.isEmpty()) {\n@@ -491,10 +501,11 @@\n   }\n \n   /**\n-   * Main program.\n+   * main() entry point for the KeyShell.  While strictly speaking the\n+   * return is void, it will System.exit() with a return code: 0 is for\n+   * success and 1 for failure.\n    *\n-   * @param args\n-   *          Command line arguments\n+   * @param args Command line arguments.\n    * @throws Exception\n    */\n   public static void main(String[] args) throws Exception {\n", "projectName": "apache.hadoop", "bugLineNum": 154, "bugNodeStartChar": 5321, "bugNodeLength": 2, "fixLineNum": 154, "fixNodeStartChar": 5321, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 5b83e38..dd31909 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -57,6 +57,16 @@\n \n   private boolean userSuppliedProvider = false;\n \n+  /**\n+   * Primary entry point for the KeyShell; called via main().\n+   *\n+   * @param args Command line arguments.\n+   * @return 0 on success and 1 on failure.  This value is passed back to\n+   * the unix shell, so we must follow shell return code conventions:\n+   * the return code is an unsigned character, and 0 means success, and\n+   * small positive integers mean failure.\n+   * @throws Exception\n+   */\n   @Override\n   public int run(String[] args) throws Exception {\n     int exitCode = 0;\n@@ -68,11 +78,11 @@\n       if (command.validate()) {\n           command.execute();\n       } else {\n-        exitCode = -1;\n+        exitCode = 1;\n       }\n     } catch (Exception e) {\n       e.printStackTrace(err);\n-      return -1;\n+      return 1;\n     }\n     return exitCode;\n   }\n@@ -86,8 +96,8 @@\n    * % hadoop key list [-provider providerPath]\n    * % hadoop key delete keyName [--provider providerPath] [-i]\n    * </pre>\n-   * @param args\n-   * @return\n+   * @param args Command line arguments.\n+   * @return 0 on success, 1 on failure.\n    * @throws IOException\n    */\n   private int init(String[] args) throws IOException {\n@@ -105,7 +115,7 @@\n         command = new CreateCommand(keyName, options);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n         String keyName = \"--help\";\n@@ -116,7 +126,7 @@\n         command = new DeleteCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n         String keyName = \"--help\";\n@@ -127,7 +137,7 @@\n         command = new RollCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n@@ -145,13 +155,13 @@\n           out.println(\"\\nAttributes must be in attribute=value form, \" +\n                   \"or quoted\\nlike \\\"attribute = value\\\"\\n\");\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         if (attributes.containsKey(attr)) {\n           out.println(\"\\nEach attribute must correspond to only one value:\\n\" +\n                   \"atttribute \\\"\" + attr + \"\\\" was repeated\\n\" );\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         attributes.put(attr, val);\n       } else if (\"--provider\".equals(args[i]) && moreTokens) {\n@@ -163,17 +173,17 @@\n         interactive = true;\n       } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n-        return -1;\n+        return 1;\n       } else {\n         printKeyShellUsage();\n         ToolRunner.printGenericCommandUsage(System.err);\n-        return -1;\n+        return 1;\n       }\n     }\n \n     if (command == null) {\n       printKeyShellUsage();\n-      return -1;\n+      return 1;\n     }\n \n     if (!attributes.isEmpty()) {\n@@ -491,10 +501,11 @@\n   }\n \n   /**\n-   * Main program.\n+   * main() entry point for the KeyShell.  While strictly speaking the\n+   * return is void, it will System.exit() with a return code: 0 is for\n+   * success and 1 for failure.\n    *\n-   * @param args\n-   *          Command line arguments\n+   * @param args Command line arguments.\n    * @throws Exception\n    */\n   public static void main(String[] args) throws Exception {\n", "projectName": "apache.hadoop", "bugLineNum": 166, "bugNodeStartChar": 5838, "bugNodeLength": 2, "fixLineNum": 166, "fixNodeStartChar": 5838, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 5b83e38..dd31909 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -57,6 +57,16 @@\n \n   private boolean userSuppliedProvider = false;\n \n+  /**\n+   * Primary entry point for the KeyShell; called via main().\n+   *\n+   * @param args Command line arguments.\n+   * @return 0 on success and 1 on failure.  This value is passed back to\n+   * the unix shell, so we must follow shell return code conventions:\n+   * the return code is an unsigned character, and 0 means success, and\n+   * small positive integers mean failure.\n+   * @throws Exception\n+   */\n   @Override\n   public int run(String[] args) throws Exception {\n     int exitCode = 0;\n@@ -68,11 +78,11 @@\n       if (command.validate()) {\n           command.execute();\n       } else {\n-        exitCode = -1;\n+        exitCode = 1;\n       }\n     } catch (Exception e) {\n       e.printStackTrace(err);\n-      return -1;\n+      return 1;\n     }\n     return exitCode;\n   }\n@@ -86,8 +96,8 @@\n    * % hadoop key list [-provider providerPath]\n    * % hadoop key delete keyName [--provider providerPath] [-i]\n    * </pre>\n-   * @param args\n-   * @return\n+   * @param args Command line arguments.\n+   * @return 0 on success, 1 on failure.\n    * @throws IOException\n    */\n   private int init(String[] args) throws IOException {\n@@ -105,7 +115,7 @@\n         command = new CreateCommand(keyName, options);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n         String keyName = \"--help\";\n@@ -116,7 +126,7 @@\n         command = new DeleteCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n         String keyName = \"--help\";\n@@ -127,7 +137,7 @@\n         command = new RollCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n@@ -145,13 +155,13 @@\n           out.println(\"\\nAttributes must be in attribute=value form, \" +\n                   \"or quoted\\nlike \\\"attribute = value\\\"\\n\");\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         if (attributes.containsKey(attr)) {\n           out.println(\"\\nEach attribute must correspond to only one value:\\n\" +\n                   \"atttribute \\\"\" + attr + \"\\\" was repeated\\n\" );\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         attributes.put(attr, val);\n       } else if (\"--provider\".equals(args[i]) && moreTokens) {\n@@ -163,17 +173,17 @@\n         interactive = true;\n       } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n-        return -1;\n+        return 1;\n       } else {\n         printKeyShellUsage();\n         ToolRunner.printGenericCommandUsage(System.err);\n-        return -1;\n+        return 1;\n       }\n     }\n \n     if (command == null) {\n       printKeyShellUsage();\n-      return -1;\n+      return 1;\n     }\n \n     if (!attributes.isEmpty()) {\n@@ -491,10 +501,11 @@\n   }\n \n   /**\n-   * Main program.\n+   * main() entry point for the KeyShell.  While strictly speaking the\n+   * return is void, it will System.exit() with a return code: 0 is for\n+   * success and 1 for failure.\n    *\n-   * @param args\n-   *          Command line arguments\n+   * @param args Command line arguments.\n    * @throws Exception\n    */\n   public static void main(String[] args) throws Exception {\n", "projectName": "apache.hadoop", "bugLineNum": 170, "bugNodeStartChar": 5959, "bugNodeLength": 2, "fixLineNum": 170, "fixNodeStartChar": 5959, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\nindex 5b83e38..dd31909 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java\n@@ -57,6 +57,16 @@\n \n   private boolean userSuppliedProvider = false;\n \n+  /**\n+   * Primary entry point for the KeyShell; called via main().\n+   *\n+   * @param args Command line arguments.\n+   * @return 0 on success and 1 on failure.  This value is passed back to\n+   * the unix shell, so we must follow shell return code conventions:\n+   * the return code is an unsigned character, and 0 means success, and\n+   * small positive integers mean failure.\n+   * @throws Exception\n+   */\n   @Override\n   public int run(String[] args) throws Exception {\n     int exitCode = 0;\n@@ -68,11 +78,11 @@\n       if (command.validate()) {\n           command.execute();\n       } else {\n-        exitCode = -1;\n+        exitCode = 1;\n       }\n     } catch (Exception e) {\n       e.printStackTrace(err);\n-      return -1;\n+      return 1;\n     }\n     return exitCode;\n   }\n@@ -86,8 +96,8 @@\n    * % hadoop key list [-provider providerPath]\n    * % hadoop key delete keyName [--provider providerPath] [-i]\n    * </pre>\n-   * @param args\n-   * @return\n+   * @param args Command line arguments.\n+   * @return 0 on success, 1 on failure.\n    * @throws IOException\n    */\n   private int init(String[] args) throws IOException {\n@@ -105,7 +115,7 @@\n         command = new CreateCommand(keyName, options);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n         String keyName = \"--help\";\n@@ -116,7 +126,7 @@\n         command = new DeleteCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n         String keyName = \"--help\";\n@@ -127,7 +137,7 @@\n         command = new RollCommand(keyName);\n         if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n       } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n@@ -145,13 +155,13 @@\n           out.println(\"\\nAttributes must be in attribute=value form, \" +\n                   \"or quoted\\nlike \\\"attribute = value\\\"\\n\");\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         if (attributes.containsKey(attr)) {\n           out.println(\"\\nEach attribute must correspond to only one value:\\n\" +\n                   \"atttribute \\\"\" + attr + \"\\\" was repeated\\n\" );\n           printKeyShellUsage();\n-          return -1;\n+          return 1;\n         }\n         attributes.put(attr, val);\n       } else if (\"--provider\".equals(args[i]) && moreTokens) {\n@@ -163,17 +173,17 @@\n         interactive = true;\n       } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n-        return -1;\n+        return 1;\n       } else {\n         printKeyShellUsage();\n         ToolRunner.printGenericCommandUsage(System.err);\n-        return -1;\n+        return 1;\n       }\n     }\n \n     if (command == null) {\n       printKeyShellUsage();\n-      return -1;\n+      return 1;\n     }\n \n     if (!attributes.isEmpty()) {\n@@ -491,10 +501,11 @@\n   }\n \n   /**\n-   * Main program.\n+   * main() entry point for the KeyShell.  While strictly speaking the\n+   * return is void, it will System.exit() with a return code: 0 is for\n+   * success and 1 for failure.\n    *\n-   * @param args\n-   *          Command line arguments\n+   * @param args Command line arguments.\n    * @throws Exception\n    */\n   public static void main(String[] args) throws Exception {\n", "projectName": "apache.hadoop", "bugLineNum": 176, "bugNodeStartChar": 6046, "bugNodeLength": 2, "fixLineNum": 176, "fixNodeStartChar": 6046, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\nindex 6b24a25..e22949ee 100644\n--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n@@ -161,7 +161,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -174,7 +174,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -187,7 +187,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -216,7 +216,7 @@\n     config.set(KeyProviderFactory.KEY_PROVIDER_PATH, \"user:///\");\n     ks.setConf(config);\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -262,19 +262,19 @@\n     final String[] args2 = {\"create\", \"keyattr2\", \"--provider\", jceksProvider,\n             \"--attr\", \"=bar\"};\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Not in attribute = value form */\n     outContent.reset();\n     args2[5] = \"foo\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* No attribute or value */\n     outContent.reset();\n     args2[5] = \"=\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Legal: attribute is a, value is b=c */\n     outContent.reset();\n@@ -308,7 +308,7 @@\n             \"--attr\", \"foo=bar\",\n             \"--attr\", \"foo=glarch\"};\n     rc = ks.run(args4);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Clean up to be a good citizen */\n     deleteKey(ks, \"keyattr1\");\n", "projectName": "apache.hadoop", "bugLineNum": 164, "bugNodeStartChar": 5446, "bugNodeLength": 2, "fixLineNum": 164, "fixNodeStartChar": 5446, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\nindex 6b24a25..e22949ee 100644\n--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n@@ -161,7 +161,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -174,7 +174,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -187,7 +187,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -216,7 +216,7 @@\n     config.set(KeyProviderFactory.KEY_PROVIDER_PATH, \"user:///\");\n     ks.setConf(config);\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -262,19 +262,19 @@\n     final String[] args2 = {\"create\", \"keyattr2\", \"--provider\", jceksProvider,\n             \"--attr\", \"=bar\"};\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Not in attribute = value form */\n     outContent.reset();\n     args2[5] = \"foo\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* No attribute or value */\n     outContent.reset();\n     args2[5] = \"=\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Legal: attribute is a, value is b=c */\n     outContent.reset();\n@@ -308,7 +308,7 @@\n             \"--attr\", \"foo=bar\",\n             \"--attr\", \"foo=glarch\"};\n     rc = ks.run(args4);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Clean up to be a good citizen */\n     deleteKey(ks, \"keyattr1\");\n", "projectName": "apache.hadoop", "bugLineNum": 177, "bugNodeStartChar": 5835, "bugNodeLength": 2, "fixLineNum": 177, "fixNodeStartChar": 5835, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\nindex 6b24a25..e22949ee 100644\n--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n@@ -161,7 +161,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -174,7 +174,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -187,7 +187,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -216,7 +216,7 @@\n     config.set(KeyProviderFactory.KEY_PROVIDER_PATH, \"user:///\");\n     ks.setConf(config);\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -262,19 +262,19 @@\n     final String[] args2 = {\"create\", \"keyattr2\", \"--provider\", jceksProvider,\n             \"--attr\", \"=bar\"};\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Not in attribute = value form */\n     outContent.reset();\n     args2[5] = \"foo\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* No attribute or value */\n     outContent.reset();\n     args2[5] = \"=\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Legal: attribute is a, value is b=c */\n     outContent.reset();\n@@ -308,7 +308,7 @@\n             \"--attr\", \"foo=bar\",\n             \"--attr\", \"foo=glarch\"};\n     rc = ks.run(args4);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Clean up to be a good citizen */\n     deleteKey(ks, \"keyattr1\");\n", "projectName": "apache.hadoop", "bugLineNum": 190, "bugNodeStartChar": 6239, "bugNodeLength": 2, "fixLineNum": 190, "fixNodeStartChar": 6239, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\nindex 6b24a25..e22949ee 100644\n--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n@@ -161,7 +161,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -174,7 +174,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -187,7 +187,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -216,7 +216,7 @@\n     config.set(KeyProviderFactory.KEY_PROVIDER_PATH, \"user:///\");\n     ks.setConf(config);\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -262,19 +262,19 @@\n     final String[] args2 = {\"create\", \"keyattr2\", \"--provider\", jceksProvider,\n             \"--attr\", \"=bar\"};\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Not in attribute = value form */\n     outContent.reset();\n     args2[5] = \"foo\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* No attribute or value */\n     outContent.reset();\n     args2[5] = \"=\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Legal: attribute is a, value is b=c */\n     outContent.reset();\n@@ -308,7 +308,7 @@\n             \"--attr\", \"foo=bar\",\n             \"--attr\", \"foo=glarch\"};\n     rc = ks.run(args4);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Clean up to be a good citizen */\n     deleteKey(ks, \"keyattr1\");\n", "projectName": "apache.hadoop", "bugLineNum": 219, "bugNodeStartChar": 7126, "bugNodeLength": 2, "fixLineNum": 219, "fixNodeStartChar": 7126, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\nindex 6b24a25..e22949ee 100644\n--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n@@ -161,7 +161,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -174,7 +174,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -187,7 +187,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -216,7 +216,7 @@\n     config.set(KeyProviderFactory.KEY_PROVIDER_PATH, \"user:///\");\n     ks.setConf(config);\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -262,19 +262,19 @@\n     final String[] args2 = {\"create\", \"keyattr2\", \"--provider\", jceksProvider,\n             \"--attr\", \"=bar\"};\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Not in attribute = value form */\n     outContent.reset();\n     args2[5] = \"foo\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* No attribute or value */\n     outContent.reset();\n     args2[5] = \"=\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Legal: attribute is a, value is b=c */\n     outContent.reset();\n@@ -308,7 +308,7 @@\n             \"--attr\", \"foo=bar\",\n             \"--attr\", \"foo=glarch\"};\n     rc = ks.run(args4);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Clean up to be a good citizen */\n     deleteKey(ks, \"keyattr1\");\n", "projectName": "apache.hadoop", "bugLineNum": 265, "bugNodeStartChar": 8603, "bugNodeLength": 2, "fixLineNum": 265, "fixNodeStartChar": 8603, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\nindex 6b24a25..e22949ee 100644\n--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n@@ -161,7 +161,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -174,7 +174,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -187,7 +187,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -216,7 +216,7 @@\n     config.set(KeyProviderFactory.KEY_PROVIDER_PATH, \"user:///\");\n     ks.setConf(config);\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -262,19 +262,19 @@\n     final String[] args2 = {\"create\", \"keyattr2\", \"--provider\", jceksProvider,\n             \"--attr\", \"=bar\"};\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Not in attribute = value form */\n     outContent.reset();\n     args2[5] = \"foo\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* No attribute or value */\n     outContent.reset();\n     args2[5] = \"=\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Legal: attribute is a, value is b=c */\n     outContent.reset();\n@@ -308,7 +308,7 @@\n             \"--attr\", \"foo=bar\",\n             \"--attr\", \"foo=glarch\"};\n     rc = ks.run(args4);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Clean up to be a good citizen */\n     deleteKey(ks, \"keyattr1\");\n", "projectName": "apache.hadoop", "bugLineNum": 271, "bugNodeStartChar": 8740, "bugNodeLength": 2, "fixLineNum": 271, "fixNodeStartChar": 8740, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\nindex 6b24a25..e22949ee 100644\n--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n@@ -161,7 +161,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -174,7 +174,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -187,7 +187,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -216,7 +216,7 @@\n     config.set(KeyProviderFactory.KEY_PROVIDER_PATH, \"user:///\");\n     ks.setConf(config);\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -262,19 +262,19 @@\n     final String[] args2 = {\"create\", \"keyattr2\", \"--provider\", jceksProvider,\n             \"--attr\", \"=bar\"};\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Not in attribute = value form */\n     outContent.reset();\n     args2[5] = \"foo\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* No attribute or value */\n     outContent.reset();\n     args2[5] = \"=\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Legal: attribute is a, value is b=c */\n     outContent.reset();\n@@ -308,7 +308,7 @@\n             \"--attr\", \"foo=bar\",\n             \"--attr\", \"foo=glarch\"};\n     rc = ks.run(args4);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Clean up to be a good citizen */\n     deleteKey(ks, \"keyattr1\");\n", "projectName": "apache.hadoop", "bugLineNum": 277, "bugNodeStartChar": 8867, "bugNodeLength": 2, "fixLineNum": 277, "fixNodeStartChar": 8867, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "9467bef8ea9a62658b32dd43a76f4f98087d1986", "fixCommitParentSHA1": "0ef751797e78f04798933bb87cf8bbb291248753", "bugFilePath": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\nindex 6b24a25..e22949ee 100644\n--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java\n@@ -161,7 +161,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -174,7 +174,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n@@ -187,7 +187,7 @@\n     KeyShell ks = new KeyShell();\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -216,7 +216,7 @@\n     config.set(KeyProviderFactory.KEY_PROVIDER_PATH, \"user:///\");\n     ks.setConf(config);\n     rc = ks.run(args1);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n     assertTrue(outContent.toString().contains(\"There are no valid \" +\n \t\t\"KeyProviders configured.\"));\n   }\n@@ -262,19 +262,19 @@\n     final String[] args2 = {\"create\", \"keyattr2\", \"--provider\", jceksProvider,\n             \"--attr\", \"=bar\"};\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Not in attribute = value form */\n     outContent.reset();\n     args2[5] = \"foo\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* No attribute or value */\n     outContent.reset();\n     args2[5] = \"=\";\n     rc = ks.run(args2);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Legal: attribute is a, value is b=c */\n     outContent.reset();\n@@ -308,7 +308,7 @@\n             \"--attr\", \"foo=bar\",\n             \"--attr\", \"foo=glarch\"};\n     rc = ks.run(args4);\n-    assertEquals(-1, rc);\n+    assertEquals(1, rc);\n \n     /* Clean up to be a good citizen */\n     deleteKey(ks, \"keyattr1\");\n", "projectName": "apache.hadoop", "bugLineNum": 311, "bugNodeStartChar": 10006, "bugNodeLength": 2, "fixLineNum": 311, "fixNodeStartChar": 10006, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "d2314c583d3463e3fa020e9d42aecf91b55c582c", "fixCommitParentSHA1": "62497eb64fdf8a92fe09c0165886c260cd1f488c", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java\nindex efe0721..e289ad5 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java\n@@ -526,7 +526,7 @@\n \n   private void setTrackingUrlToRMAppPage() {\n     originalTrackingUrl = pjoin(\n-        WebAppUtils.getResolvedRMWebAppURLWithoutScheme(conf),\n+        WebAppUtils.getResolvedRMWebAppURLWithScheme(conf),\n         \"cluster\", \"app\", getAppAttemptId().getApplicationId());\n     proxiedTrackingUrl = originalTrackingUrl;\n   }\n", "projectName": "apache.hadoop", "bugLineNum": 529, "bugNodeStartChar": 23719, "bugNodeLength": 53, "fixLineNum": 529, "fixNodeStartChar": 23719, "fixNodeLength": 50, "sourceBeforeFix": "WebAppUtils.getResolvedRMWebAppURLWithoutScheme(conf)", "sourceAfterFix": "WebAppUtils.getResolvedRMWebAppURLWithScheme(conf)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "d2314c583d3463e3fa020e9d42aecf91b55c582c", "fixCommitParentSHA1": "62497eb64fdf8a92fe09c0165886c260cd1f488c", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java\nindex efe0721..e289ad5 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java\n@@ -526,7 +526,7 @@\n \n   private void setTrackingUrlToRMAppPage() {\n     originalTrackingUrl = pjoin(\n-        WebAppUtils.getResolvedRMWebAppURLWithoutScheme(conf),\n+        WebAppUtils.getResolvedRMWebAppURLWithScheme(conf),\n         \"cluster\", \"app\", getAppAttemptId().getApplicationId());\n     proxiedTrackingUrl = originalTrackingUrl;\n   }\n", "projectName": "apache.hadoop", "bugLineNum": 529, "bugNodeStartChar": 23719, "bugNodeLength": 53, "fixLineNum": 529, "fixNodeStartChar": 23719, "fixNodeLength": 50, "sourceBeforeFix": "WebAppUtils.getResolvedRMWebAppURLWithoutScheme(conf)", "sourceAfterFix": "WebAppUtils.getResolvedRMWebAppURLWithScheme(conf)"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "d2314c583d3463e3fa020e9d42aecf91b55c582c", "fixCommitParentSHA1": "62497eb64fdf8a92fe09c0165886c260cd1f488c", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java\nindex 7868fa0..133e12f 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java\n@@ -113,7 +113,7 @@\n   \n   private static final String EMPTY_DIAGNOSTICS = \"\";\n   private static final String RM_WEBAPP_ADDR =\n-      WebAppUtils.getResolvedRMWebAppURLWithoutScheme(new Configuration());\n+      WebAppUtils.getResolvedRMWebAppURLWithScheme(new Configuration());\n   \n   private boolean isSecurityEnabled;\n   private RMContext rmContext;\n", "projectName": "apache.hadoop", "bugLineNum": 116, "bugNodeStartChar": 6441, "bugNodeLength": 68, "fixLineNum": 116, "fixNodeStartChar": 6441, "fixNodeLength": 65, "sourceBeforeFix": "WebAppUtils.getResolvedRMWebAppURLWithoutScheme(new Configuration())", "sourceAfterFix": "WebAppUtils.getResolvedRMWebAppURLWithScheme(new Configuration())"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "d2314c583d3463e3fa020e9d42aecf91b55c582c", "fixCommitParentSHA1": "62497eb64fdf8a92fe09c0165886c260cd1f488c", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java\nindex 7868fa0..133e12f 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java\n@@ -113,7 +113,7 @@\n   \n   private static final String EMPTY_DIAGNOSTICS = \"\";\n   private static final String RM_WEBAPP_ADDR =\n-      WebAppUtils.getResolvedRMWebAppURLWithoutScheme(new Configuration());\n+      WebAppUtils.getResolvedRMWebAppURLWithScheme(new Configuration());\n   \n   private boolean isSecurityEnabled;\n   private RMContext rmContext;\n", "projectName": "apache.hadoop", "bugLineNum": 116, "bugNodeStartChar": 6441, "bugNodeLength": 68, "fixLineNum": 116, "fixNodeStartChar": 6441, "fixNodeLength": 65, "sourceBeforeFix": "WebAppUtils.getResolvedRMWebAppURLWithoutScheme(new Configuration())", "sourceAfterFix": "WebAppUtils.getResolvedRMWebAppURLWithScheme(new Configuration())"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "f10bf59e6ecf33a0dc996b884e486e42e9241853", "fixCommitParentSHA1": "770aef5053563d6e7984a6873d86456b00413dc6", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/ahs/TestRMApplicationHistoryWriter.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/ahs/TestRMApplicationHistoryWriter.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/ahs/TestRMApplicationHistoryWriter.java\nindex d8f2a37..91a1a65 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/ahs/TestRMApplicationHistoryWriter.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/ahs/TestRMApplicationHistoryWriter.java\n@@ -439,7 +439,7 @@\n     int cleanedSize = cleaned.size();\n     waitCount = 0;\n     while (cleanedSize < allocatedSize && waitCount++ < 200) {\n-      Thread.sleep(100);\n+      Thread.sleep(300);\n       resp = nm.nodeHeartbeat(true);\n       cleaned = resp.getContainersToCleanup();\n       cleanedSize += cleaned.size();\n", "projectName": "apache.hadoop", "bugLineNum": 442, "bugNodeStartChar": 17052, "bugNodeLength": 17, "fixLineNum": 442, "fixNodeStartChar": 17052, "fixNodeLength": 17, "sourceBeforeFix": "Thread.sleep(100)", "sourceAfterFix": "Thread.sleep(300)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "a558f6310502c9db5bdda7f63fd0c7357706cdd6", "fixCommitParentSHA1": "85c733feeeaeab151ac6ca3eb4cc1d3b4be035eb", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java\nindex eefb620..7a39dee 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java\n@@ -53,7 +53,7 @@\n   private static final Log LOG = LogFactory.getLog(\n   \"org.apache.hadoop.hdfs.TestBalancerWithNodeGroup\");\n   \n-  final private static long CAPACITY = 500L;\n+  final private static long CAPACITY = 6000L;\n   final private static String RACK0 = \"/rack0\";\n   final private static String RACK1 = \"/rack1\";\n   final private static String NODEGROUP0 = \"/nodegroup0\";\n@@ -68,7 +68,7 @@\n   static final long TIMEOUT = 40000L; //msec\n   static final double CAPACITY_ALLOWED_VARIANCE = 0.005;  // 0.5%\n   static final double BALANCE_ALLOWED_VARIANCE = 0.11;    // 10%+delta\n-  static final int DEFAULT_BLOCK_SIZE = 10;\n+  static final int DEFAULT_BLOCK_SIZE = 100;\n \n   static {\n     Balancer.setBlockMoveWaitTime(1000L) ;\n", "projectName": "apache.hadoop", "bugLineNum": 56, "bugNodeStartChar": 2254, "bugNodeLength": 15, "fixLineNum": 56, "fixNodeStartChar": 2254, "fixNodeLength": 16, "sourceBeforeFix": "CAPACITY=500L", "sourceAfterFix": "CAPACITY=6000L"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "a558f6310502c9db5bdda7f63fd0c7357706cdd6", "fixCommitParentSHA1": "85c733feeeaeab151ac6ca3eb4cc1d3b4be035eb", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java\nindex eefb620..7a39dee 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java\n@@ -53,7 +53,7 @@\n   private static final Log LOG = LogFactory.getLog(\n   \"org.apache.hadoop.hdfs.TestBalancerWithNodeGroup\");\n   \n-  final private static long CAPACITY = 500L;\n+  final private static long CAPACITY = 6000L;\n   final private static String RACK0 = \"/rack0\";\n   final private static String RACK1 = \"/rack1\";\n   final private static String NODEGROUP0 = \"/nodegroup0\";\n@@ -68,7 +68,7 @@\n   static final long TIMEOUT = 40000L; //msec\n   static final double CAPACITY_ALLOWED_VARIANCE = 0.005;  // 0.5%\n   static final double BALANCE_ALLOWED_VARIANCE = 0.11;    // 10%+delta\n-  static final int DEFAULT_BLOCK_SIZE = 10;\n+  static final int DEFAULT_BLOCK_SIZE = 100;\n \n   static {\n     Balancer.setBlockMoveWaitTime(1000L) ;\n", "projectName": "apache.hadoop", "bugLineNum": 71, "bugNodeStartChar": 2920, "bugNodeLength": 23, "fixLineNum": 71, "fixNodeStartChar": 2920, "fixNodeLength": 24, "sourceBeforeFix": "DEFAULT_BLOCK_SIZE=10", "sourceAfterFix": "DEFAULT_BLOCK_SIZE=100"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "2c3feeb727c012f119e4c55efb51665a71a2d259", "fixCommitParentSHA1": "e1bda693ddb891ad34252dafe4c0abcb7a7c8d19", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobsWithProfiler.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobsWithProfiler.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobsWithProfiler.java\nindex de17528..ac81896 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobsWithProfiler.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobsWithProfiler.java\n@@ -104,7 +104,7 @@\n   }\n \n \n-  @Test (timeout = 120000)\n+  @Test (timeout = 150000)\n   public void testProfiler() throws IOException, InterruptedException,\n       ClassNotFoundException {\n     if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {\n", "projectName": "apache.hadoop", "bugLineNum": 107, "bugNodeStartChar": 3472, "bugNodeLength": 16, "fixLineNum": 107, "fixNodeStartChar": 3472, "fixNodeLength": 16, "sourceBeforeFix": "timeout=120000", "sourceAfterFix": "timeout=150000"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "7feb5bc8aa102782bd8c1ce381449c839207646d", "fixCommitParentSHA1": "10e98ce6bb32c21573b5645014968bba750d324c", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java\nindex bbb73c5..bc1eebc 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java\n@@ -54,7 +54,7 @@\n   private final static long USER_GROUP_STRID_MASK = (1 << 24) - 1;\n   private final static int USER_STRID_OFFSET = 40;\n   private final static int GROUP_STRID_OFFSET = 16;\n-  private static final Log LOG = LogFactory.getLog(FSImageFormatProtobuf.class);\n+  private static final Log LOG = LogFactory.getLog(FSImageFormatPBINode.class);\n \n   public final static class Loader {\n     public static PermissionStatus loadPermission(long id,\n", "projectName": "apache.hadoop", "bugLineNum": 57, "bugNodeStartChar": 2647, "bugNodeLength": 27, "fixLineNum": 57, "fixNodeStartChar": 2647, "fixNodeLength": 26, "sourceBeforeFix": "FSImageFormatProtobuf.class", "sourceAfterFix": "FSImageFormatPBINode.class"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "5c26966bb7d8ea08fc45b1068c104d5814e93852", "fixCommitParentSHA1": "d34f27becfb7c815c8e33b6d63db3f308d0beaeb", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java\nindex 1f5883c..2342b37 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancer.java\n@@ -74,7 +74,7 @@\n \n   ClientProtocol client;\n \n-  static final long TIMEOUT = 20000L; //msec\n+  static final long TIMEOUT = 40000L; //msec\n   static final double CAPACITY_ALLOWED_VARIANCE = 0.005;  // 0.5%\n   static final double BALANCE_ALLOWED_VARIANCE = 0.11;    // 10%+delta\n   static final int DEFAULT_BLOCK_SIZE = 10;\n", "projectName": "apache.hadoop", "bugLineNum": 77, "bugNodeStartChar": 2891, "bugNodeLength": 16, "fixLineNum": 77, "fixNodeStartChar": 2891, "fixNodeLength": 16, "sourceBeforeFix": "TIMEOUT=20000L", "sourceAfterFix": "TIMEOUT=40000L"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "5d53da64dbc5b3f166ef81fbc408dacb1bb28de3", "fixCommitParentSHA1": "673ba4c00ee2ee48a350f3828ca588ca2b841172", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java\nindex 7e7fdb7..7785e56 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java\n@@ -432,7 +432,7 @@\n       .transferStateFromPreviousAppSchedulingInfo(appAttempt.appSchedulingInfo);\n   }\n   \n-  public void move(Queue newQueue) {\n+  public synchronized void move(Queue newQueue) {\n     QueueMetrics oldMetrics = queue.getMetrics();\n     QueueMetrics newMetrics = newQueue.getMetrics();\n     String user = getUser();\n", "projectName": "apache.hadoop", "bugLineNum": 435, "bugNodeStartChar": 15670, "bugNodeLength": 811, "fixLineNum": 435, "fixNodeStartChar": 15670, "fixNodeLength": 824, "sourceBeforeFix": "1", "sourceAfterFix": "33"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "14b92d8c8c61a1acf6c472026bd66948da6fb28f", "fixCommitParentSHA1": "3945a44bdb05ee89ab1cc0fe7c5a9bbd259bafa7", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java\nindex ff9ea07..eefb620 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/balancer/TestBalancerWithNodeGroup.java\n@@ -65,7 +65,7 @@\n \n   ClientProtocol client;\n \n-  static final long TIMEOUT = 20000L; //msec\n+  static final long TIMEOUT = 40000L; //msec\n   static final double CAPACITY_ALLOWED_VARIANCE = 0.005;  // 0.5%\n   static final double BALANCE_ALLOWED_VARIANCE = 0.11;    // 10%+delta\n   static final int DEFAULT_BLOCK_SIZE = 10;\n", "projectName": "apache.hadoop", "bugLineNum": 68, "bugNodeStartChar": 2739, "bugNodeLength": 16, "fixLineNum": 68, "fixNodeStartChar": 2739, "fixNodeLength": 16, "sourceBeforeFix": "TIMEOUT=20000L", "sourceAfterFix": "TIMEOUT=40000L"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "d349a29b7e7acfceb67454a40bbca333fb18f960", "fixCommitParentSHA1": "e775dc3485fbd59e041d5198cbc71851587508f2", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ApplicationCLI.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ApplicationCLI.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ApplicationCLI.java\nindex 9b465b7..d520866 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ApplicationCLI.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/ApplicationCLI.java\n@@ -197,7 +197,7 @@\n         listApplications(appTypes, appStates);\n       }\n     } else if (cliParser.hasOption(KILL_CMD)) {\n-      if (args.length != 2) {\n+      if (args.length != 3) {\n         printUsage(opts);\n         return exitCode;\n       }\n", "projectName": "apache.hadoop", "bugLineNum": 200, "bugNodeStartChar": 8073, "bugNodeLength": 16, "fixLineNum": 200, "fixNodeStartChar": 8073, "fixNodeLength": 16, "sourceBeforeFix": "args.length != 2", "sourceAfterFix": "args.length != 3"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "b6eb90370ad063bff5f74d4dc90632fe7ac6ccd3", "fixCommitParentSHA1": "a6dfb7b0453268d332e45f33ffb26ab7a507dee9", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryServer.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryServer.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryServer.java\nindex 3c6d69d..0ad48e2 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryServer.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryServer.java\n@@ -40,7 +40,7 @@\n     Configuration config = new YarnConfiguration();\n     historyServer.init(config);\n     assertEquals(STATE.INITED, historyServer.getServiceState());\n-    assertEquals(3, historyServer.getServices().size());\n+    assertEquals(2, historyServer.getServices().size());\n     ApplicationHistoryClientService historyService = historyServer\n         .getClientService();\n     assertNotNull(historyServer.getClientService());\n", "projectName": "apache.hadoop", "bugLineNum": 43, "bugNodeStartChar": 1699, "bugNodeLength": 51, "fixLineNum": 43, "fixNodeStartChar": 1699, "fixNodeLength": 51, "sourceBeforeFix": "assertEquals(3,historyServer.getServices().size())", "sourceAfterFix": "assertEquals(2,historyServer.getServices().size())"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "a6acbd2403be12f2c9d24a6dc24eea6b0c530c0f", "fixCommitParentSHA1": "858308553f1f83c75dcfee699c70de0c7f80433e", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java\nindex 69aade1..33bf6be 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java\n@@ -1548,7 +1548,7 @@\n       return Subject.doAs(subject, action);\n     } catch (PrivilegedActionException pae) {\n       Throwable cause = pae.getCause();\n-      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n+      LOG.warn(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n       if (cause instanceof IOException) {\n         throw (IOException) cause;\n       } else if (cause instanceof Error) {\n", "projectName": "apache.hadoop", "bugLineNum": 1551, "bugNodeStartChar": 52591, "bugNodeLength": 64, "fixLineNum": 1551, "fixNodeStartChar": 52591, "fixNodeLength": 63, "sourceBeforeFix": "LOG.error(\"PriviledgedActionException as:\" + this + \" cause:\"+ cause)", "sourceAfterFix": "LOG.warn(\"PriviledgedActionException as:\" + this + \" cause:\"+ cause)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "a6acbd2403be12f2c9d24a6dc24eea6b0c530c0f", "fixCommitParentSHA1": "858308553f1f83c75dcfee699c70de0c7f80433e", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java\nindex 69aade1..33bf6be 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java\n@@ -1548,7 +1548,7 @@\n       return Subject.doAs(subject, action);\n     } catch (PrivilegedActionException pae) {\n       Throwable cause = pae.getCause();\n-      LOG.error(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n+      LOG.warn(\"PriviledgedActionException as:\"+this+\" cause:\"+cause);\n       if (cause instanceof IOException) {\n         throw (IOException) cause;\n       } else if (cause instanceof Error) {\n", "projectName": "apache.hadoop", "bugLineNum": 1551, "bugNodeStartChar": 52591, "bugNodeLength": 64, "fixLineNum": 1551, "fixNodeStartChar": 52591, "fixNodeLength": 63, "sourceBeforeFix": "LOG.error(\"PriviledgedActionException as:\" + this + \" cause:\"+ cause)", "sourceAfterFix": "LOG.warn(\"PriviledgedActionException as:\" + this + \" cause:\"+ cause)"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "fc73a88bd3c8a87b50ffcd0770e5d0d893cf0068", "fixCommitParentSHA1": "082d4713bb2e5ae59b4457d5411a983f0992ce88", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java\nindex fc717c2..995d822 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java\n@@ -768,7 +768,7 @@\n   }\n \n   @InterfaceAudience.Private\n-  public void terminateConnection() {\n+  public synchronized void terminateConnection() {\n     if (zkClient == null) {\n       return;\n     }\n", "projectName": "apache.hadoop", "bugLineNum": 770, "bugNodeStartChar": 27298, "bugNodeLength": 425, "fixLineNum": 770, "fixNodeStartChar": 27298, "fixNodeLength": 438, "sourceBeforeFix": "1", "sourceAfterFix": "33"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "05ff546482a53f3f7f145a9d9058e84c50e5187b", "fixCommitParentSHA1": "b99dd5de748d89ae57da159426a098122de334f8", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java\nindex 33230d8..10e7326 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/AdminService.java\n@@ -174,7 +174,7 @@\n     }\n   }\n \n-  private synchronized boolean isRMActive() {\n+  private boolean isRMActive() {\n     return HAServiceState.ACTIVE == rmContext.getHAServiceState();\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 177, "bugNodeStartChar": 7258, "bugNodeLength": 114, "fixLineNum": 177, "fixNodeStartChar": 7258, "fixNodeLength": 101, "sourceBeforeFix": "34", "sourceAfterFix": "2"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "9c0060ead8e75ab1ce6add6768a82b8db16c1f8a", "fixCommitParentSHA1": "02b722fba10653d8e9efec6c5b6d661796406d6f", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\nindex 6e0ed12..f3ab514 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n@@ -3976,13 +3976,13 @@\n    */\n   void renewLease(String holder) throws IOException {\n     checkOperation(OperationCategory.WRITE);\n-    writeLock();\n+    readLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot renew lease for \" + holder);\n       leaseManager.renewLease(holder);\n     } finally {\n-      writeUnlock();\n+      readUnlock();\n     }\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 3979, "bugNodeStartChar": 154332, "bugNodeLength": 11, "fixLineNum": 3979, "fixNodeStartChar": 154332, "fixNodeLength": 10, "sourceBeforeFix": "writeLock()", "sourceAfterFix": "readLock()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "9c0060ead8e75ab1ce6add6768a82b8db16c1f8a", "fixCommitParentSHA1": "02b722fba10653d8e9efec6c5b6d661796406d6f", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\nindex 6e0ed12..f3ab514 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n@@ -3976,13 +3976,13 @@\n    */\n   void renewLease(String holder) throws IOException {\n     checkOperation(OperationCategory.WRITE);\n-    writeLock();\n+    readLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot renew lease for \" + holder);\n       leaseManager.renewLease(holder);\n     } finally {\n-      writeUnlock();\n+      readUnlock();\n     }\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 3979, "bugNodeStartChar": 154332, "bugNodeLength": 11, "fixLineNum": 3979, "fixNodeStartChar": 154332, "fixNodeLength": 10, "sourceBeforeFix": "writeLock()", "sourceAfterFix": "readLock()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "9c0060ead8e75ab1ce6add6768a82b8db16c1f8a", "fixCommitParentSHA1": "02b722fba10653d8e9efec6c5b6d661796406d6f", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\nindex 6e0ed12..f3ab514 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n@@ -3976,13 +3976,13 @@\n    */\n   void renewLease(String holder) throws IOException {\n     checkOperation(OperationCategory.WRITE);\n-    writeLock();\n+    readLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot renew lease for \" + holder);\n       leaseManager.renewLease(holder);\n     } finally {\n-      writeUnlock();\n+      readUnlock();\n     }\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 3985, "bugNodeStartChar": 154528, "bugNodeLength": 13, "fixLineNum": 3985, "fixNodeStartChar": 154528, "fixNodeLength": 12, "sourceBeforeFix": "writeUnlock()", "sourceAfterFix": "readUnlock()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "9c0060ead8e75ab1ce6add6768a82b8db16c1f8a", "fixCommitParentSHA1": "02b722fba10653d8e9efec6c5b6d661796406d6f", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\nindex 6e0ed12..f3ab514 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java\n@@ -3976,13 +3976,13 @@\n    */\n   void renewLease(String holder) throws IOException {\n     checkOperation(OperationCategory.WRITE);\n-    writeLock();\n+    readLock();\n     try {\n       checkOperation(OperationCategory.WRITE);\n       checkNameNodeSafeMode(\"Cannot renew lease for \" + holder);\n       leaseManager.renewLease(holder);\n     } finally {\n-      writeUnlock();\n+      readUnlock();\n     }\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 3985, "bugNodeStartChar": 154528, "bugNodeLength": 13, "fixLineNum": 3985, "fixNodeStartChar": 154528, "fixNodeLength": 12, "sourceBeforeFix": "writeUnlock()", "sourceAfterFix": "readUnlock()"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "bbc30c2607acb6b849d25219bbf07774809d90d7", "fixCommitParentSHA1": "63cb6749bbcea4cffa3ad9f17107d80c81304392", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java\nindex 316168c..4ca9225 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java\n@@ -146,7 +146,7 @@\n     return false;\n   }\n   \n-  String getBlockPoolId() {\n+  synchronized String getBlockPoolId() {\n     if (bpNSInfo != null) {\n       return bpNSInfo.getBlockPoolID();\n     } else {\n@@ -161,7 +161,7 @@\n   }\n   \n   @Override\n-  public String toString() {\n+  public synchronized String toString() {\n     if (bpNSInfo == null) {\n       // If we haven't yet connected to our NN, we don't yet know our\n       // own block pool ID.\n", "projectName": "apache.hadoop", "bugLineNum": 149, "bugNodeStartChar": 5566, "bugNodeLength": 249, "fixLineNum": 149, "fixNodeStartChar": 5566, "fixNodeLength": 262, "sourceBeforeFix": "0", "sourceAfterFix": "32"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "bbc30c2607acb6b849d25219bbf07774809d90d7", "fixCommitParentSHA1": "63cb6749bbcea4cffa3ad9f17107d80c81304392", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java\nindex 316168c..4ca9225 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java\n@@ -146,7 +146,7 @@\n     return false;\n   }\n   \n-  String getBlockPoolId() {\n+  synchronized String getBlockPoolId() {\n     if (bpNSInfo != null) {\n       return bpNSInfo.getBlockPoolID();\n     } else {\n@@ -161,7 +161,7 @@\n   }\n   \n   @Override\n-  public String toString() {\n+  public synchronized String toString() {\n     if (bpNSInfo == null) {\n       // If we haven't yet connected to our NN, we don't yet know our\n       // own block pool ID.\n", "projectName": "apache.hadoop", "bugLineNum": 163, "bugNodeStartChar": 5899, "bugNodeLength": 625, "fixLineNum": 163, "fixNodeStartChar": 5899, "fixNodeLength": 638, "sourceBeforeFix": "1", "sourceAfterFix": "33"}, {"bugType": "OVERLOAD_METHOD_DELETED_ARGS", "fixCommitSHA1": "c03521fd86fa3352841c04e9b0d4c89e4990c02c", "fixCommitParentSHA1": "7b2cd4b41191abc5742d716cdc5b6da197ed4b93", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java\nindex b507f83a..3c51dba 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java\n@@ -226,7 +226,7 @@\n       List<String> retCommand = new ArrayList<String>();\n       retCommand.addAll(Arrays.asList(\"nice\", \"-n\",\n           Integer.toString(containerSchedPriorityAdjustment)));\n-      retCommand.addAll(Arrays.asList(\"bash\", \"-c\", command));\n+      retCommand.addAll(Arrays.asList(\"bash\", command));\n       return retCommand.toArray(new String[retCommand.size()]);\n     }\n   }   \n", "projectName": "apache.hadoop", "bugLineNum": 229, "bugNodeStartChar": 8055, "bugNodeLength": 36, "fixLineNum": 229, "fixNodeStartChar": 8055, "fixNodeLength": 30, "sourceBeforeFix": "Arrays.asList(\"bash\",\"-c\",command)", "sourceAfterFix": "Arrays.asList(\"bash\",command)"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "4ff49a75165cd362b30f586629d3935c964f49dd", "fixCommitParentSHA1": "e26ac59ca080fa2a9455f6b41dbe4e4617af4d13", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ReflectionUtils.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ReflectionUtils.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ReflectionUtils.java\nindex 4fee5f4..be63c81 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ReflectionUtils.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ReflectionUtils.java\n@@ -154,7 +154,7 @@\n    * @param stream the stream to\n    * @param title a string title for the stack trace\n    */\n-  public static void printThreadInfo(PrintWriter stream,\n+  public synchronized static void printThreadInfo(PrintWriter stream,\n                                      String title) {\n     final int STACK_DEPTH = 20;\n     boolean contention = threadBean.isThreadContentionMonitoringEnabled();\n", "projectName": "apache.hadoop", "bugLineNum": 151, "bugNodeStartChar": 5175, "bugNodeLength": 1847, "fixLineNum": 151, "fixNodeStartChar": 5175, "fixNodeLength": 1860, "sourceBeforeFix": "9", "sourceAfterFix": "41"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "47e4c2c121664a1e230785a8bb0767132e1e18bc", "fixCommitParentSHA1": "56cfd507f12db2e2bab1bbef95d295c7a1f8bf4a", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcServer.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcServer.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcServer.java\nindex fffedc1..9408028 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcServer.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcServer.java\n@@ -104,7 +104,7 @@\n         if (LOG.isDebugEnabled())\n           LOG.debug(\"Kerberos principal name is \" + fullName);\n         // don't use KerberosName because we don't want auth_to_local\n-        String[] parts = fullName.split(\"[/@]\", 2);\n+        String[] parts = fullName.split(\"[/@]\", 3);\n         protocol = parts[0];\n         // should verify service host is present here rather than in create()\n         // but lazy tests are using a UGI that isn't a SPN...\n", "projectName": "apache.hadoop", "bugLineNum": 107, "bugNodeStartChar": 3778, "bugNodeLength": 25, "fixLineNum": 107, "fixNodeStartChar": 3778, "fixNodeLength": 25, "sourceBeforeFix": "fullName.split(\"[/@]\",2)", "sourceAfterFix": "fullName.split(\"[/@]\",3)"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "0055e402262a4068722f33359974ca5b72db1511", "fixCommitParentSHA1": "1642f06cd8b2c594f27e9b1ebc41e02d17dea9e9", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java\nindex f343311..8c20b5f 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java\n@@ -159,7 +159,7 @@\n       Preconditions.checkArgument(containerCount > 0,\n           \"The number of containers to request should larger than 0\");\n       Preconditions.checkArgument(\n-              (!relaxLocality && (racks == null || racks.length == 0) \n+              !(!relaxLocality && (racks == null || racks.length == 0) \n                   && (nodes == null || nodes.length == 0)),\n               \"Can't turn off locality relaxation on a \" + \n               \"request with no location constraints\");\n", "projectName": "apache.hadoop", "bugLineNum": 162, "bugNodeStartChar": 7040, "bugNodeLength": 115, "fixLineNum": 162, "fixNodeStartChar": 7040, "fixNodeLength": 116, "sourceBeforeFix": "(!relaxLocality && (racks == null || racks.length == 0) && (nodes == null || nodes.length == 0))", "sourceAfterFix": "!(!relaxLocality && (racks == null || racks.length == 0) && (nodes == null || nodes.length == 0))"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "f3c4f2b3e22645f15032757e239cdcae6f2c5a37", "fixCommitParentSHA1": "f09946bd47df6a9269d6b733ccbd42987c0075ef", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java\nindex af358e4..b9af3af 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java\n@@ -153,7 +153,7 @@\n     new JobTokenSecretManager();\n \n   public static final String SHUFFLE_PORT_CONFIG_KEY = \"mapreduce.shuffle.port\";\n-  public static final int DEFAULT_SHUFFLE_PORT = 11000;\n+  public static final int DEFAULT_SHUFFLE_PORT = 13562;\n \n   public static final String SUFFLE_SSL_FILE_BUFFER_SIZE_KEY =\n     \"mapreduce.shuffle.ssl.file.buffer.size\";\n", "projectName": "apache.hadoop", "bugLineNum": 156, "bugNodeStartChar": 7197, "bugNodeLength": 28, "fixLineNum": 156, "fixNodeStartChar": 7197, "fixNodeLength": 28, "sourceBeforeFix": "DEFAULT_SHUFFLE_PORT=11000", "sourceAfterFix": "DEFAULT_SHUFFLE_PORT=13562"}, {"bugType": "CHANGE_OPERAND", "fixCommitSHA1": "6bd7b27abf18fdfb301a7c95e1d2e7888f884c48", "fixCommitParentSHA1": "4519131f77ee7cc48d09819b3919a469514a41d1", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\nindex d13c7dc..cab6dd7 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n@@ -190,7 +190,7 @@\n    * balancing purpose at a datanode\n    */\n   public static final int MAX_NUM_CONCURRENT_MOVES = 5;\n-  public static final int MAX_NO_PENDING_BLOCK_INTERATIONS = 5;\n+  private static final int MAX_NO_PENDING_BLOCK_ITERATIONS = 5;\n   \n   private static final String USAGE = \"Usage: java \"\n       + Balancer.class.getSimpleName()\n@@ -782,7 +782,7 @@\n           noPendingBlockIteration++;\n           // in case no blocks can be moved for source node's task,\n           // jump out of while-loop after 5 iterations.\n-          if (noPendingBlockIteration >= MAX_NO_PENDING_BLOCK_INTERATIONS) {\n+          if (noPendingBlockIteration >= MAX_NO_PENDING_BLOCK_ITERATIONS) {\n             scheduledSize = 0;\n           }\n         }\n", "projectName": "apache.hadoop", "bugLineNum": 785, "bugNodeStartChar": 29421, "bugNodeLength": 59, "fixLineNum": 785, "fixNodeStartChar": 29421, "fixNodeLength": 58, "sourceBeforeFix": "noPendingBlockIteration >= MAX_NO_PENDING_BLOCK_INTERATIONS", "sourceAfterFix": "noPendingBlockIteration >= MAX_NO_PENDING_BLOCK_ITERATIONS"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "6bd7b27abf18fdfb301a7c95e1d2e7888f884c48", "fixCommitParentSHA1": "4519131f77ee7cc48d09819b3919a469514a41d1", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\nindex d13c7dc..cab6dd7 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java\n@@ -190,7 +190,7 @@\n    * balancing purpose at a datanode\n    */\n   public static final int MAX_NUM_CONCURRENT_MOVES = 5;\n-  public static final int MAX_NO_PENDING_BLOCK_INTERATIONS = 5;\n+  private static final int MAX_NO_PENDING_BLOCK_ITERATIONS = 5;\n   \n   private static final String USAGE = \"Usage: java \"\n       + Balancer.class.getSimpleName()\n@@ -782,7 +782,7 @@\n           noPendingBlockIteration++;\n           // in case no blocks can be moved for source node's task,\n           // jump out of while-loop after 5 iterations.\n-          if (noPendingBlockIteration >= MAX_NO_PENDING_BLOCK_INTERATIONS) {\n+          if (noPendingBlockIteration >= MAX_NO_PENDING_BLOCK_ITERATIONS) {\n             scheduledSize = 0;\n           }\n         }\n", "projectName": "apache.hadoop", "bugLineNum": 785, "bugNodeStartChar": 29421, "bugNodeLength": 59, "fixLineNum": 785, "fixNodeStartChar": 29421, "fixNodeLength": 58, "sourceBeforeFix": "noPendingBlockIteration >= MAX_NO_PENDING_BLOCK_INTERATIONS", "sourceAfterFix": "noPendingBlockIteration >= MAX_NO_PENDING_BLOCK_ITERATIONS"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "419fb4a4dee3cd001631daa06fe6b8c30095b468", "fixCommitParentSHA1": "dbe14af2997bc076aecccfd8eace039fde84e43c", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java\nindex 7d97c97..9d9cde4 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java\n@@ -251,7 +251,7 @@\n                                                   ) throws IOException {\n     try {\n       //Renew TGT if needed\n-      ugi.reloginFromKeytab();\n+      ugi.checkTGTAndReloginFromKeytab();\n       return ugi.doAs(new PrivilegedExceptionAction<Token<?>>() {\n         @Override\n         public Token<?> run() throws IOException {\n@@ -704,7 +704,7 @@\n     public long renew(Token<?> token, \n                       Configuration conf) throws IOException {\n       // update the kerberos credentials, if they are coming from a keytab\n-      UserGroupInformation.getLoginUser().reloginFromKeytab();\n+      UserGroupInformation.getLoginUser().checkTGTAndReloginFromKeytab();\n       // use http to renew the token\n       InetSocketAddress serviceAddr = SecurityUtil.getTokenServiceAddr(token);\n       return \n", "projectName": "apache.hadoop", "bugLineNum": 254, "bugNodeStartChar": 8744, "bugNodeLength": 23, "fixLineNum": 254, "fixNodeStartChar": 8744, "fixNodeLength": 34, "sourceBeforeFix": "ugi.reloginFromKeytab()", "sourceAfterFix": "ugi.checkTGTAndReloginFromKeytab()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "419fb4a4dee3cd001631daa06fe6b8c30095b468", "fixCommitParentSHA1": "dbe14af2997bc076aecccfd8eace039fde84e43c", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java\nindex 7d97c97..9d9cde4 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java\n@@ -251,7 +251,7 @@\n                                                   ) throws IOException {\n     try {\n       //Renew TGT if needed\n-      ugi.reloginFromKeytab();\n+      ugi.checkTGTAndReloginFromKeytab();\n       return ugi.doAs(new PrivilegedExceptionAction<Token<?>>() {\n         @Override\n         public Token<?> run() throws IOException {\n@@ -704,7 +704,7 @@\n     public long renew(Token<?> token, \n                       Configuration conf) throws IOException {\n       // update the kerberos credentials, if they are coming from a keytab\n-      UserGroupInformation.getLoginUser().reloginFromKeytab();\n+      UserGroupInformation.getLoginUser().checkTGTAndReloginFromKeytab();\n       // use http to renew the token\n       InetSocketAddress serviceAddr = SecurityUtil.getTokenServiceAddr(token);\n       return \n", "projectName": "apache.hadoop", "bugLineNum": 254, "bugNodeStartChar": 8744, "bugNodeLength": 23, "fixLineNum": 254, "fixNodeStartChar": 8744, "fixNodeLength": 34, "sourceBeforeFix": "ugi.reloginFromKeytab()", "sourceAfterFix": "ugi.checkTGTAndReloginFromKeytab()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "419fb4a4dee3cd001631daa06fe6b8c30095b468", "fixCommitParentSHA1": "dbe14af2997bc076aecccfd8eace039fde84e43c", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java\nindex 7d97c97..9d9cde4 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java\n@@ -251,7 +251,7 @@\n                                                   ) throws IOException {\n     try {\n       //Renew TGT if needed\n-      ugi.reloginFromKeytab();\n+      ugi.checkTGTAndReloginFromKeytab();\n       return ugi.doAs(new PrivilegedExceptionAction<Token<?>>() {\n         @Override\n         public Token<?> run() throws IOException {\n@@ -704,7 +704,7 @@\n     public long renew(Token<?> token, \n                       Configuration conf) throws IOException {\n       // update the kerberos credentials, if they are coming from a keytab\n-      UserGroupInformation.getLoginUser().reloginFromKeytab();\n+      UserGroupInformation.getLoginUser().checkTGTAndReloginFromKeytab();\n       // use http to renew the token\n       InetSocketAddress serviceAddr = SecurityUtil.getTokenServiceAddr(token);\n       return \n", "projectName": "apache.hadoop", "bugLineNum": 707, "bugNodeStartChar": 24207, "bugNodeLength": 55, "fixLineNum": 707, "fixNodeStartChar": 24207, "fixNodeLength": 66, "sourceBeforeFix": "UserGroupInformation.getLoginUser().reloginFromKeytab()", "sourceAfterFix": "UserGroupInformation.getLoginUser().checkTGTAndReloginFromKeytab()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "419fb4a4dee3cd001631daa06fe6b8c30095b468", "fixCommitParentSHA1": "dbe14af2997bc076aecccfd8eace039fde84e43c", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java\nindex 7d97c97..9d9cde4 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HftpFileSystem.java\n@@ -251,7 +251,7 @@\n                                                   ) throws IOException {\n     try {\n       //Renew TGT if needed\n-      ugi.reloginFromKeytab();\n+      ugi.checkTGTAndReloginFromKeytab();\n       return ugi.doAs(new PrivilegedExceptionAction<Token<?>>() {\n         @Override\n         public Token<?> run() throws IOException {\n@@ -704,7 +704,7 @@\n     public long renew(Token<?> token, \n                       Configuration conf) throws IOException {\n       // update the kerberos credentials, if they are coming from a keytab\n-      UserGroupInformation.getLoginUser().reloginFromKeytab();\n+      UserGroupInformation.getLoginUser().checkTGTAndReloginFromKeytab();\n       // use http to renew the token\n       InetSocketAddress serviceAddr = SecurityUtil.getTokenServiceAddr(token);\n       return \n", "projectName": "apache.hadoop", "bugLineNum": 707, "bugNodeStartChar": 24207, "bugNodeLength": 55, "fixLineNum": 707, "fixNodeStartChar": 24207, "fixNodeLength": 66, "sourceBeforeFix": "UserGroupInformation.getLoginUser().reloginFromKeytab()", "sourceAfterFix": "UserGroupInformation.getLoginUser().checkTGTAndReloginFromKeytab()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "419fb4a4dee3cd001631daa06fe6b8c30095b468", "fixCommitParentSHA1": "dbe14af2997bc076aecccfd8eace039fde84e43c", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/GetImageServlet.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/GetImageServlet.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/GetImageServlet.java\nindex dcec0ef..dfe1c6a 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/GetImageServlet.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/GetImageServlet.java\n@@ -157,7 +157,7 @@\n               \n               // We may have lost our ticket since last checkpoint, log in again, just in case\n               if (UserGroupInformation.isSecurityEnabled()) {\n-                UserGroupInformation.getCurrentUser().reloginFromKeytab();\n+                UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();\n               }\n               \n               // issue a HTTP get request to download the new fsimage \n", "projectName": "apache.hadoop", "bugLineNum": 160, "bugNodeStartChar": 7085, "bugNodeLength": 57, "fixLineNum": 160, "fixNodeStartChar": 7085, "fixNodeLength": 68, "sourceBeforeFix": "UserGroupInformation.getCurrentUser().reloginFromKeytab()", "sourceAfterFix": "UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "419fb4a4dee3cd001631daa06fe6b8c30095b468", "fixCommitParentSHA1": "dbe14af2997bc076aecccfd8eace039fde84e43c", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/GetImageServlet.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/GetImageServlet.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/GetImageServlet.java\nindex dcec0ef..dfe1c6a 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/GetImageServlet.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/GetImageServlet.java\n@@ -157,7 +157,7 @@\n               \n               // We may have lost our ticket since last checkpoint, log in again, just in case\n               if (UserGroupInformation.isSecurityEnabled()) {\n-                UserGroupInformation.getCurrentUser().reloginFromKeytab();\n+                UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();\n               }\n               \n               // issue a HTTP get request to download the new fsimage \n", "projectName": "apache.hadoop", "bugLineNum": 160, "bugNodeStartChar": 7085, "bugNodeLength": 57, "fixLineNum": 160, "fixNodeStartChar": 7085, "fixNodeLength": 68, "sourceBeforeFix": "UserGroupInformation.getCurrentUser().reloginFromKeytab()", "sourceAfterFix": "UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "419fb4a4dee3cd001631daa06fe6b8c30095b468", "fixCommitParentSHA1": "dbe14af2997bc076aecccfd8eace039fde84e43c", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java\nindex 4384126..b370c39 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java\n@@ -371,7 +371,7 @@\n       try {\n         // We may have lost our ticket since last checkpoint, log in again, just in case\n         if(UserGroupInformation.isSecurityEnabled())\n-          UserGroupInformation.getCurrentUser().reloginFromKeytab();\n+          UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();\n         \n         long now = Time.now();\n \n", "projectName": "apache.hadoop", "bugLineNum": 374, "bugNodeStartChar": 13524, "bugNodeLength": 57, "fixLineNum": 374, "fixNodeStartChar": 13524, "fixNodeLength": 68, "sourceBeforeFix": "UserGroupInformation.getCurrentUser().reloginFromKeytab()", "sourceAfterFix": "UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "419fb4a4dee3cd001631daa06fe6b8c30095b468", "fixCommitParentSHA1": "dbe14af2997bc076aecccfd8eace039fde84e43c", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java\nindex 4384126..b370c39 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java\n@@ -371,7 +371,7 @@\n       try {\n         // We may have lost our ticket since last checkpoint, log in again, just in case\n         if(UserGroupInformation.isSecurityEnabled())\n-          UserGroupInformation.getCurrentUser().reloginFromKeytab();\n+          UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();\n         \n         long now = Time.now();\n \n", "projectName": "apache.hadoop", "bugLineNum": 374, "bugNodeStartChar": 13524, "bugNodeLength": 57, "fixLineNum": 374, "fixNodeStartChar": 13524, "fixNodeLength": 68, "sourceBeforeFix": "UserGroupInformation.getCurrentUser().reloginFromKeytab()", "sourceAfterFix": "UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "419fb4a4dee3cd001631daa06fe6b8c30095b468", "fixCommitParentSHA1": "dbe14af2997bc076aecccfd8eace039fde84e43c", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java\nindex a97e55d..46f13f0 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java\n@@ -262,7 +262,7 @@\n         try {\n           // We may have lost our ticket since last checkpoint, log in again, just in case\n           if (UserGroupInformation.isSecurityEnabled()) {\n-            UserGroupInformation.getCurrentUser().reloginFromKeytab();\n+            UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();\n           }\n           \n           long now = now();\n", "projectName": "apache.hadoop", "bugLineNum": 265, "bugNodeStartChar": 9547, "bugNodeLength": 57, "fixLineNum": 265, "fixNodeStartChar": 9547, "fixNodeLength": 68, "sourceBeforeFix": "UserGroupInformation.getCurrentUser().reloginFromKeytab()", "sourceAfterFix": "UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "419fb4a4dee3cd001631daa06fe6b8c30095b468", "fixCommitParentSHA1": "dbe14af2997bc076aecccfd8eace039fde84e43c", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java\nindex a97e55d..46f13f0 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java\n@@ -262,7 +262,7 @@\n         try {\n           // We may have lost our ticket since last checkpoint, log in again, just in case\n           if (UserGroupInformation.isSecurityEnabled()) {\n-            UserGroupInformation.getCurrentUser().reloginFromKeytab();\n+            UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab();\n           }\n           \n           long now = now();\n", "projectName": "apache.hadoop", "bugLineNum": 265, "bugNodeStartChar": 9547, "bugNodeLength": 57, "fixLineNum": 265, "fixNodeStartChar": 9547, "fixNodeLength": 68, "sourceBeforeFix": "UserGroupInformation.getCurrentUser().reloginFromKeytab()", "sourceAfterFix": "UserGroupInformation.getCurrentUser().checkTGTAndReloginFromKeytab()"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "0411165008651e59f6ca01070c5ac514b508364e", "fixCommitParentSHA1": "bd198d7dfeb5580520dca90645d35b7757237dc9", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java\nindex 0beb430..ad6c3bb 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/src/main/java/org/apache/hadoop/mapred/ShuffleHandler.java\n@@ -153,7 +153,7 @@\n     new JobTokenSecretManager();\n \n   public static final String SHUFFLE_PORT_CONFIG_KEY = \"mapreduce.shuffle.port\";\n-  public static final int DEFAULT_SHUFFLE_PORT = 8080;\n+  public static final int DEFAULT_SHUFFLE_PORT = 11000;\n \n   public static final String SUFFLE_SSL_FILE_BUFFER_SIZE_KEY =\n     \"mapreduce.shuffle.ssl.file.buffer.size\";\n", "projectName": "apache.hadoop", "bugLineNum": 156, "bugNodeStartChar": 7198, "bugNodeLength": 27, "fixLineNum": 156, "fixNodeStartChar": 7198, "fixNodeLength": 28, "sourceBeforeFix": "DEFAULT_SHUFFLE_PORT=8080", "sourceAfterFix": "DEFAULT_SHUFFLE_PORT=11000"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "e11dd72dc1b56e49d14bde084a77e894cfdea206", "fixCommitParentSHA1": "d5331b18a910844e30b3d6ccb36c08d982c13114", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java\nindex 45a464e..66b3663 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java\n@@ -348,7 +348,7 @@\n       String hadoopBuildVersion, String hadoopVersion, long startedon) {\n     WebServicesTestUtils.checkStringMatch(\"hadoopVersionBuiltOn\",\n         VersionInfo.getDate(), hadoopVersionBuiltOn);\n-    WebServicesTestUtils.checkStringMatch(\"hadoopBuildVersion\",\n+    WebServicesTestUtils.checkStringEqual(\"hadoopBuildVersion\",\n         VersionInfo.getBuildVersion(), hadoopBuildVersion);\n     WebServicesTestUtils.checkStringMatch(\"hadoopVersion\",\n         VersionInfo.getVersion(), hadoopVersion);\n", "projectName": "apache.hadoop", "bugLineNum": 351, "bugNodeStartChar": 12469, "bugNodeLength": 118, "fixLineNum": 351, "fixNodeStartChar": 12469, "fixNodeLength": 118, "sourceBeforeFix": "WebServicesTestUtils.checkStringMatch(\"hadoopBuildVersion\",VersionInfo.getBuildVersion(),hadoopBuildVersion)", "sourceAfterFix": "WebServicesTestUtils.checkStringEqual(\"hadoopBuildVersion\",VersionInfo.getBuildVersion(),hadoopBuildVersion)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "e11dd72dc1b56e49d14bde084a77e894cfdea206", "fixCommitParentSHA1": "d5331b18a910844e30b3d6ccb36c08d982c13114", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java\nindex 45a464e..66b3663 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/webapp/TestHsWebServices.java\n@@ -348,7 +348,7 @@\n       String hadoopBuildVersion, String hadoopVersion, long startedon) {\n     WebServicesTestUtils.checkStringMatch(\"hadoopVersionBuiltOn\",\n         VersionInfo.getDate(), hadoopVersionBuiltOn);\n-    WebServicesTestUtils.checkStringMatch(\"hadoopBuildVersion\",\n+    WebServicesTestUtils.checkStringEqual(\"hadoopBuildVersion\",\n         VersionInfo.getBuildVersion(), hadoopBuildVersion);\n     WebServicesTestUtils.checkStringMatch(\"hadoopVersion\",\n         VersionInfo.getVersion(), hadoopVersion);\n", "projectName": "apache.hadoop", "bugLineNum": 351, "bugNodeStartChar": 12469, "bugNodeLength": 118, "fixLineNum": 351, "fixNodeStartChar": 12469, "fixNodeLength": 118, "sourceBeforeFix": "WebServicesTestUtils.checkStringMatch(\"hadoopBuildVersion\",VersionInfo.getBuildVersion(),hadoopBuildVersion)", "sourceAfterFix": "WebServicesTestUtils.checkStringEqual(\"hadoopBuildVersion\",VersionInfo.getBuildVersion(),hadoopBuildVersion)"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "af07edf1ab9024a78cb280e85982b1d6661a09ad", "fixCommitParentSHA1": "937ef703ebc4d08fdd772baf66a26f6c12b8dfe3", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\nindex 3bfc772..67b73ce 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n@@ -67,7 +67,8 @@\n    * \n    * @param queueName Name of the job queue.\n    */\n-  protected void setQueueName(String queueName) {\n+  @InterfaceAudience.Private\n+  public void setQueueName(String queueName) {\n     super.setQueueName(queueName);\n   }\n \n@@ -76,7 +77,8 @@\n    * \n    * @param schedulingInfo\n    */\n-  protected void setSchedulingInfo(String schedulingInfo) {\n+  @InterfaceAudience.Private\n+  public void setSchedulingInfo(String schedulingInfo) {\n     super.setSchedulingInfo(schedulingInfo);\n   }\n \n@@ -84,15 +86,21 @@\n    * Set the state of the queue\n    * @param state state of the queue.\n    */\n-  protected void setQueueState(String state) {\n+  @InterfaceAudience.Private\n+  public void setQueueState(String state) {\n     super.setState(QueueState.getState(state));\n   }\n   \n-  String getQueueState() {\n+  /**\n+   * Use getState() instead\n+   */\n+  @Deprecated\n+  public String getQueueState() {\n     return super.getState().toString();\n   }\n   \n-  protected void setChildren(List<JobQueueInfo> children) {\n+  @InterfaceAudience.Private\n+  public void setChildren(List<JobQueueInfo> children) {\n     List<QueueInfo> list = new ArrayList<QueueInfo>();\n     for (JobQueueInfo q : children) {\n       list.add(q);\n@@ -108,7 +116,8 @@\n     return list;\n   }\n \n-  protected void setProperties(Properties props) {\n+  @InterfaceAudience.Private\n+  public void setProperties(Properties props) {\n     super.setProperties(props);\n   }\n \n@@ -141,7 +150,8 @@\n     setChildren(children);\n   }\n \n-  protected void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n+  @InterfaceAudience.Private\n+  public void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n     super.setJobStatuses(stats);\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 65, "bugNodeStartChar": 2129, "bugNodeLength": 193, "fixLineNum": 65, "fixNodeStartChar": 2129, "fixNodeLength": 219, "sourceBeforeFix": "4", "sourceAfterFix": "1"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "af07edf1ab9024a78cb280e85982b1d6661a09ad", "fixCommitParentSHA1": "937ef703ebc4d08fdd772baf66a26f6c12b8dfe3", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\nindex 3bfc772..67b73ce 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n@@ -67,7 +67,8 @@\n    * \n    * @param queueName Name of the job queue.\n    */\n-  protected void setQueueName(String queueName) {\n+  @InterfaceAudience.Private\n+  public void setQueueName(String queueName) {\n     super.setQueueName(queueName);\n   }\n \n@@ -76,7 +77,8 @@\n    * \n    * @param schedulingInfo\n    */\n-  protected void setSchedulingInfo(String schedulingInfo) {\n+  @InterfaceAudience.Private\n+  public void setSchedulingInfo(String schedulingInfo) {\n     super.setSchedulingInfo(schedulingInfo);\n   }\n \n@@ -84,15 +86,21 @@\n    * Set the state of the queue\n    * @param state state of the queue.\n    */\n-  protected void setQueueState(String state) {\n+  @InterfaceAudience.Private\n+  public void setQueueState(String state) {\n     super.setState(QueueState.getState(state));\n   }\n   \n-  String getQueueState() {\n+  /**\n+   * Use getState() instead\n+   */\n+  @Deprecated\n+  public String getQueueState() {\n     return super.getState().toString();\n   }\n   \n-  protected void setChildren(List<JobQueueInfo> children) {\n+  @InterfaceAudience.Private\n+  public void setChildren(List<JobQueueInfo> children) {\n     List<QueueInfo> list = new ArrayList<QueueInfo>();\n     for (JobQueueInfo q : children) {\n       list.add(q);\n@@ -108,7 +116,8 @@\n     return list;\n   }\n \n-  protected void setProperties(Properties props) {\n+  @InterfaceAudience.Private\n+  public void setProperties(Properties props) {\n     super.setProperties(props);\n   }\n \n@@ -141,7 +150,8 @@\n     setChildren(children);\n   }\n \n-  protected void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n+  @InterfaceAudience.Private\n+  public void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n     super.setJobStatuses(stats);\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 74, "bugNodeStartChar": 2326, "bugNodeLength": 222, "fixLineNum": 74, "fixNodeStartChar": 2326, "fixNodeLength": 248, "sourceBeforeFix": "4", "sourceAfterFix": "1"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "af07edf1ab9024a78cb280e85982b1d6661a09ad", "fixCommitParentSHA1": "937ef703ebc4d08fdd772baf66a26f6c12b8dfe3", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\nindex 3bfc772..67b73ce 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n@@ -67,7 +67,8 @@\n    * \n    * @param queueName Name of the job queue.\n    */\n-  protected void setQueueName(String queueName) {\n+  @InterfaceAudience.Private\n+  public void setQueueName(String queueName) {\n     super.setQueueName(queueName);\n   }\n \n@@ -76,7 +77,8 @@\n    * \n    * @param schedulingInfo\n    */\n-  protected void setSchedulingInfo(String schedulingInfo) {\n+  @InterfaceAudience.Private\n+  public void setSchedulingInfo(String schedulingInfo) {\n     super.setSchedulingInfo(schedulingInfo);\n   }\n \n@@ -84,15 +86,21 @@\n    * Set the state of the queue\n    * @param state state of the queue.\n    */\n-  protected void setQueueState(String state) {\n+  @InterfaceAudience.Private\n+  public void setQueueState(String state) {\n     super.setState(QueueState.getState(state));\n   }\n   \n-  String getQueueState() {\n+  /**\n+   * Use getState() instead\n+   */\n+  @Deprecated\n+  public String getQueueState() {\n     return super.getState().toString();\n   }\n   \n-  protected void setChildren(List<JobQueueInfo> children) {\n+  @InterfaceAudience.Private\n+  public void setChildren(List<JobQueueInfo> children) {\n     List<QueueInfo> list = new ArrayList<QueueInfo>();\n     for (JobQueueInfo q : children) {\n       list.add(q);\n@@ -108,7 +116,8 @@\n     return list;\n   }\n \n-  protected void setProperties(Properties props) {\n+  @InterfaceAudience.Private\n+  public void setProperties(Properties props) {\n     super.setProperties(props);\n   }\n \n@@ -141,7 +150,8 @@\n     setChildren(children);\n   }\n \n-  protected void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n+  @InterfaceAudience.Private\n+  public void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n     super.setJobStatuses(stats);\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 83, "bugNodeStartChar": 2552, "bugNodeLength": 178, "fixLineNum": 83, "fixNodeStartChar": 2552, "fixNodeLength": 204, "sourceBeforeFix": "4", "sourceAfterFix": "1"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "af07edf1ab9024a78cb280e85982b1d6661a09ad", "fixCommitParentSHA1": "937ef703ebc4d08fdd772baf66a26f6c12b8dfe3", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\nindex 3bfc772..67b73ce 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n@@ -67,7 +67,8 @@\n    * \n    * @param queueName Name of the job queue.\n    */\n-  protected void setQueueName(String queueName) {\n+  @InterfaceAudience.Private\n+  public void setQueueName(String queueName) {\n     super.setQueueName(queueName);\n   }\n \n@@ -76,7 +77,8 @@\n    * \n    * @param schedulingInfo\n    */\n-  protected void setSchedulingInfo(String schedulingInfo) {\n+  @InterfaceAudience.Private\n+  public void setSchedulingInfo(String schedulingInfo) {\n     super.setSchedulingInfo(schedulingInfo);\n   }\n \n@@ -84,15 +86,21 @@\n    * Set the state of the queue\n    * @param state state of the queue.\n    */\n-  protected void setQueueState(String state) {\n+  @InterfaceAudience.Private\n+  public void setQueueState(String state) {\n     super.setState(QueueState.getState(state));\n   }\n   \n-  String getQueueState() {\n+  /**\n+   * Use getState() instead\n+   */\n+  @Deprecated\n+  public String getQueueState() {\n     return super.getState().toString();\n   }\n   \n-  protected void setChildren(List<JobQueueInfo> children) {\n+  @InterfaceAudience.Private\n+  public void setChildren(List<JobQueueInfo> children) {\n     List<QueueInfo> list = new ArrayList<QueueInfo>();\n     for (JobQueueInfo q : children) {\n       list.add(q);\n@@ -108,7 +116,8 @@\n     return list;\n   }\n \n-  protected void setProperties(Properties props) {\n+  @InterfaceAudience.Private\n+  public void setProperties(Properties props) {\n     super.setProperties(props);\n   }\n \n@@ -141,7 +150,8 @@\n     setChildren(children);\n   }\n \n-  protected void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n+  @InterfaceAudience.Private\n+  public void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n     super.setJobStatuses(stats);\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 91, "bugNodeStartChar": 2736, "bugNodeLength": 68, "fixLineNum": 91, "fixNodeStartChar": 2736, "fixNodeLength": 129, "sourceBeforeFix": "0", "sourceAfterFix": "1"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "af07edf1ab9024a78cb280e85982b1d6661a09ad", "fixCommitParentSHA1": "937ef703ebc4d08fdd772baf66a26f6c12b8dfe3", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\nindex 3bfc772..67b73ce 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n@@ -67,7 +67,8 @@\n    * \n    * @param queueName Name of the job queue.\n    */\n-  protected void setQueueName(String queueName) {\n+  @InterfaceAudience.Private\n+  public void setQueueName(String queueName) {\n     super.setQueueName(queueName);\n   }\n \n@@ -76,7 +77,8 @@\n    * \n    * @param schedulingInfo\n    */\n-  protected void setSchedulingInfo(String schedulingInfo) {\n+  @InterfaceAudience.Private\n+  public void setSchedulingInfo(String schedulingInfo) {\n     super.setSchedulingInfo(schedulingInfo);\n   }\n \n@@ -84,15 +86,21 @@\n    * Set the state of the queue\n    * @param state state of the queue.\n    */\n-  protected void setQueueState(String state) {\n+  @InterfaceAudience.Private\n+  public void setQueueState(String state) {\n     super.setState(QueueState.getState(state));\n   }\n   \n-  String getQueueState() {\n+  /**\n+   * Use getState() instead\n+   */\n+  @Deprecated\n+  public String getQueueState() {\n     return super.getState().toString();\n   }\n   \n-  protected void setChildren(List<JobQueueInfo> children) {\n+  @InterfaceAudience.Private\n+  public void setChildren(List<JobQueueInfo> children) {\n     List<QueueInfo> list = new ArrayList<QueueInfo>();\n     for (JobQueueInfo q : children) {\n       list.add(q);\n@@ -108,7 +116,8 @@\n     return list;\n   }\n \n-  protected void setProperties(Properties props) {\n+  @InterfaceAudience.Private\n+  public void setProperties(Properties props) {\n     super.setProperties(props);\n   }\n \n@@ -141,7 +150,8 @@\n     setChildren(children);\n   }\n \n-  protected void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n+  @InterfaceAudience.Private\n+  public void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n     super.setJobStatuses(stats);\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 95, "bugNodeStartChar": 2810, "bugNodeLength": 213, "fixLineNum": 95, "fixNodeStartChar": 2810, "fixNodeLength": 239, "sourceBeforeFix": "4", "sourceAfterFix": "1"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "af07edf1ab9024a78cb280e85982b1d6661a09ad", "fixCommitParentSHA1": "937ef703ebc4d08fdd772baf66a26f6c12b8dfe3", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\nindex 3bfc772..67b73ce 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n@@ -67,7 +67,8 @@\n    * \n    * @param queueName Name of the job queue.\n    */\n-  protected void setQueueName(String queueName) {\n+  @InterfaceAudience.Private\n+  public void setQueueName(String queueName) {\n     super.setQueueName(queueName);\n   }\n \n@@ -76,7 +77,8 @@\n    * \n    * @param schedulingInfo\n    */\n-  protected void setSchedulingInfo(String schedulingInfo) {\n+  @InterfaceAudience.Private\n+  public void setSchedulingInfo(String schedulingInfo) {\n     super.setSchedulingInfo(schedulingInfo);\n   }\n \n@@ -84,15 +86,21 @@\n    * Set the state of the queue\n    * @param state state of the queue.\n    */\n-  protected void setQueueState(String state) {\n+  @InterfaceAudience.Private\n+  public void setQueueState(String state) {\n     super.setState(QueueState.getState(state));\n   }\n   \n-  String getQueueState() {\n+  /**\n+   * Use getState() instead\n+   */\n+  @Deprecated\n+  public String getQueueState() {\n     return super.getState().toString();\n   }\n   \n-  protected void setChildren(List<JobQueueInfo> children) {\n+  @InterfaceAudience.Private\n+  public void setChildren(List<JobQueueInfo> children) {\n     List<QueueInfo> list = new ArrayList<QueueInfo>();\n     for (JobQueueInfo q : children) {\n       list.add(q);\n@@ -108,7 +116,8 @@\n     return list;\n   }\n \n-  protected void setProperties(Properties props) {\n+  @InterfaceAudience.Private\n+  public void setProperties(Properties props) {\n     super.setProperties(props);\n   }\n \n@@ -141,7 +150,8 @@\n     setChildren(children);\n   }\n \n-  protected void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n+  @InterfaceAudience.Private\n+  public void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n     super.setJobStatuses(stats);\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 111, "bugNodeStartChar": 3244, "bugNodeLength": 84, "fixLineNum": 111, "fixNodeStartChar": 3244, "fixNodeLength": 110, "sourceBeforeFix": "4", "sourceAfterFix": "1"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "af07edf1ab9024a78cb280e85982b1d6661a09ad", "fixCommitParentSHA1": "937ef703ebc4d08fdd772baf66a26f6c12b8dfe3", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\nindex 3bfc772..67b73ce 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobQueueInfo.java\n@@ -67,7 +67,8 @@\n    * \n    * @param queueName Name of the job queue.\n    */\n-  protected void setQueueName(String queueName) {\n+  @InterfaceAudience.Private\n+  public void setQueueName(String queueName) {\n     super.setQueueName(queueName);\n   }\n \n@@ -76,7 +77,8 @@\n    * \n    * @param schedulingInfo\n    */\n-  protected void setSchedulingInfo(String schedulingInfo) {\n+  @InterfaceAudience.Private\n+  public void setSchedulingInfo(String schedulingInfo) {\n     super.setSchedulingInfo(schedulingInfo);\n   }\n \n@@ -84,15 +86,21 @@\n    * Set the state of the queue\n    * @param state state of the queue.\n    */\n-  protected void setQueueState(String state) {\n+  @InterfaceAudience.Private\n+  public void setQueueState(String state) {\n     super.setState(QueueState.getState(state));\n   }\n   \n-  String getQueueState() {\n+  /**\n+   * Use getState() instead\n+   */\n+  @Deprecated\n+  public String getQueueState() {\n     return super.getState().toString();\n   }\n   \n-  protected void setChildren(List<JobQueueInfo> children) {\n+  @InterfaceAudience.Private\n+  public void setChildren(List<JobQueueInfo> children) {\n     List<QueueInfo> list = new ArrayList<QueueInfo>();\n     for (JobQueueInfo q : children) {\n       list.add(q);\n@@ -108,7 +116,8 @@\n     return list;\n   }\n \n-  protected void setProperties(Properties props) {\n+  @InterfaceAudience.Private\n+  public void setProperties(Properties props) {\n     super.setProperties(props);\n   }\n \n@@ -141,7 +150,8 @@\n     setChildren(children);\n   }\n \n-  protected void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n+  @InterfaceAudience.Private\n+  public void setJobStatuses(org.apache.hadoop.mapreduce.JobStatus[] stats) {\n     super.setJobStatuses(stats);\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 144, "bugNodeStartChar": 4082, "bugNodeLength": 115, "fixLineNum": 144, "fixNodeStartChar": 4082, "fixNodeLength": 141, "sourceBeforeFix": "4", "sourceAfterFix": "1"}, {"bugType": "SWAP_BOOLEAN_LITERAL", "fixCommitSHA1": "c3448fe289734ed7498eee4444af630f7abd6986", "fixCommitParentSHA1": "0e1b19a1116a7ff61dd35bb8d8a9dff01c4cdc60", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java\nindex 8a7f7fc..f5706d9 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairSchedulerConfiguration.java\n@@ -18,7 +18,7 @@\n   /** Whether to use the user name as the queue name (instead of \"default\") if\n    * the request does not specify a queue. */\n   protected static final String  USER_AS_DEFAULT_QUEUE = CONF_PREFIX + \"user-as-default-queue\";\n-  protected static final boolean DEFAULT_USER_AS_DEFAULT_QUEUE = false;\n+  protected static final boolean DEFAULT_USER_AS_DEFAULT_QUEUE = true;\n \n   protected static final String LOCALITY_THRESHOLD = CONF_PREFIX + \"locality.threshold\";\n   protected static final float  DEFAULT_LOCALITY_THRESHOLD = -1.0f;\n", "projectName": "apache.hadoop", "bugLineNum": 21, "bugNodeStartChar": 930, "bugNodeLength": 37, "fixLineNum": 21, "fixNodeStartChar": 930, "fixNodeLength": 36, "sourceBeforeFix": "DEFAULT_USER_AS_DEFAULT_QUEUE=false", "sourceAfterFix": "DEFAULT_USER_AS_DEFAULT_QUEUE=true"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "c3448fe289734ed7498eee4444af630f7abd6986", "fixCommitParentSHA1": "0e1b19a1116a7ff61dd35bb8d8a9dff01c4cdc60", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java\nindex 4a26920..620d0fb 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java\n@@ -401,11 +401,11 @@\n         createAppAttemptId(1, 1), \"default\", \"user1\");\n     scheduler.handle(appAddedEvent1);\n \n-    // Scheduler should have one queue (the default)\n-    assertEquals(1, scheduler.getQueueManager().getQueues().size());\n+    // Scheduler should have two queues (the default and the one created for user1)\n+    assertEquals(2, scheduler.getQueueManager().getQueues().size());\n \n     // That queue should have one app\n-    assertEquals(1, scheduler.getQueueManager().getQueue(\"default\").getApplications().size());\n+    assertEquals(1, scheduler.getQueueManager().getQueue(\"user1\").getApplications().size());\n \n     AppRemovedSchedulerEvent appRemovedEvent1 = new AppRemovedSchedulerEvent(\n         createAppAttemptId(1, 1), RMAppAttemptState.FINISHED);\n@@ -413,8 +413,8 @@\n     // Now remove app\n     scheduler.handle(appRemovedEvent1);\n \n-    // Default queue should have no apps\n-    assertEquals(0, scheduler.getQueueManager().getQueue(\"default\").getApplications().size());\n+    // Queue should have no apps\n+    assertEquals(0, scheduler.getQueueManager().getQueue(\"user1\").getApplications().size());\n   }\n \n   @Test\n", "projectName": "apache.hadoop", "bugLineNum": 405, "bugNodeStartChar": 16847, "bugNodeLength": 63, "fixLineNum": 405, "fixNodeStartChar": 16878, "fixNodeLength": 63, "sourceBeforeFix": "assertEquals(1,scheduler.getQueueManager().getQueues().size())", "sourceAfterFix": "assertEquals(2,scheduler.getQueueManager().getQueues().size())"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "3d732616366b93a75351f4f43ecaa80d2d544abe", "fixCommitParentSHA1": "6b264008a9bcdf5ffc9ebf0f6a4b71be07766208", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\nindex be1c154..dacaae9 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n@@ -431,7 +431,7 @@\n         if (!root.exists()) {\n           // storage directory does not exist\n           if (startOpt != StartupOption.FORMAT) {\n-            LOG.info(\"Storage directory \" + rootPath + \" does not exist\");\n+            LOG.warn(\"Storage directory \" + rootPath + \" does not exist\");\n             return StorageState.NON_EXISTENT;\n           }\n           LOG.info(rootPath + \" does not exist. Creating ...\");\n@@ -440,15 +440,15 @@\n         }\n         // or is inaccessible\n         if (!root.isDirectory()) {\n-          LOG.info(rootPath + \"is not a directory\");\n+          LOG.warn(rootPath + \"is not a directory\");\n           return StorageState.NON_EXISTENT;\n         }\n         if (!root.canWrite()) {\n-          LOG.info(\"Cannot access storage directory \" + rootPath);\n+          LOG.warn(\"Cannot access storage directory \" + rootPath);\n           return StorageState.NON_EXISTENT;\n         }\n       } catch(SecurityException ex) {\n-        LOG.info(\"Cannot access storage directory \" + rootPath, ex);\n+        LOG.warn(\"Cannot access storage directory \" + rootPath, ex);\n         return StorageState.NON_EXISTENT;\n       }\n \n", "projectName": "apache.hadoop", "bugLineNum": 434, "bugNodeStartChar": 14714, "bugNodeLength": 61, "fixLineNum": 434, "fixNodeStartChar": 14714, "fixNodeLength": 61, "sourceBeforeFix": "LOG.info(\"Storage directory \" + rootPath + \" does not exist\")", "sourceAfterFix": "LOG.warn(\"Storage directory \" + rootPath + \" does not exist\")"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "3d732616366b93a75351f4f43ecaa80d2d544abe", "fixCommitParentSHA1": "6b264008a9bcdf5ffc9ebf0f6a4b71be07766208", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\nindex be1c154..dacaae9 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n@@ -431,7 +431,7 @@\n         if (!root.exists()) {\n           // storage directory does not exist\n           if (startOpt != StartupOption.FORMAT) {\n-            LOG.info(\"Storage directory \" + rootPath + \" does not exist\");\n+            LOG.warn(\"Storage directory \" + rootPath + \" does not exist\");\n             return StorageState.NON_EXISTENT;\n           }\n           LOG.info(rootPath + \" does not exist. Creating ...\");\n@@ -440,15 +440,15 @@\n         }\n         // or is inaccessible\n         if (!root.isDirectory()) {\n-          LOG.info(rootPath + \"is not a directory\");\n+          LOG.warn(rootPath + \"is not a directory\");\n           return StorageState.NON_EXISTENT;\n         }\n         if (!root.canWrite()) {\n-          LOG.info(\"Cannot access storage directory \" + rootPath);\n+          LOG.warn(\"Cannot access storage directory \" + rootPath);\n           return StorageState.NON_EXISTENT;\n         }\n       } catch(SecurityException ex) {\n-        LOG.info(\"Cannot access storage directory \" + rootPath, ex);\n+        LOG.warn(\"Cannot access storage directory \" + rootPath, ex);\n         return StorageState.NON_EXISTENT;\n       }\n \n", "projectName": "apache.hadoop", "bugLineNum": 434, "bugNodeStartChar": 14714, "bugNodeLength": 61, "fixLineNum": 434, "fixNodeStartChar": 14714, "fixNodeLength": 61, "sourceBeforeFix": "LOG.info(\"Storage directory \" + rootPath + \" does not exist\")", "sourceAfterFix": "LOG.warn(\"Storage directory \" + rootPath + \" does not exist\")"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "3d732616366b93a75351f4f43ecaa80d2d544abe", "fixCommitParentSHA1": "6b264008a9bcdf5ffc9ebf0f6a4b71be07766208", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\nindex be1c154..dacaae9 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n@@ -431,7 +431,7 @@\n         if (!root.exists()) {\n           // storage directory does not exist\n           if (startOpt != StartupOption.FORMAT) {\n-            LOG.info(\"Storage directory \" + rootPath + \" does not exist\");\n+            LOG.warn(\"Storage directory \" + rootPath + \" does not exist\");\n             return StorageState.NON_EXISTENT;\n           }\n           LOG.info(rootPath + \" does not exist. Creating ...\");\n@@ -440,15 +440,15 @@\n         }\n         // or is inaccessible\n         if (!root.isDirectory()) {\n-          LOG.info(rootPath + \"is not a directory\");\n+          LOG.warn(rootPath + \"is not a directory\");\n           return StorageState.NON_EXISTENT;\n         }\n         if (!root.canWrite()) {\n-          LOG.info(\"Cannot access storage directory \" + rootPath);\n+          LOG.warn(\"Cannot access storage directory \" + rootPath);\n           return StorageState.NON_EXISTENT;\n         }\n       } catch(SecurityException ex) {\n-        LOG.info(\"Cannot access storage directory \" + rootPath, ex);\n+        LOG.warn(\"Cannot access storage directory \" + rootPath, ex);\n         return StorageState.NON_EXISTENT;\n       }\n \n", "projectName": "apache.hadoop", "bugLineNum": 443, "bugNodeStartChar": 15088, "bugNodeLength": 41, "fixLineNum": 443, "fixNodeStartChar": 15088, "fixNodeLength": 41, "sourceBeforeFix": "LOG.info(rootPath + \"is not a directory\")", "sourceAfterFix": "LOG.warn(rootPath + \"is not a directory\")"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "3d732616366b93a75351f4f43ecaa80d2d544abe", "fixCommitParentSHA1": "6b264008a9bcdf5ffc9ebf0f6a4b71be07766208", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\nindex be1c154..dacaae9 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n@@ -431,7 +431,7 @@\n         if (!root.exists()) {\n           // storage directory does not exist\n           if (startOpt != StartupOption.FORMAT) {\n-            LOG.info(\"Storage directory \" + rootPath + \" does not exist\");\n+            LOG.warn(\"Storage directory \" + rootPath + \" does not exist\");\n             return StorageState.NON_EXISTENT;\n           }\n           LOG.info(rootPath + \" does not exist. Creating ...\");\n@@ -440,15 +440,15 @@\n         }\n         // or is inaccessible\n         if (!root.isDirectory()) {\n-          LOG.info(rootPath + \"is not a directory\");\n+          LOG.warn(rootPath + \"is not a directory\");\n           return StorageState.NON_EXISTENT;\n         }\n         if (!root.canWrite()) {\n-          LOG.info(\"Cannot access storage directory \" + rootPath);\n+          LOG.warn(\"Cannot access storage directory \" + rootPath);\n           return StorageState.NON_EXISTENT;\n         }\n       } catch(SecurityException ex) {\n-        LOG.info(\"Cannot access storage directory \" + rootPath, ex);\n+        LOG.warn(\"Cannot access storage directory \" + rootPath, ex);\n         return StorageState.NON_EXISTENT;\n       }\n \n", "projectName": "apache.hadoop", "bugLineNum": 443, "bugNodeStartChar": 15088, "bugNodeLength": 41, "fixLineNum": 443, "fixNodeStartChar": 15088, "fixNodeLength": 41, "sourceBeforeFix": "LOG.info(rootPath + \"is not a directory\")", "sourceAfterFix": "LOG.warn(rootPath + \"is not a directory\")"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "3d732616366b93a75351f4f43ecaa80d2d544abe", "fixCommitParentSHA1": "6b264008a9bcdf5ffc9ebf0f6a4b71be07766208", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\nindex be1c154..dacaae9 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n@@ -431,7 +431,7 @@\n         if (!root.exists()) {\n           // storage directory does not exist\n           if (startOpt != StartupOption.FORMAT) {\n-            LOG.info(\"Storage directory \" + rootPath + \" does not exist\");\n+            LOG.warn(\"Storage directory \" + rootPath + \" does not exist\");\n             return StorageState.NON_EXISTENT;\n           }\n           LOG.info(rootPath + \" does not exist. Creating ...\");\n@@ -440,15 +440,15 @@\n         }\n         // or is inaccessible\n         if (!root.isDirectory()) {\n-          LOG.info(rootPath + \"is not a directory\");\n+          LOG.warn(rootPath + \"is not a directory\");\n           return StorageState.NON_EXISTENT;\n         }\n         if (!root.canWrite()) {\n-          LOG.info(\"Cannot access storage directory \" + rootPath);\n+          LOG.warn(\"Cannot access storage directory \" + rootPath);\n           return StorageState.NON_EXISTENT;\n         }\n       } catch(SecurityException ex) {\n-        LOG.info(\"Cannot access storage directory \" + rootPath, ex);\n+        LOG.warn(\"Cannot access storage directory \" + rootPath, ex);\n         return StorageState.NON_EXISTENT;\n       }\n \n", "projectName": "apache.hadoop", "bugLineNum": 447, "bugNodeStartChar": 15227, "bugNodeLength": 55, "fixLineNum": 447, "fixNodeStartChar": 15227, "fixNodeLength": 55, "sourceBeforeFix": "LOG.info(\"Cannot access storage directory \" + rootPath)", "sourceAfterFix": "LOG.warn(\"Cannot access storage directory \" + rootPath)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "3d732616366b93a75351f4f43ecaa80d2d544abe", "fixCommitParentSHA1": "6b264008a9bcdf5ffc9ebf0f6a4b71be07766208", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\nindex be1c154..dacaae9 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n@@ -431,7 +431,7 @@\n         if (!root.exists()) {\n           // storage directory does not exist\n           if (startOpt != StartupOption.FORMAT) {\n-            LOG.info(\"Storage directory \" + rootPath + \" does not exist\");\n+            LOG.warn(\"Storage directory \" + rootPath + \" does not exist\");\n             return StorageState.NON_EXISTENT;\n           }\n           LOG.info(rootPath + \" does not exist. Creating ...\");\n@@ -440,15 +440,15 @@\n         }\n         // or is inaccessible\n         if (!root.isDirectory()) {\n-          LOG.info(rootPath + \"is not a directory\");\n+          LOG.warn(rootPath + \"is not a directory\");\n           return StorageState.NON_EXISTENT;\n         }\n         if (!root.canWrite()) {\n-          LOG.info(\"Cannot access storage directory \" + rootPath);\n+          LOG.warn(\"Cannot access storage directory \" + rootPath);\n           return StorageState.NON_EXISTENT;\n         }\n       } catch(SecurityException ex) {\n-        LOG.info(\"Cannot access storage directory \" + rootPath, ex);\n+        LOG.warn(\"Cannot access storage directory \" + rootPath, ex);\n         return StorageState.NON_EXISTENT;\n       }\n \n", "projectName": "apache.hadoop", "bugLineNum": 447, "bugNodeStartChar": 15227, "bugNodeLength": 55, "fixLineNum": 447, "fixNodeStartChar": 15227, "fixNodeLength": 55, "sourceBeforeFix": "LOG.info(\"Cannot access storage directory \" + rootPath)", "sourceAfterFix": "LOG.warn(\"Cannot access storage directory \" + rootPath)"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "3d732616366b93a75351f4f43ecaa80d2d544abe", "fixCommitParentSHA1": "6b264008a9bcdf5ffc9ebf0f6a4b71be07766208", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\nindex be1c154..dacaae9 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n@@ -431,7 +431,7 @@\n         if (!root.exists()) {\n           // storage directory does not exist\n           if (startOpt != StartupOption.FORMAT) {\n-            LOG.info(\"Storage directory \" + rootPath + \" does not exist\");\n+            LOG.warn(\"Storage directory \" + rootPath + \" does not exist\");\n             return StorageState.NON_EXISTENT;\n           }\n           LOG.info(rootPath + \" does not exist. Creating ...\");\n@@ -440,15 +440,15 @@\n         }\n         // or is inaccessible\n         if (!root.isDirectory()) {\n-          LOG.info(rootPath + \"is not a directory\");\n+          LOG.warn(rootPath + \"is not a directory\");\n           return StorageState.NON_EXISTENT;\n         }\n         if (!root.canWrite()) {\n-          LOG.info(\"Cannot access storage directory \" + rootPath);\n+          LOG.warn(\"Cannot access storage directory \" + rootPath);\n           return StorageState.NON_EXISTENT;\n         }\n       } catch(SecurityException ex) {\n-        LOG.info(\"Cannot access storage directory \" + rootPath, ex);\n+        LOG.warn(\"Cannot access storage directory \" + rootPath, ex);\n         return StorageState.NON_EXISTENT;\n       }\n \n", "projectName": "apache.hadoop", "bugLineNum": 451, "bugNodeStartChar": 15384, "bugNodeLength": 59, "fixLineNum": 451, "fixNodeStartChar": 15384, "fixNodeLength": 59, "sourceBeforeFix": "LOG.info(\"Cannot access storage directory \" + rootPath,ex)", "sourceAfterFix": "LOG.warn(\"Cannot access storage directory \" + rootPath,ex)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "3d732616366b93a75351f4f43ecaa80d2d544abe", "fixCommitParentSHA1": "6b264008a9bcdf5ffc9ebf0f6a4b71be07766208", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\nindex be1c154..dacaae9 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java\n@@ -431,7 +431,7 @@\n         if (!root.exists()) {\n           // storage directory does not exist\n           if (startOpt != StartupOption.FORMAT) {\n-            LOG.info(\"Storage directory \" + rootPath + \" does not exist\");\n+            LOG.warn(\"Storage directory \" + rootPath + \" does not exist\");\n             return StorageState.NON_EXISTENT;\n           }\n           LOG.info(rootPath + \" does not exist. Creating ...\");\n@@ -440,15 +440,15 @@\n         }\n         // or is inaccessible\n         if (!root.isDirectory()) {\n-          LOG.info(rootPath + \"is not a directory\");\n+          LOG.warn(rootPath + \"is not a directory\");\n           return StorageState.NON_EXISTENT;\n         }\n         if (!root.canWrite()) {\n-          LOG.info(\"Cannot access storage directory \" + rootPath);\n+          LOG.warn(\"Cannot access storage directory \" + rootPath);\n           return StorageState.NON_EXISTENT;\n         }\n       } catch(SecurityException ex) {\n-        LOG.info(\"Cannot access storage directory \" + rootPath, ex);\n+        LOG.warn(\"Cannot access storage directory \" + rootPath, ex);\n         return StorageState.NON_EXISTENT;\n       }\n \n", "projectName": "apache.hadoop", "bugLineNum": 451, "bugNodeStartChar": 15384, "bugNodeLength": 59, "fixLineNum": 451, "fixNodeStartChar": 15384, "fixNodeLength": 59, "sourceBeforeFix": "LOG.info(\"Cannot access storage directory \" + rootPath,ex)", "sourceAfterFix": "LOG.warn(\"Cannot access storage directory \" + rootPath,ex)"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "3d732616366b93a75351f4f43ecaa80d2d544abe", "fixCommitParentSHA1": "6b264008a9bcdf5ffc9ebf0f6a4b71be07766208", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java\nindex 107ce5d..66ecdbc 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java\n@@ -137,7 +137,7 @@\n     if (removedVols != null && removedVols.size() > 0) {\n       // Replace volume list\n       volumes = Collections.unmodifiableList(volumeList);\n-      FsDatasetImpl.LOG.info(\"Completed checkDirs. Removed \" + removedVols.size()\n+      FsDatasetImpl.LOG.warn(\"Completed checkDirs. Removed \" + removedVols.size()\n           + \" volumes. Current volumes: \" + this);\n     }\n \n", "projectName": "apache.hadoop", "bugLineNum": 140, "bugNodeStartChar": 4545, "bugNodeLength": 125, "fixLineNum": 140, "fixNodeStartChar": 4545, "fixNodeLength": 125, "sourceBeforeFix": "FsDatasetImpl.LOG.info(\"Completed checkDirs. Removed \" + removedVols.size() + \" volumes. Current volumes: \"+ this)", "sourceAfterFix": "FsDatasetImpl.LOG.warn(\"Completed checkDirs. Removed \" + removedVols.size() + \" volumes. Current volumes: \"+ this)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "3d732616366b93a75351f4f43ecaa80d2d544abe", "fixCommitParentSHA1": "6b264008a9bcdf5ffc9ebf0f6a4b71be07766208", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java\nindex 107ce5d..66ecdbc 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java\n@@ -137,7 +137,7 @@\n     if (removedVols != null && removedVols.size() > 0) {\n       // Replace volume list\n       volumes = Collections.unmodifiableList(volumeList);\n-      FsDatasetImpl.LOG.info(\"Completed checkDirs. Removed \" + removedVols.size()\n+      FsDatasetImpl.LOG.warn(\"Completed checkDirs. Removed \" + removedVols.size()\n           + \" volumes. Current volumes: \" + this);\n     }\n \n", "projectName": "apache.hadoop", "bugLineNum": 140, "bugNodeStartChar": 4545, "bugNodeLength": 125, "fixLineNum": 140, "fixNodeStartChar": 4545, "fixNodeLength": 125, "sourceBeforeFix": "FsDatasetImpl.LOG.info(\"Completed checkDirs. Removed \" + removedVols.size() + \" volumes. Current volumes: \"+ this)", "sourceAfterFix": "FsDatasetImpl.LOG.warn(\"Completed checkDirs. Removed \" + removedVols.size() + \" volumes. Current volumes: \"+ this)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "bfb69f6eeabb7ba6140f95d27c6b16f6f2e40d47", "fixCommitParentSHA1": "10f82d19223e19a2e8b360ced7972126942416df", "bugFilePath": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueInformation.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueInformation.java b/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueInformation.java\nindex b18a527..ea2980c 100644\n--- a/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueInformation.java\n+++ b/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueInformation.java\n@@ -98,7 +98,7 @@\n     dfsCluster.shutdown();\n   }\n \n-  public void testJobQueues() throws IOException {\n+  public void testJobQueues() throws Exception {\n     JobClient jc = new JobClient(mrCluster.createJobConf());\n     String expectedQueueInfo = \"Maximum Tasks Per Job :: 10\";\n     JobQueueInfo[] queueInfos = jc.getQueues();\n", "projectName": "apache.hadoop", "bugLineNum": 101, "bugNodeStartChar": 3601, "bugNodeLength": 1577, "fixLineNum": 101, "fixNodeStartChar": 3601, "fixNodeLength": 1575, "sourceBeforeFix": "public void testJobQueues() throws IOException {   JobClient jc=new JobClient(mrCluster.createJobConf());   String expectedQueueInfo=\"Maximum Tasks Per Job :: 10\";   JobQueueInfo[] queueInfos=jc.getQueues();   assertNotNull(queueInfos);   assertEquals(1,queueInfos.length);   assertEquals(\"default\",queueInfos[0].getQueueName());   assertEquals(QueueState.RUNNING.getStateName(),queueInfos[0].getQueueState());   JobConf conf=mrCluster.createJobConf();   FileSystem fileSys=dfsCluster.getFileSystem();   conf=configureWaitingJob(conf);   conf.setJobName(\"test-job-queue-info-test\");   fileSys.delete(SHARE_DIR,true);   RunningJob rJob=jc.submitJob(conf);   while (rJob.getJobState() != JobStatus.RUNNING) {     UtilsForTests.waitFor(10);   }   int numberOfJobs=0;   for (  JobQueueInfo queueInfo : queueInfos) {     JobStatus[] jobStatusList=jc.getJobsFromQueue(queueInfo.getQueueName());     assertNotNull(queueInfo.getQueueName());     assertNotNull(queueInfo.getSchedulingInfo());     assertEquals(expectedQueueInfo,queueInfo.getSchedulingInfo());     numberOfJobs+=jobStatusList.length;     for (    JobStatus status : jobStatusList) {       assertEquals(JOB_SCHEDULING_INFO,status.getSchedulingInfo());     }   }   assertEquals(1,numberOfJobs);   UtilsForTests.signalTasks(dfsCluster,fileSys,getSignalFile(),getSignalFile(),4); } ", "sourceAfterFix": "public void testJobQueues() throws Exception {   JobClient jc=new JobClient(mrCluster.createJobConf());   String expectedQueueInfo=\"Maximum Tasks Per Job :: 10\";   JobQueueInfo[] queueInfos=jc.getQueues();   assertNotNull(queueInfos);   assertEquals(1,queueInfos.length);   assertEquals(\"default\",queueInfos[0].getQueueName());   assertEquals(QueueState.RUNNING.getStateName(),queueInfos[0].getQueueState());   JobConf conf=mrCluster.createJobConf();   FileSystem fileSys=dfsCluster.getFileSystem();   conf=configureWaitingJob(conf);   conf.setJobName(\"test-job-queue-info-test\");   fileSys.delete(SHARE_DIR,true);   RunningJob rJob=jc.submitJob(conf);   while (rJob.getJobState() != JobStatus.RUNNING) {     UtilsForTests.waitFor(10);   }   int numberOfJobs=0;   for (  JobQueueInfo queueInfo : queueInfos) {     JobStatus[] jobStatusList=jc.getJobsFromQueue(queueInfo.getQueueName());     assertNotNull(queueInfo.getQueueName());     assertNotNull(queueInfo.getSchedulingInfo());     assertEquals(expectedQueueInfo,queueInfo.getSchedulingInfo());     numberOfJobs+=jobStatusList.length;     for (    JobStatus status : jobStatusList) {       assertEquals(JOB_SCHEDULING_INFO,status.getSchedulingInfo());     }   }   assertEquals(1,numberOfJobs);   UtilsForTests.signalTasks(dfsCluster,fileSys,getSignalFile(),getSignalFile(),4); } "}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "bfb69f6eeabb7ba6140f95d27c6b16f6f2e40d47", "fixCommitParentSHA1": "10f82d19223e19a2e8b360ced7972126942416df", "bugFilePath": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestSetupAndCleanupFailure.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestSetupAndCleanupFailure.java b/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestSetupAndCleanupFailure.java\nindex 07706b1..2c4d999 100644\n--- a/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestSetupAndCleanupFailure.java\n+++ b/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestSetupAndCleanupFailure.java\n@@ -149,7 +149,7 @@\n   private void testSetupAndCleanupKill(MiniMRCluster mr, \n                                        MiniDFSCluster dfs, \n                                        boolean commandLineKill) \n-  throws IOException {\n+  throws Exception {\n     // launch job with waiting setup/cleanup\n     RunningJob job = launchJobWithWaitingSetupAndCleanup(mr);\n     \n@@ -223,7 +223,7 @@\n   // Also Tests the command-line kill for setup/cleanup attempts. \n   // tests the setup/cleanup attempts getting killed if \n   // they were running on a lost tracker\n-  public void testWithDFS() throws IOException {\n+  public void testWithDFS() throws Exception {\n     MiniDFSCluster dfs = null;\n     MiniMRCluster mr = null;\n     FileSystem fileSys = null;\n", "projectName": "apache.hadoop", "bugLineNum": 139, "bugNodeStartChar": 4830, "bugNodeLength": 2156, "fixLineNum": 139, "fixNodeStartChar": 4830, "fixNodeLength": 2154, "sourceBeforeFix": "/**   * Tests setup and cleanup attempts getting killed from command-line  and lost tracker  * @param mr  * @param dfs  * @param commandLineKill if true, test with command-line killelse, test with lost tracker  * @throws IOException  */ private void testSetupAndCleanupKill(MiniMRCluster mr,MiniDFSCluster dfs,boolean commandLineKill) throws IOException {   RunningJob job=launchJobWithWaitingSetupAndCleanup(mr);   JobTracker jt=mr.getJobTrackerRunner().getJobTracker();   JobInProgress jip=jt.getJob(job.getID());   TaskAttemptID setupID=getRunningTaskID(jip.getTasks(TaskType.JOB_SETUP));   if (commandLineKill) {     killTaskFromCommandLine(job,setupID,jt);   }  else {     killTaskWithLostTracker(mr,setupID);   }   UtilsForTests.writeFile(dfs.getNameNode(),dfs.getFileSystem().getConf(),setupSignalFile,(short)3);   while (job.reduceProgress() != 1.0f) {     try {       Thread.sleep(100);     }  catch (    InterruptedException ie) {     }   }   TaskAttemptID cleanupID=getRunningTaskID(jip.getTasks(TaskType.JOB_CLEANUP));   if (commandLineKill) {     killTaskFromCommandLine(job,cleanupID,jt);   }  else {     killTaskWithLostTracker(mr,cleanupID);   }   UtilsForTests.writeFile(dfs.getNameNode(),dfs.getFileSystem().getConf(),cleanupSignalFile,(short)3);   job.waitForCompletion();   assertEquals(JobStatus.SUCCEEDED,job.getJobState());   assertEquals(TaskStatus.State.KILLED,jt.getTaskStatus(setupID).getRunState());   assertEquals(TaskStatus.State.KILLED,jt.getTaskStatus(cleanupID).getRunState()); } ", "sourceAfterFix": "/**   * Tests setup and cleanup attempts getting killed from command-line  and lost tracker  * @param mr  * @param dfs  * @param commandLineKill if true, test with command-line killelse, test with lost tracker  * @throws IOException  */ private void testSetupAndCleanupKill(MiniMRCluster mr,MiniDFSCluster dfs,boolean commandLineKill) throws Exception {   RunningJob job=launchJobWithWaitingSetupAndCleanup(mr);   JobTracker jt=mr.getJobTrackerRunner().getJobTracker();   JobInProgress jip=jt.getJob(job.getID());   TaskAttemptID setupID=getRunningTaskID(jip.getTasks(TaskType.JOB_SETUP));   if (commandLineKill) {     killTaskFromCommandLine(job,setupID,jt);   }  else {     killTaskWithLostTracker(mr,setupID);   }   UtilsForTests.writeFile(dfs.getNameNode(),dfs.getFileSystem().getConf(),setupSignalFile,(short)3);   while (job.reduceProgress() != 1.0f) {     try {       Thread.sleep(100);     }  catch (    InterruptedException ie) {     }   }   TaskAttemptID cleanupID=getRunningTaskID(jip.getTasks(TaskType.JOB_CLEANUP));   if (commandLineKill) {     killTaskFromCommandLine(job,cleanupID,jt);   }  else {     killTaskWithLostTracker(mr,cleanupID);   }   UtilsForTests.writeFile(dfs.getNameNode(),dfs.getFileSystem().getConf(),cleanupSignalFile,(short)3);   job.waitForCompletion();   assertEquals(JobStatus.SUCCEEDED,job.getJobState());   assertEquals(TaskStatus.State.KILLED,jt.getTaskStatus(setupID).getRunState());   assertEquals(TaskStatus.State.KILLED,jt.getTaskStatus(cleanupID).getRunState()); } "}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "bfb69f6eeabb7ba6140f95d27c6b16f6f2e40d47", "fixCommitParentSHA1": "10f82d19223e19a2e8b360ced7972126942416df", "bugFilePath": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestSetupAndCleanupFailure.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestSetupAndCleanupFailure.java b/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestSetupAndCleanupFailure.java\nindex 07706b1..2c4d999 100644\n--- a/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestSetupAndCleanupFailure.java\n+++ b/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestSetupAndCleanupFailure.java\n@@ -149,7 +149,7 @@\n   private void testSetupAndCleanupKill(MiniMRCluster mr, \n                                        MiniDFSCluster dfs, \n                                        boolean commandLineKill) \n-  throws IOException {\n+  throws Exception {\n     // launch job with waiting setup/cleanup\n     RunningJob job = launchJobWithWaitingSetupAndCleanup(mr);\n     \n@@ -223,7 +223,7 @@\n   // Also Tests the command-line kill for setup/cleanup attempts. \n   // tests the setup/cleanup attempts getting killed if \n   // they were running on a lost tracker\n-  public void testWithDFS() throws IOException {\n+  public void testWithDFS() throws Exception {\n     MiniDFSCluster dfs = null;\n     MiniMRCluster mr = null;\n     FileSystem fileSys = null;\n", "projectName": "apache.hadoop", "bugLineNum": 226, "bugNodeStartChar": 8173, "bugNodeLength": 1365, "fixLineNum": 226, "fixNodeStartChar": 8173, "fixNodeLength": 1363, "sourceBeforeFix": "public void testWithDFS() throws IOException {   MiniDFSCluster dfs=null;   MiniMRCluster mr=null;   FileSystem fileSys=null;   try {     final int taskTrackers=4;     Configuration conf=new Configuration();     dfs=new MiniDFSCluster(conf,4,true,null);     fileSys=dfs.getFileSystem();     JobConf jtConf=new JobConf();     jtConf.setInt(TTConfig.TT_MAP_SLOTS,1);     jtConf.setInt(TTConfig.TT_REDUCE_SLOTS,1);     jtConf.setLong(JTConfig.JT_TRACKER_EXPIRY_INTERVAL,10 * 1000);     mr=new MiniMRCluster(taskTrackers,fileSys.getUri().toString(),1,null,null,jtConf);     testFailCommitter(CommitterWithFailSetup.class,mr.createJobConf());     testFailCommitter(CommitterWithFailCommit.class,mr.createJobConf());     testSetupAndCleanupKill(mr,dfs,true);     fileSys.delete(setupSignalFile,true);     fileSys.delete(cleanupSignalFile,true);     testSetupAndCleanupKill(mr,dfs,false);   }   finally {     if (dfs != null) {       dfs.shutdown();     }     if (mr != null) {       mr.shutdown();     }   } } ", "sourceAfterFix": "public void testWithDFS() throws Exception {   MiniDFSCluster dfs=null;   MiniMRCluster mr=null;   FileSystem fileSys=null;   try {     final int taskTrackers=4;     Configuration conf=new Configuration();     dfs=new MiniDFSCluster(conf,4,true,null);     fileSys=dfs.getFileSystem();     JobConf jtConf=new JobConf();     jtConf.setInt(TTConfig.TT_MAP_SLOTS,1);     jtConf.setInt(TTConfig.TT_REDUCE_SLOTS,1);     jtConf.setLong(JTConfig.JT_TRACKER_EXPIRY_INTERVAL,10 * 1000);     mr=new MiniMRCluster(taskTrackers,fileSys.getUri().toString(),1,null,null,jtConf);     testFailCommitter(CommitterWithFailSetup.class,mr.createJobConf());     testFailCommitter(CommitterWithFailCommit.class,mr.createJobConf());     testSetupAndCleanupKill(mr,dfs,true);     fileSys.delete(setupSignalFile,true);     fileSys.delete(cleanupSignalFile,true);     testSetupAndCleanupKill(mr,dfs,false);   }   finally {     if (dfs != null) {       dfs.shutdown();     }     if (mr != null) {       mr.shutdown();     }   } } "}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "bfb69f6eeabb7ba6140f95d27c6b16f6f2e40d47", "fixCommitParentSHA1": "10f82d19223e19a2e8b360ced7972126942416df", "bugFilePath": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java b/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java\nindex fc3c617..1c7e70c 100644\n--- a/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java\n+++ b/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java\n@@ -449,7 +449,7 @@\n   static void signalTasks(MiniDFSCluster dfs, FileSystem fileSys, \n                           String mapSignalFile, \n                           String reduceSignalFile, int replication) \n-  throws IOException {\n+  throws Exception {\n     writeFile(dfs.getNameNode(), fileSys.getConf(), new Path(mapSignalFile), \n               (short)replication);\n     writeFile(dfs.getNameNode(), fileSys.getConf(), new Path(reduceSignalFile), \n@@ -462,7 +462,7 @@\n   static void signalTasks(MiniDFSCluster dfs, FileSystem fileSys, \n                           boolean isMap, String mapSignalFile, \n                           String reduceSignalFile)\n-  throws IOException {\n+  throws Exception {\n     //  signal the maps to complete\n     writeFile(dfs.getNameNode(), fileSys.getConf(),\n               isMap \n@@ -483,7 +483,7 @@\n   }\n   \n   static void writeFile(NameNode namenode, Configuration conf, Path name, \n-      short replication) throws IOException {\n+      short replication) throws Exception {\n     FileSystem fileSys = FileSystem.get(conf);\n     SequenceFile.Writer writer = \n       SequenceFile.createWriter(fileSys, conf, name, \n", "projectName": "apache.hadoop", "bugLineNum": 446, "bugNodeStartChar": 13529, "bugNodeLength": 489, "fixLineNum": 446, "fixNodeStartChar": 13529, "fixNodeLength": 487, "sourceBeforeFix": "/**   * Signal the maps/reduces to start.  */ static void signalTasks(MiniDFSCluster dfs,FileSystem fileSys,String mapSignalFile,String reduceSignalFile,int replication) throws IOException {   writeFile(dfs.getNameNode(),fileSys.getConf(),new Path(mapSignalFile),(short)replication);   writeFile(dfs.getNameNode(),fileSys.getConf(),new Path(reduceSignalFile),(short)replication); } ", "sourceAfterFix": "/**   * Signal the maps/reduces to start.  */ static void signalTasks(MiniDFSCluster dfs,FileSystem fileSys,String mapSignalFile,String reduceSignalFile,int replication) throws Exception {   writeFile(dfs.getNameNode(),fileSys.getConf(),new Path(mapSignalFile),(short)replication);   writeFile(dfs.getNameNode(),fileSys.getConf(),new Path(reduceSignalFile),(short)replication); } "}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "bfb69f6eeabb7ba6140f95d27c6b16f6f2e40d47", "fixCommitParentSHA1": "10f82d19223e19a2e8b360ced7972126942416df", "bugFilePath": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java b/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java\nindex fc3c617..1c7e70c 100644\n--- a/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java\n+++ b/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java\n@@ -449,7 +449,7 @@\n   static void signalTasks(MiniDFSCluster dfs, FileSystem fileSys, \n                           String mapSignalFile, \n                           String reduceSignalFile, int replication) \n-  throws IOException {\n+  throws Exception {\n     writeFile(dfs.getNameNode(), fileSys.getConf(), new Path(mapSignalFile), \n               (short)replication);\n     writeFile(dfs.getNameNode(), fileSys.getConf(), new Path(reduceSignalFile), \n@@ -462,7 +462,7 @@\n   static void signalTasks(MiniDFSCluster dfs, FileSystem fileSys, \n                           boolean isMap, String mapSignalFile, \n                           String reduceSignalFile)\n-  throws IOException {\n+  throws Exception {\n     //  signal the maps to complete\n     writeFile(dfs.getNameNode(), fileSys.getConf(),\n               isMap \n@@ -483,7 +483,7 @@\n   }\n   \n   static void writeFile(NameNode namenode, Configuration conf, Path name, \n-      short replication) throws IOException {\n+      short replication) throws Exception {\n     FileSystem fileSys = FileSystem.get(conf);\n     SequenceFile.Writer writer = \n       SequenceFile.createWriter(fileSys, conf, name, \n", "projectName": "apache.hadoop", "bugLineNum": 459, "bugNodeStartChar": 14024, "bugNodeLength": 461, "fixLineNum": 459, "fixNodeStartChar": 14024, "fixNodeLength": 459, "sourceBeforeFix": "/**   * Signal the maps/reduces to start.  */ static void signalTasks(MiniDFSCluster dfs,FileSystem fileSys,boolean isMap,String mapSignalFile,String reduceSignalFile) throws IOException {   writeFile(dfs.getNameNode(),fileSys.getConf(),isMap ? new Path(mapSignalFile) : new Path(reduceSignalFile),(short)1); } ", "sourceAfterFix": "/**   * Signal the maps/reduces to start.  */ static void signalTasks(MiniDFSCluster dfs,FileSystem fileSys,boolean isMap,String mapSignalFile,String reduceSignalFile) throws Exception {   writeFile(dfs.getNameNode(),fileSys.getConf(),isMap ? new Path(mapSignalFile) : new Path(reduceSignalFile),(short)1); } "}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "bfb69f6eeabb7ba6140f95d27c6b16f6f2e40d47", "fixCommitParentSHA1": "10f82d19223e19a2e8b360ced7972126942416df", "bugFilePath": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java b/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java\nindex fc3c617..1c7e70c 100644\n--- a/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java\n+++ b/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/UtilsForTests.java\n@@ -449,7 +449,7 @@\n   static void signalTasks(MiniDFSCluster dfs, FileSystem fileSys, \n                           String mapSignalFile, \n                           String reduceSignalFile, int replication) \n-  throws IOException {\n+  throws Exception {\n     writeFile(dfs.getNameNode(), fileSys.getConf(), new Path(mapSignalFile), \n               (short)replication);\n     writeFile(dfs.getNameNode(), fileSys.getConf(), new Path(reduceSignalFile), \n@@ -462,7 +462,7 @@\n   static void signalTasks(MiniDFSCluster dfs, FileSystem fileSys, \n                           boolean isMap, String mapSignalFile, \n                           String reduceSignalFile)\n-  throws IOException {\n+  throws Exception {\n     //  signal the maps to complete\n     writeFile(dfs.getNameNode(), fileSys.getConf(),\n               isMap \n@@ -483,7 +483,7 @@\n   }\n   \n   static void writeFile(NameNode namenode, Configuration conf, Path name, \n-      short replication) throws IOException {\n+      short replication) throws Exception {\n     FileSystem fileSys = FileSystem.get(conf);\n     SequenceFile.Writer writer = \n       SequenceFile.createWriter(fileSys, conf, name, \n", "projectName": "apache.hadoop", "bugLineNum": 485, "bugNodeStartChar": 14803, "bugNodeLength": 575, "fixLineNum": 485, "fixNodeStartChar": 14803, "fixNodeLength": 573, "sourceBeforeFix": "static void writeFile(NameNode namenode,Configuration conf,Path name,short replication) throws IOException {   FileSystem fileSys=FileSystem.get(conf);   SequenceFile.Writer writer=SequenceFile.createWriter(fileSys,conf,name,BytesWritable.class,BytesWritable.class,CompressionType.NONE);   writer.append(new BytesWritable(),new BytesWritable());   writer.close();   fileSys.setReplication(name,replication);   DFSTestUtil.waitReplication(fileSys,name,replication); } ", "sourceAfterFix": "static void writeFile(NameNode namenode,Configuration conf,Path name,short replication) throws Exception {   FileSystem fileSys=FileSystem.get(conf);   SequenceFile.Writer writer=SequenceFile.createWriter(fileSys,conf,name,BytesWritable.class,BytesWritable.class,CompressionType.NONE);   writer.append(new BytesWritable(),new BytesWritable());   writer.close();   fileSys.setReplication(name,replication);   DFSTestUtil.waitReplication(fileSys,name,replication); } "}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "38ec8babda50215f984e5f5532ae70918af77378", "fixCommitParentSHA1": "e7b414289c47fe090268d42fa41ac0a49c3985ff", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManager.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManager.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManager.java\nindex cda36fe..37634b6 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManager.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManager.java\n@@ -93,7 +93,7 @@\n   /**\n    * The length of the random keys to use.\n    */\n-  private static final int KEY_LENGTH = 20;\n+  private static final int KEY_LENGTH = 64;\n \n   /**\n    * A thread local store for the Macs.\n", "projectName": "apache.hadoop", "bugLineNum": 96, "bugNodeStartChar": 3135, "bugNodeLength": 15, "fixLineNum": 96, "fixNodeStartChar": 3135, "fixNodeLength": 15, "sourceBeforeFix": "KEY_LENGTH=20", "sourceAfterFix": "KEY_LENGTH=64"}, {"bugType": "OVERLOAD_METHOD_DELETED_ARGS", "fixCommitSHA1": "cde65024a20d660e586494ec6a443f8732fee068", "fixCommitParentSHA1": "bbfbbf31c812570f92efc4bff4a51f5c432b28ab", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java\nindex 2b2decc..f9ba80c 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java\n@@ -200,7 +200,7 @@\n   private RMNodeImpl getRunningNode() {\n     NodeId nodeId = BuilderUtils.newNodeId(\"localhost\", 0);\n     RMNodeImpl node = new RMNodeImpl(nodeId, rmContext,null, 0, 0,\n-        null, null, null);\n+        null, null);\n     node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.STARTED));\n     Assert.assertEquals(NodeState.RUNNING, node.getState());\n     return node;\n@@ -212,7 +212,7 @@\n     status.setHealthReport(\"sick\");\n     status.setIsNodeHealthy(false);\n     node.handle(new RMNodeStatusEvent(node.getNodeID(), status,\n-        new ArrayList<ContainerStatus>(), null, null, null));\n+        new ArrayList<ContainerStatus>(), null, null));\n     Assert.assertEquals(NodeState.UNHEALTHY, node.getState());\n     return node;\n   }\n", "projectName": "apache.hadoop", "bugLineNum": 202, "bugNodeStartChar": 7815, "bugNodeLength": 70, "fixLineNum": 202, "fixNodeStartChar": 7815, "fixNodeLength": 64, "sourceBeforeFix": "new RMNodeImpl(nodeId,rmContext,null,0,0,null,null,null)", "sourceAfterFix": "new RMNodeImpl(nodeId,rmContext,null,0,0,null,null)"}, {"bugType": "OVERLOAD_METHOD_DELETED_ARGS", "fixCommitSHA1": "cde65024a20d660e586494ec6a443f8732fee068", "fixCommitParentSHA1": "bbfbbf31c812570f92efc4bff4a51f5c432b28ab", "bugFilePath": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java", "fixPatch": "diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java\nindex 2b2decc..f9ba80c 100644\n--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java\n+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java\n@@ -200,7 +200,7 @@\n   private RMNodeImpl getRunningNode() {\n     NodeId nodeId = BuilderUtils.newNodeId(\"localhost\", 0);\n     RMNodeImpl node = new RMNodeImpl(nodeId, rmContext,null, 0, 0,\n-        null, null, null);\n+        null, null);\n     node.handle(new RMNodeEvent(node.getNodeID(), RMNodeEventType.STARTED));\n     Assert.assertEquals(NodeState.RUNNING, node.getState());\n     return node;\n@@ -212,7 +212,7 @@\n     status.setHealthReport(\"sick\");\n     status.setIsNodeHealthy(false);\n     node.handle(new RMNodeStatusEvent(node.getNodeID(), status,\n-        new ArrayList<ContainerStatus>(), null, null, null));\n+        new ArrayList<ContainerStatus>(), null, null));\n     Assert.assertEquals(NodeState.UNHEALTHY, node.getState());\n     return node;\n   }\n", "projectName": "apache.hadoop", "bugLineNum": 214, "bugNodeStartChar": 8275, "bugNodeLength": 107, "fixLineNum": 214, "fixNodeStartChar": 8275, "fixNodeLength": 101, "sourceBeforeFix": "new RMNodeStatusEvent(node.getNodeID(),status,new ArrayList<ContainerStatus>(),null,null,null)", "sourceAfterFix": "new RMNodeStatusEvent(node.getNodeID(),status,new ArrayList<ContainerStatus>(),null,null)"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "8309c8f182b1f7febfff37ad92d724d642f56f97", "fixCommitParentSHA1": "97c17afccc1fc2b9e4876040d4affc093f4b3127", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\nindex ce4caba..b677072 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n@@ -187,7 +187,7 @@\n   protected FSNamesystem namesystem; \n   protected final Configuration conf;\n   protected NamenodeRole role;\n-  private HAState state;\n+  private volatile HAState state;\n   private final boolean haEnabled;\n   private final HAContext haContext;\n   protected boolean allowStaleStandbyReads;\n", "projectName": "apache.hadoop", "bugLineNum": 190, "bugNodeStartChar": 8362, "bugNodeLength": 22, "fixLineNum": 190, "fixNodeStartChar": 8362, "fixNodeLength": 31, "sourceBeforeFix": "2", "sourceAfterFix": "66"}, {"bugType": "SWAP_BOOLEAN_LITERAL", "fixCommitSHA1": "6d0b8df8802f95880b33ad6823598b01175e97f6", "fixCommitParentSHA1": "fbae7714cf93c0670662804fa2fae41d9712fceb", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\nindex e984d54..ea59ae4 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n@@ -207,7 +207,7 @@\n   /** Format a new filesystem.  Destroys any filesystem that may already\n    * exist at this location.  **/\n   public static void format(Configuration conf) throws IOException {\n-    format(conf, false);\n+    format(conf, true);\n   }\n \n   static NameNodeMetrics metrics;\n@@ -644,12 +644,12 @@\n    * for each existing directory and format them.\n    * \n    * @param conf\n-   * @param isConfirmationNeeded\n+   * @param force\n    * @return true if formatting was aborted, false otherwise\n    * @throws IOException\n    */\n   private static boolean format(Configuration conf,\n-                                boolean isConfirmationNeeded)\n+                                boolean force)\n       throws IOException {\n     String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n     String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n@@ -659,7 +659,7 @@\n     Collection<URI> dirsToFormat = FSNamesystem.getNamespaceDirs(conf);\n     List<URI> editDirsToFormat = \n                  FSNamesystem.getNamespaceEditsDirs(conf);\n-    if (!confirmFormat(dirsToFormat, isConfirmationNeeded, true)) {\n+    if (!confirmFormat(dirsToFormat, force, true)) {\n       return true; // aborted\n     }\n \n@@ -868,7 +868,7 @@\n \n     switch (startOpt) {\n       case FORMAT:\n-        boolean aborted = format(conf, true);\n+        boolean aborted = format(conf, false);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       case GENCLUSTERID:\n", "projectName": "apache.hadoop", "bugLineNum": 210, "bugNodeStartChar": 9030, "bugNodeLength": 19, "fixLineNum": 210, "fixNodeStartChar": 9030, "fixNodeLength": 18, "sourceBeforeFix": "format(conf,false)", "sourceAfterFix": "format(conf,true)"}, {"bugType": "SWAP_BOOLEAN_LITERAL", "fixCommitSHA1": "6d0b8df8802f95880b33ad6823598b01175e97f6", "fixCommitParentSHA1": "fbae7714cf93c0670662804fa2fae41d9712fceb", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\nindex e984d54..ea59ae4 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java\n@@ -207,7 +207,7 @@\n   /** Format a new filesystem.  Destroys any filesystem that may already\n    * exist at this location.  **/\n   public static void format(Configuration conf) throws IOException {\n-    format(conf, false);\n+    format(conf, true);\n   }\n \n   static NameNodeMetrics metrics;\n@@ -644,12 +644,12 @@\n    * for each existing directory and format them.\n    * \n    * @param conf\n-   * @param isConfirmationNeeded\n+   * @param force\n    * @return true if formatting was aborted, false otherwise\n    * @throws IOException\n    */\n   private static boolean format(Configuration conf,\n-                                boolean isConfirmationNeeded)\n+                                boolean force)\n       throws IOException {\n     String nsId = DFSUtil.getNamenodeNameServiceId(conf);\n     String namenodeId = HAUtil.getNameNodeId(conf, nsId);\n@@ -659,7 +659,7 @@\n     Collection<URI> dirsToFormat = FSNamesystem.getNamespaceDirs(conf);\n     List<URI> editDirsToFormat = \n                  FSNamesystem.getNamespaceEditsDirs(conf);\n-    if (!confirmFormat(dirsToFormat, isConfirmationNeeded, true)) {\n+    if (!confirmFormat(dirsToFormat, force, true)) {\n       return true; // aborted\n     }\n \n@@ -868,7 +868,7 @@\n \n     switch (startOpt) {\n       case FORMAT:\n-        boolean aborted = format(conf, true);\n+        boolean aborted = format(conf, false);\n         System.exit(aborted ? 1 : 0);\n         return null; // avoid javac warning\n       case GENCLUSTERID:\n", "projectName": "apache.hadoop", "bugLineNum": 871, "bugNodeStartChar": 31097, "bugNodeLength": 18, "fixLineNum": 871, "fixNodeStartChar": 31097, "fixNodeLength": 19, "sourceBeforeFix": "format(conf,true)", "sourceAfterFix": "format(conf,false)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "e9c32d2ad17def9bb7a4037bebe22d4c42c91b49", "fixCommitParentSHA1": "df164668782b8fd535be2a4fd11ee62218ed5a77", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/JobEndNotifier.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/JobEndNotifier.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/JobEndNotifier.java\nindex 9ffe218..ca7f625 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/JobEndNotifier.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/JobEndNotifier.java\n@@ -119,7 +119,8 @@\n     boolean success = false;\n     try {\n       Log.info(\"Job end notification trying \" + urlToNotify);\n-      HttpURLConnection conn = (HttpURLConnection) urlToNotify.openConnection();\n+      HttpURLConnection conn =\n+        (HttpURLConnection) urlToNotify.openConnection(proxyToUse);\n       conn.setConnectTimeout(5*1000);\n       conn.setReadTimeout(5*1000);\n       conn.setAllowUserInteraction(false);\n", "projectName": "apache.hadoop", "bugLineNum": 122, "bugNodeStartChar": 4768, "bugNodeLength": 28, "fixLineNum": 123, "fixNodeStartChar": 4776, "fixNodeLength": 38, "sourceBeforeFix": "urlToNotify.openConnection()", "sourceAfterFix": "urlToNotify.openConnection(proxyToUse)"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "63bd96d5f9a9e7f8283886dc8f0505ee21343955", "fixCommitParentSHA1": "65e6abb4775eb3e4abc38ca60d351ee286b5a944", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java\nindex c1e290c..0778e7f 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java\n@@ -514,7 +514,7 @@\n     for (int i = 0; i < mountPoints.size(); ++i) {\n       String serviceName =\n           mountPoints.get(i).target.targetFileSystem.getCanonicalServiceName();\n-      if (seenServiceNames.contains(serviceName)) {\n+      if (serviceName == null || seenServiceNames.contains(serviceName)) {\n         continue;\n       }\n       seenServiceNames.add(serviceName);\n", "projectName": "apache.hadoop", "bugLineNum": 517, "bugNodeStartChar": 18177, "bugNodeLength": 38, "fixLineNum": 517, "fixNodeStartChar": 18177, "fixNodeLength": 61, "sourceBeforeFix": "seenServiceNames.contains(serviceName)", "sourceAfterFix": "serviceName == null || seenServiceNames.contains(serviceName)"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "88f815fc54a2e1790c4e225dee68507cf015b67a", "fixCommitParentSHA1": "f340be0927b0e676ba9e53aa168cb264853c0efd", "bugFilePath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DefaultContainerExecutor.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DefaultContainerExecutor.java b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DefaultContainerExecutor.java\nindex bd95317..3d46053 100644\n--- a/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DefaultContainerExecutor.java\n+++ b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DefaultContainerExecutor.java\n@@ -75,7 +75,7 @@\n   }\n   \n   @Override\n-  public void startLocalizer(Path nmPrivateContainerTokensPath,\n+  public synchronized void startLocalizer(Path nmPrivateContainerTokensPath,\n       InetSocketAddress nmAddr, String user, String appId, String locId,\n       List<String> localDirs, List<String> logDirs)\n       throws IOException, InterruptedException {\n", "projectName": "apache.hadoop", "bugLineNum": 77, "bugNodeStartChar": 2809, "bugNodeLength": 1238, "fixLineNum": 77, "fixNodeStartChar": 2809, "fixNodeLength": 1251, "sourceBeforeFix": "1", "sourceAfterFix": "33"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "14a2aa36d5a5636320eae56b8b52738ea0f17a45", "fixCommitParentSHA1": "0991b7560e6527cc8d59ca3fd980ace973def431", "bugFilePath": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java", "fixPatch": "diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java\nindex 89652fc..e95ade8 100644\n--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java\n+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java\n@@ -119,7 +119,7 @@\n    * Get the token kind\n    * @return the kind of the token\n    */\n-  public Text getKind() {\n+  public synchronized Text getKind() {\n     return kind;\n   }\n \n", "projectName": "apache.hadoop", "bugLineNum": 118, "bugNodeStartChar": 3434, "bugNodeLength": 115, "fixLineNum": 118, "fixNodeStartChar": 3434, "fixNodeLength": 128, "sourceBeforeFix": "1", "sourceAfterFix": "33"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "3e04036dbec732f74124bc13e3f8fe146b7d5712", "fixCommitParentSHA1": "77b6810f49b38b5189117f7c251d6713ccbfd230", "bugFilePath": "hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java b/hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java\nindex a019be7..b68e73e 100644\n--- a/hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java\n+++ b/hadoop-mapreduce-project/src/contrib/streaming/src/java/org/apache/hadoop/streaming/StreamJob.java\n@@ -718,7 +718,7 @@\n     }\n \n     // general MapRed job properties\n-    jobConf_ = new JobConf(config_);\n+    jobConf_ = new JobConf(config_, StreamJob.class);\n     \n     // All streaming jobs get the task timeout value\n     // from the configuration settings.\n", "projectName": "apache.hadoop", "bugLineNum": 721, "bugNodeStartChar": 28062, "bugNodeLength": 20, "fixLineNum": 721, "fixNodeStartChar": 28062, "fixNodeLength": 37, "sourceBeforeFix": "new JobConf(config_)", "sourceAfterFix": "new JobConf(config_,StreamJob.class)"}, {"bugType": "CHANGE_CALLER_IN_FUNCTION_CALL", "fixCommitSHA1": "72ada35360fe7e9187c07d504edb6cc8675d49a2", "fixCommitParentSHA1": "e70025b903f269f57cc620f9c8a863067862aaaf", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java\nindex 97ae5f0..e90d6fb 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java\n@@ -309,7 +309,7 @@\n         final String js = JsonUtil.toJsonString(\"boolean\", b);\n         return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n       } else {\n-        np.rename(fullpath, dstPath.getValue(),\n+        np.rename(fullpath, destination.getValue(),\n             s.toArray(new Options.Rename[s.size()]));\n         return Response.ok().type(MediaType.APPLICATION_JSON).build();\n       }\n", "projectName": "apache.hadoop", "bugLineNum": 312, "bugNodeStartChar": 13870, "bugNodeLength": 18, "fixLineNum": 312, "fixNodeStartChar": 13870, "fixNodeLength": 22, "sourceBeforeFix": "dstPath.getValue()", "sourceAfterFix": "destination.getValue()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "72ada35360fe7e9187c07d504edb6cc8675d49a2", "fixCommitParentSHA1": "e70025b903f269f57cc620f9c8a863067862aaaf", "bugFilePath": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java", "fixPatch": "diff --git a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java\nindex 97ae5f0..e90d6fb 100644\n--- a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java\n+++ b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/web/resources/NamenodeWebHdfsMethods.java\n@@ -309,7 +309,7 @@\n         final String js = JsonUtil.toJsonString(\"boolean\", b);\n         return Response.ok(js).type(MediaType.APPLICATION_JSON).build();\n       } else {\n-        np.rename(fullpath, dstPath.getValue(),\n+        np.rename(fullpath, destination.getValue(),\n             s.toArray(new Options.Rename[s.size()]));\n         return Response.ok().type(MediaType.APPLICATION_JSON).build();\n       }\n", "projectName": "apache.hadoop", "bugLineNum": 312, "bugNodeStartChar": 13870, "bugNodeLength": 18, "fixLineNum": 312, "fixNodeStartChar": 13870, "fixNodeLength": 22, "sourceBeforeFix": "dstPath.getValue()", "sourceAfterFix": "destination.getValue()"}, {"bugType": "SWAP_BOOLEAN_LITERAL", "fixCommitSHA1": "1ae4238dc21de898b3649df294e59aca0ee764de", "fixCommitParentSHA1": "6a1deabbb764aca01a8b41cade5558332554a79d", "bugFilePath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java\nindex 23be2a0..aec4e19 100644\n--- a/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java\n+++ b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/conf/YarnConfiguration.java\n@@ -129,7 +129,7 @@\n   /** Are RM acls enabled.*/\n   public static final String RM_ACL_ENABLE = \n     RM_PREFIX + \"acl.enable\";\n-  public static final boolean DEFAULT_RM_ACL_ENABLE = false;\n+  public static final boolean DEFAULT_RM_ACL_ENABLE = true;\n   \n   /** ACL of who can be admin of RM.*/\n   public static final String RM_ADMIN_ACL = \n", "projectName": "apache.hadoop", "bugLineNum": 132, "bugNodeStartChar": 5066, "bugNodeLength": 29, "fixLineNum": 132, "fixNodeStartChar": 5066, "fixNodeLength": 28, "sourceBeforeFix": "DEFAULT_RM_ACL_ENABLE=false", "sourceAfterFix": "DEFAULT_RM_ACL_ENABLE=true"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "be05e014717ec6a9891bf65ce2b6a2cd51ce1577", "fixCommitParentSHA1": "38a4915dab14fac65b3036e6f4cae3c64665446a", "bugFilePath": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java\nindex cc9f6bd..95c344b 100644\n--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java\n+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java\n@@ -153,7 +153,7 @@\n   private Token<JobTokenIdentifier> jobToken;\n   private static AtomicBoolean initialClasspathFlag = new AtomicBoolean();\n   private static String initialClasspath = null;\n-  private final Object classpathLock = new Object();\n+  private static final Object classpathLock = new Object();\n   private long launchTime;\n   private long finishTime;\n   private WrappedProgressSplitsBlock progressSplitBlock;\n", "projectName": "apache.hadoop", "bugLineNum": 156, "bugNodeStartChar": 7929, "bugNodeLength": 50, "fixLineNum": 156, "fixNodeStartChar": 7929, "fixNodeLength": 57, "sourceBeforeFix": "18", "sourceAfterFix": "26"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "301e32209610a63a67d360ee1d9ee0d89ed9396a", "fixCommitParentSHA1": "65d5b0b9d3659270619ee8330bbaa6703e79f3ec", "bugFilePath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java\nindex bcd2115..585ec85 100644\n--- a/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java\n+++ b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java\n@@ -100,7 +100,8 @@\n       String appIdStr = app.toString();\n       Path containerLogDir =\n           this.logDirsSelector.getLocalPathForWrite(appIdStr + Path.SEPARATOR\n-              + containerIdStr, LocalDirAllocator.SIZE_UNKNOWN, this.conf);\n+              + containerIdStr, LocalDirAllocator.SIZE_UNKNOWN, this.conf, \n+              false);\n       for (String str : command) {\n         // TODO: Should we instead work via symlinks without this grammar?\n         newCmds.add(str.replace(ApplicationConstants.LOG_DIR_EXPANSION_VAR,\n@@ -147,7 +148,7 @@\n               + Path.SEPARATOR + user + Path.SEPARATOR\n               + ContainerLocalizer.APPCACHE + Path.SEPARATOR + appIdStr\n               + Path.SEPARATOR + containerIdStr,\n-              LocalDirAllocator.SIZE_UNKNOWN, this.conf);\n+              LocalDirAllocator.SIZE_UNKNOWN, this.conf, false);\n       try {\n         // /////////// Write out the container-script in the nmPrivate space.\n         String[] localDirs =\n", "projectName": "apache.hadoop", "bugLineNum": 102, "bugNodeStartChar": 4469, "bugNodeLength": 142, "fixLineNum": 102, "fixNodeStartChar": 4469, "fixNodeLength": 164, "sourceBeforeFix": "this.logDirsSelector.getLocalPathForWrite(appIdStr + Path.SEPARATOR + containerIdStr,LocalDirAllocator.SIZE_UNKNOWN,this.conf)", "sourceAfterFix": "this.logDirsSelector.getLocalPathForWrite(appIdStr + Path.SEPARATOR + containerIdStr,LocalDirAllocator.SIZE_UNKNOWN,this.conf,false)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "301e32209610a63a67d360ee1d9ee0d89ed9396a", "fixCommitParentSHA1": "65d5b0b9d3659270619ee8330bbaa6703e79f3ec", "bugFilePath": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java", "fixPatch": "diff --git a/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java\nindex bcd2115..585ec85 100644\n--- a/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java\n+++ b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java\n@@ -100,7 +100,8 @@\n       String appIdStr = app.toString();\n       Path containerLogDir =\n           this.logDirsSelector.getLocalPathForWrite(appIdStr + Path.SEPARATOR\n-              + containerIdStr, LocalDirAllocator.SIZE_UNKNOWN, this.conf);\n+              + containerIdStr, LocalDirAllocator.SIZE_UNKNOWN, this.conf, \n+              false);\n       for (String str : command) {\n         // TODO: Should we instead work via symlinks without this grammar?\n         newCmds.add(str.replace(ApplicationConstants.LOG_DIR_EXPANSION_VAR,\n@@ -147,7 +148,7 @@\n               + Path.SEPARATOR + user + Path.SEPARATOR\n               + ContainerLocalizer.APPCACHE + Path.SEPARATOR + appIdStr\n               + Path.SEPARATOR + containerIdStr,\n-              LocalDirAllocator.SIZE_UNKNOWN, this.conf);\n+              LocalDirAllocator.SIZE_UNKNOWN, this.conf, false);\n       try {\n         // /////////// Write out the container-script in the nmPrivate space.\n         String[] localDirs =\n", "projectName": "apache.hadoop", "bugLineNum": 146, "bugNodeStartChar": 6504, "bugNodeLength": 296, "fixLineNum": 146, "fixNodeStartChar": 6504, "fixNodeLength": 303, "sourceBeforeFix": "lDirAllocator.getLocalPathForWrite(ContainerLocalizer.USERCACHE + Path.SEPARATOR + user+ Path.SEPARATOR+ ContainerLocalizer.APPCACHE+ Path.SEPARATOR+ appIdStr+ Path.SEPARATOR+ containerIdStr,LocalDirAllocator.SIZE_UNKNOWN,this.conf)", "sourceAfterFix": "lDirAllocator.getLocalPathForWrite(ContainerLocalizer.USERCACHE + Path.SEPARATOR + user+ Path.SEPARATOR+ ContainerLocalizer.APPCACHE+ Path.SEPARATOR+ appIdStr+ Path.SEPARATOR+ containerIdStr,LocalDirAllocator.SIZE_UNKNOWN,this.conf,false)"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "ebf61db566ba2611f7fe2a19158d7d786fb7f868", "fixCommitParentSHA1": "fdae3cda8577d8f531c3064425df7556c3ad8aab", "bugFilePath": "hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLog.java", "fixPatch": "diff --git a/hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLog.java b/hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLog.java\nindex 810eea3..7e978e9 100644\n--- a/hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLog.java\n+++ b/hadoop-mapreduce/hadoop-mr-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLog.java\n@@ -183,9 +183,9 @@\n   private static long prevErrLength;\n   private static long prevLogLength;\n   \n-  private static void writeToIndexFile(String logLocation,\n-                                       boolean isCleanup) \n-  throws IOException {\n+  private static synchronized \n+  void writeToIndexFile(String logLocation,\n+                        boolean isCleanup) throws IOException {\n     // To ensure atomicity of updates to index file, write to temporary index\n     // file first and then rename.\n     File tmpIndexFile = getTmpIndexFile(currentTaskid, isCleanup);\n", "projectName": "apache.hadoop", "bugLineNum": 186, "bugNodeStartChar": 6516, "bugNodeLength": 1999, "fixLineNum": 186, "fixNodeStartChar": 6516, "fixNodeLength": 1997, "sourceBeforeFix": "10", "sourceAfterFix": "42"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "4bb06fbd0a73484f025b1c254c9b44ddee9b19af", "fixCommitParentSHA1": "95649aca30b28497e0838b7d60688227d6ddd86f", "bugFilePath": "src/java/org/apache/hadoop/io/SetFile.java", "fixPatch": "diff --git a/src/java/org/apache/hadoop/io/SetFile.java b/src/java/org/apache/hadoop/io/SetFile.java\nindex c7a0446..e4d261a 100644\n--- a/src/java/org/apache/hadoop/io/SetFile.java\n+++ b/src/java/org/apache/hadoop/io/SetFile.java\n@@ -59,7 +59,7 @@\n                   SequenceFile.CompressionType compress) throws IOException {\n       super(conf, new Path(dirName), \n             comparator(comparator), \n-            keyClass(NullWritable.class), \n+            valueClass(NullWritable.class), \n             compressionType(compress));\n     }\n \n", "projectName": "apache.hadoop", "bugLineNum": 62, "bugNodeStartChar": 2376, "bugNodeLength": 28, "fixLineNum": 62, "fixNodeStartChar": 2376, "fixNodeLength": 30, "sourceBeforeFix": "keyClass(NullWritable.class)", "sourceAfterFix": "valueClass(NullWritable.class)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "4bb06fbd0a73484f025b1c254c9b44ddee9b19af", "fixCommitParentSHA1": "95649aca30b28497e0838b7d60688227d6ddd86f", "bugFilePath": "src/java/org/apache/hadoop/io/SetFile.java", "fixPatch": "diff --git a/src/java/org/apache/hadoop/io/SetFile.java b/src/java/org/apache/hadoop/io/SetFile.java\nindex c7a0446..e4d261a 100644\n--- a/src/java/org/apache/hadoop/io/SetFile.java\n+++ b/src/java/org/apache/hadoop/io/SetFile.java\n@@ -59,7 +59,7 @@\n                   SequenceFile.CompressionType compress) throws IOException {\n       super(conf, new Path(dirName), \n             comparator(comparator), \n-            keyClass(NullWritable.class), \n+            valueClass(NullWritable.class), \n             compressionType(compress));\n     }\n \n", "projectName": "apache.hadoop", "bugLineNum": 62, "bugNodeStartChar": 2376, "bugNodeLength": 28, "fixLineNum": 62, "fixNodeStartChar": 2376, "fixNodeLength": 30, "sourceBeforeFix": "keyClass(NullWritable.class)", "sourceAfterFix": "valueClass(NullWritable.class)"}]