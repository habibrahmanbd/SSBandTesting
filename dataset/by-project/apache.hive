[{"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "198ab0e862cdf33e8dff37bf24676ffcb392ed82", "fixCommitParentSHA1": "96dc42999619a4c313e769e5335f6fbefb3d9167", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java\nindex 7ede4c8..37c3714 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java\n@@ -702,7 +702,7 @@\n     // 11. Finally, for all the pools that have changes, promote queued queries and rebalance.\n     for (String poolName : poolsToRedistribute) {\n       if (LOG.isDebugEnabled()) {\n-        LOG.info(\"Processing changes for pool \" + poolName + \": \" + pools.get(poolName));\n+        LOG.debug(\"Processing changes for pool \" + poolName + \": \" + pools.get(poolName));\n       }\n       processPoolChangesOnMasterThread(poolName, hasRequeues, syncWork);\n     }\n", "projectName": "apache.hive", "bugLineNum": 705, "bugNodeStartChar": 31940, "bugNodeLength": 80, "fixLineNum": 705, "fixNodeStartChar": 31940, "fixNodeLength": 81, "sourceBeforeFix": "LOG.info(\"Processing changes for pool \" + poolName + \": \"+ pools.get(poolName))", "sourceAfterFix": "LOG.debug(\"Processing changes for pool \" + poolName + \": \"+ pools.get(poolName))"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "198ab0e862cdf33e8dff37bf24676ffcb392ed82", "fixCommitParentSHA1": "96dc42999619a4c313e769e5335f6fbefb3d9167", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java\nindex 7ede4c8..37c3714 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java\n@@ -702,7 +702,7 @@\n     // 11. Finally, for all the pools that have changes, promote queued queries and rebalance.\n     for (String poolName : poolsToRedistribute) {\n       if (LOG.isDebugEnabled()) {\n-        LOG.info(\"Processing changes for pool \" + poolName + \": \" + pools.get(poolName));\n+        LOG.debug(\"Processing changes for pool \" + poolName + \": \" + pools.get(poolName));\n       }\n       processPoolChangesOnMasterThread(poolName, hasRequeues, syncWork);\n     }\n", "projectName": "apache.hive", "bugLineNum": 705, "bugNodeStartChar": 31940, "bugNodeLength": 80, "fixLineNum": 705, "fixNodeStartChar": 31940, "fixNodeLength": 81, "sourceBeforeFix": "LOG.info(\"Processing changes for pool \" + poolName + \": \"+ pools.get(poolName))", "sourceAfterFix": "LOG.debug(\"Processing changes for pool \" + poolName + \": \"+ pools.get(poolName))"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "12a6f937cb623ec89e570883010340016c5493a3", "fixCommitParentSHA1": "b789aebcfbf19d7b0fd6c2d6643adfd5a8de5f12", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java\nindex 596640e..7b23240 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java\n@@ -432,7 +432,7 @@\n     // Find the key/value where the ExprNodeDesc value matches the column we are searching for.\n     // The key portion of the entry will be the internal column name for the join key expression.\n     for (Map.Entry<String, ExprNodeDesc> mapEntry : reduceSinkOp.getColumnExprMap().entrySet()) {\n-      if (mapEntry.getValue().isSame(source)) {\n+      if (mapEntry.getValue().equals(source)) {\n         String columnInternalName = mapEntry.getKey();\n         if (source instanceof ExprNodeColumnDesc) {\n           // The join key is a table column. Create the ExprNodeDesc based on this column.\n", "projectName": "apache.hive", "bugLineNum": 435, "bugNodeStartChar": 16455, "bugNodeLength": 34, "fixLineNum": 435, "fixNodeStartChar": 16455, "fixNodeLength": 34, "sourceBeforeFix": "mapEntry.getValue().isSame(source)", "sourceAfterFix": "mapEntry.getValue().equals(source)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "12a6f937cb623ec89e570883010340016c5493a3", "fixCommitParentSHA1": "b789aebcfbf19d7b0fd6c2d6643adfd5a8de5f12", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java\nindex 596640e..7b23240 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDescUtils.java\n@@ -432,7 +432,7 @@\n     // Find the key/value where the ExprNodeDesc value matches the column we are searching for.\n     // The key portion of the entry will be the internal column name for the join key expression.\n     for (Map.Entry<String, ExprNodeDesc> mapEntry : reduceSinkOp.getColumnExprMap().entrySet()) {\n-      if (mapEntry.getValue().isSame(source)) {\n+      if (mapEntry.getValue().equals(source)) {\n         String columnInternalName = mapEntry.getKey();\n         if (source instanceof ExprNodeColumnDesc) {\n           // The join key is a table column. Create the ExprNodeDesc based on this column.\n", "projectName": "apache.hive", "bugLineNum": 435, "bugNodeStartChar": 16455, "bugNodeLength": 34, "fixLineNum": 435, "fixNodeStartChar": 16455, "fixNodeLength": 34, "sourceBeforeFix": "mapEntry.getValue().isSame(source)", "sourceAfterFix": "mapEntry.getValue().equals(source)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "0f2f999bbd2862ff61ce62a3b4047af6621030be", "fixCommitParentSHA1": "34de7ac80f9b6a533118af743e924c491c116d5c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\nindex e7256cc..d5a30da 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java\n@@ -201,7 +201,7 @@\n   public void addPathToAlias(Path path, String newAlias){\n     ArrayList<String> aliases = pathToAliases.get(path);\n     if (aliases == null) {\n-      aliases = new ArrayList<>();\n+      aliases = new ArrayList<>(1);\n       StringInternUtils.internUriStringsInPath(path);\n       pathToAliases.put(path, aliases);\n     }\n", "projectName": "apache.hive", "bugLineNum": 204, "bugNodeStartChar": 7674, "bugNodeLength": 17, "fixLineNum": 204, "fixNodeStartChar": 7674, "fixNodeLength": 18, "sourceBeforeFix": "new ArrayList<>()", "sourceAfterFix": "new ArrayList<>(1)"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "0656040933973f55afa8b6e8f53140b6f76b7446", "fixCommitParentSHA1": "9a5f1c2448fcffb449a753160ef453a430e21847", "bugFilePath": "standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java", "fixPatch": "diff --git a/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java b/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java\nindex 7ab64ea..47f96f3 100644\n--- a/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java\n+++ b/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java\n@@ -206576,7 +206576,7 @@\n   @org.apache.hadoop.classification.InterfaceAudience.Public @org.apache.hadoop.classification.InterfaceStability.Stable public static class add_write_notification_log_args implements org.apache.thrift.TBase<add_write_notification_log_args, add_write_notification_log_args._Fields>, java.io.Serializable, Cloneable, Comparable<add_write_notification_log_args>   {\n     private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(\"add_write_notification_log_args\");\n \n-    private static final org.apache.thrift.protocol.TField RQST_FIELD_DESC = new org.apache.thrift.protocol.TField(\"rqst\", org.apache.thrift.protocol.TType.STRUCT, (short)-1);\n+    private static final org.apache.thrift.protocol.TField RQST_FIELD_DESC = new org.apache.thrift.protocol.TField(\"rqst\", org.apache.thrift.protocol.TType.STRUCT, (short)1);\n \n     private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n     static {\n@@ -206588,7 +206588,7 @@\n \n     /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */\n     public enum _Fields implements org.apache.thrift.TFieldIdEnum {\n-      RQST((short)-1, \"rqst\");\n+      RQST((short)1, \"rqst\");\n \n       private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();\n \n@@ -206603,7 +206603,7 @@\n        */\n       public static _Fields findByThriftId(int fieldId) {\n         switch(fieldId) {\n-          case -1: // RQST\n+          case 1: // RQST\n             return RQST;\n           default:\n             return null;\n@@ -206868,7 +206868,7 @@\n             break;\n           }\n           switch (schemeField.id) {\n-            case -1: // RQST\n+            case 1: // RQST\n               if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {\n                 struct.rqst = new WriteNotificationLogRequest();\n                 struct.rqst.read(iprot);\n", "projectName": "apache.hive", "bugLineNum": 206579, "bugNodeStartChar": 7300823, "bugNodeLength": 2, "fixLineNum": 206579, "fixNodeStartChar": 7300823, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "0656040933973f55afa8b6e8f53140b6f76b7446", "fixCommitParentSHA1": "9a5f1c2448fcffb449a753160ef453a430e21847", "bugFilePath": "standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java", "fixPatch": "diff --git a/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java b/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java\nindex 7ab64ea..47f96f3 100644\n--- a/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java\n+++ b/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java\n@@ -206576,7 +206576,7 @@\n   @org.apache.hadoop.classification.InterfaceAudience.Public @org.apache.hadoop.classification.InterfaceStability.Stable public static class add_write_notification_log_args implements org.apache.thrift.TBase<add_write_notification_log_args, add_write_notification_log_args._Fields>, java.io.Serializable, Cloneable, Comparable<add_write_notification_log_args>   {\n     private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(\"add_write_notification_log_args\");\n \n-    private static final org.apache.thrift.protocol.TField RQST_FIELD_DESC = new org.apache.thrift.protocol.TField(\"rqst\", org.apache.thrift.protocol.TType.STRUCT, (short)-1);\n+    private static final org.apache.thrift.protocol.TField RQST_FIELD_DESC = new org.apache.thrift.protocol.TField(\"rqst\", org.apache.thrift.protocol.TType.STRUCT, (short)1);\n \n     private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n     static {\n@@ -206588,7 +206588,7 @@\n \n     /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */\n     public enum _Fields implements org.apache.thrift.TFieldIdEnum {\n-      RQST((short)-1, \"rqst\");\n+      RQST((short)1, \"rqst\");\n \n       private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();\n \n@@ -206603,7 +206603,7 @@\n        */\n       public static _Fields findByThriftId(int fieldId) {\n         switch(fieldId) {\n-          case -1: // RQST\n+          case 1: // RQST\n             return RQST;\n           default:\n             return null;\n@@ -206868,7 +206868,7 @@\n             break;\n           }\n           switch (schemeField.id) {\n-            case -1: // RQST\n+            case 1: // RQST\n               if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {\n                 struct.rqst = new WriteNotificationLogRequest();\n                 struct.rqst.read(iprot);\n", "projectName": "apache.hive", "bugLineNum": 206591, "bugNodeStartChar": 7301443, "bugNodeLength": 2, "fixLineNum": 206591, "fixNodeStartChar": 7301443, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "0656040933973f55afa8b6e8f53140b6f76b7446", "fixCommitParentSHA1": "9a5f1c2448fcffb449a753160ef453a430e21847", "bugFilePath": "standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java", "fixPatch": "diff --git a/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java b/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java\nindex 7ab64ea..47f96f3 100644\n--- a/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java\n+++ b/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java\n@@ -206576,7 +206576,7 @@\n   @org.apache.hadoop.classification.InterfaceAudience.Public @org.apache.hadoop.classification.InterfaceStability.Stable public static class add_write_notification_log_args implements org.apache.thrift.TBase<add_write_notification_log_args, add_write_notification_log_args._Fields>, java.io.Serializable, Cloneable, Comparable<add_write_notification_log_args>   {\n     private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(\"add_write_notification_log_args\");\n \n-    private static final org.apache.thrift.protocol.TField RQST_FIELD_DESC = new org.apache.thrift.protocol.TField(\"rqst\", org.apache.thrift.protocol.TType.STRUCT, (short)-1);\n+    private static final org.apache.thrift.protocol.TField RQST_FIELD_DESC = new org.apache.thrift.protocol.TField(\"rqst\", org.apache.thrift.protocol.TType.STRUCT, (short)1);\n \n     private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n     static {\n@@ -206588,7 +206588,7 @@\n \n     /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */\n     public enum _Fields implements org.apache.thrift.TFieldIdEnum {\n-      RQST((short)-1, \"rqst\");\n+      RQST((short)1, \"rqst\");\n \n       private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();\n \n@@ -206603,7 +206603,7 @@\n        */\n       public static _Fields findByThriftId(int fieldId) {\n         switch(fieldId) {\n-          case -1: // RQST\n+          case 1: // RQST\n             return RQST;\n           default:\n             return null;\n@@ -206868,7 +206868,7 @@\n             break;\n           }\n           switch (schemeField.id) {\n-            case -1: // RQST\n+            case 1: // RQST\n               if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {\n                 struct.rqst = new WriteNotificationLogRequest();\n                 struct.rqst.read(iprot);\n", "projectName": "apache.hive", "bugLineNum": 206606, "bugNodeStartChar": 7301895, "bugNodeLength": 2, "fixLineNum": 206606, "fixNodeStartChar": 7301895, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "0656040933973f55afa8b6e8f53140b6f76b7446", "fixCommitParentSHA1": "9a5f1c2448fcffb449a753160ef453a430e21847", "bugFilePath": "standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java", "fixPatch": "diff --git a/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java b/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java\nindex 7ab64ea..47f96f3 100644\n--- a/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java\n+++ b/standalone-metastore/metastore-common/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/metastore/api/ThriftHiveMetastore.java\n@@ -206576,7 +206576,7 @@\n   @org.apache.hadoop.classification.InterfaceAudience.Public @org.apache.hadoop.classification.InterfaceStability.Stable public static class add_write_notification_log_args implements org.apache.thrift.TBase<add_write_notification_log_args, add_write_notification_log_args._Fields>, java.io.Serializable, Cloneable, Comparable<add_write_notification_log_args>   {\n     private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(\"add_write_notification_log_args\");\n \n-    private static final org.apache.thrift.protocol.TField RQST_FIELD_DESC = new org.apache.thrift.protocol.TField(\"rqst\", org.apache.thrift.protocol.TType.STRUCT, (short)-1);\n+    private static final org.apache.thrift.protocol.TField RQST_FIELD_DESC = new org.apache.thrift.protocol.TField(\"rqst\", org.apache.thrift.protocol.TType.STRUCT, (short)1);\n \n     private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();\n     static {\n@@ -206588,7 +206588,7 @@\n \n     /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */\n     public enum _Fields implements org.apache.thrift.TFieldIdEnum {\n-      RQST((short)-1, \"rqst\");\n+      RQST((short)1, \"rqst\");\n \n       private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();\n \n@@ -206603,7 +206603,7 @@\n        */\n       public static _Fields findByThriftId(int fieldId) {\n         switch(fieldId) {\n-          case -1: // RQST\n+          case 1: // RQST\n             return RQST;\n           default:\n             return null;\n@@ -206868,7 +206868,7 @@\n             break;\n           }\n           switch (schemeField.id) {\n-            case -1: // RQST\n+            case 1: // RQST\n               if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {\n                 struct.rqst = new WriteNotificationLogRequest();\n                 struct.rqst.read(iprot);\n", "projectName": "apache.hive", "bugLineNum": 206871, "bugNodeStartChar": 7309781, "bugNodeLength": 2, "fixLineNum": 206871, "fixNodeStartChar": 7309781, "fixNodeLength": 1, "sourceBeforeFix": "-1", "sourceAfterFix": "1"}, {"bugType": "OVERLOAD_METHOD_DELETED_ARGS", "fixCommitSHA1": "e8d7cdcc372e14f8a0a664911b5ae6934201e30b", "fixCommitParentSHA1": "b17a3471c93216976a9224c2c827b72e45c9d37d", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java\nindex f34cb61..a50ec18 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java\n@@ -447,7 +447,7 @@\n     }\n     // TODO: we should probably skip updating if writeId is from an active txn\n     boolean isTxnValid = (writeIdString == null) || ObjectStore.isCurrentStatsValidForTheQuery(\n-        conf, db, tbl, params, statsWriteId , writeIdString, false);\n+        conf, params, statsWriteId , writeIdString, false);\n     return getExistingStatsToUpdate(existingStats, params, isTxnValid);\n   }\n \n@@ -472,7 +472,7 @@\n     }\n     // TODO: we should probably skip updating if writeId is from an active txn\n     if (writeIdString != null && !ObjectStore.isCurrentStatsValidForTheQuery(\n-        conf, db, tbl, params, statsWriteId, writeIdString, false)) {\n+        conf, params, statsWriteId, writeIdString, false)) {\n       return allCols;\n     }\n     List<String> colsToUpdate = new ArrayList<>();\n", "projectName": "apache.hive", "bugLineNum": 449, "bugNodeStartChar": 18731, "bugNodeLength": 111, "fixLineNum": 449, "fixNodeStartChar": 18731, "fixNodeLength": 102, "sourceBeforeFix": "ObjectStore.isCurrentStatsValidForTheQuery(conf,db,tbl,params,statsWriteId,writeIdString,false)", "sourceAfterFix": "ObjectStore.isCurrentStatsValidForTheQuery(conf,params,statsWriteId,writeIdString,false)"}, {"bugType": "OVERLOAD_METHOD_DELETED_ARGS", "fixCommitSHA1": "e8d7cdcc372e14f8a0a664911b5ae6934201e30b", "fixCommitParentSHA1": "b17a3471c93216976a9224c2c827b72e45c9d37d", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java\nindex f34cb61..a50ec18 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUpdaterThread.java\n@@ -447,7 +447,7 @@\n     }\n     // TODO: we should probably skip updating if writeId is from an active txn\n     boolean isTxnValid = (writeIdString == null) || ObjectStore.isCurrentStatsValidForTheQuery(\n-        conf, db, tbl, params, statsWriteId , writeIdString, false);\n+        conf, params, statsWriteId , writeIdString, false);\n     return getExistingStatsToUpdate(existingStats, params, isTxnValid);\n   }\n \n@@ -472,7 +472,7 @@\n     }\n     // TODO: we should probably skip updating if writeId is from an active txn\n     if (writeIdString != null && !ObjectStore.isCurrentStatsValidForTheQuery(\n-        conf, db, tbl, params, statsWriteId, writeIdString, false)) {\n+        conf, params, statsWriteId, writeIdString, false)) {\n       return allCols;\n     }\n     List<String> colsToUpdate = new ArrayList<>();\n", "projectName": "apache.hive", "bugLineNum": 474, "bugNodeStartChar": 19940, "bugNodeLength": 110, "fixLineNum": 474, "fixNodeStartChar": 19940, "fixNodeLength": 101, "sourceBeforeFix": "ObjectStore.isCurrentStatsValidForTheQuery(conf,db,tbl,params,statsWriteId,writeIdString,false)", "sourceAfterFix": "ObjectStore.isCurrentStatsValidForTheQuery(conf,params,statsWriteId,writeIdString,false)"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "6340a81f487c407f63cf2a00e39b2f905328d74e", "fixCommitParentSHA1": "4a0814b7f2edccb98f028a1528fc45c31a0d286f", "bugFilePath": "itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java", "fixPatch": "diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\nindex d883e4b..92c1292 100644\n--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\n+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\n@@ -119,7 +119,7 @@\n       List<HivePrivilegeObject> privObjs) throws SemanticException {\n     List<HivePrivilegeObject> needRewritePrivObjs = new ArrayList<>(); \n     for (HivePrivilegeObject privObj : privObjs) {\n-      if (privObj.getObjectName().equals(\"masking_test\")) {\n+      if (privObj.getObjectName().equals(\"masking_test\") || privObj.getObjectName().startsWith(\"masking_test_n\")) {\n         privObj.setRowFilterExpression(\"key % 2 = 0 and key < 10\");\n         List<String> cellValueTransformers = new ArrayList<>();\n         for (String columnName : privObj.getColumns()) {\n@@ -131,7 +131,7 @@\n         }\n         privObj.setCellValueTransformers(cellValueTransformers);\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_view\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_view\") || privObj.getObjectName().startsWith(\"masking_test_view_n\")) {\n         privObj.setRowFilterExpression(\"key > 6\");\n         List<String> cellValueTransformers = new ArrayList<>();\n         for (String columnName : privObj.getColumns()) {\n@@ -143,14 +143,14 @@\n         }\n         privObj.setCellValueTransformers(cellValueTransformers);\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_subq\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_subq\") || privObj.getObjectName().startsWith(\"masking_test_subq_n\")) {\n         privObj\n-            .setRowFilterExpression(\"key in (select key from src where src.key = masking_test_subq.key)\");\n+            .setRowFilterExpression(\"key in (select key from src where src.key = \" + privObj.getObjectName() + \".key)\");\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_acid_no_masking\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_acid_no_masking\") || privObj.getObjectName().startsWith(\"masking_acid_no_masking_n\")) {\n         // testing acid usage when no masking/filtering is present\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_druid\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_druid\") || privObj.getObjectName().startsWith(\"masking_test_druid_n\")) {\n         // testing druid queries row filtering is present\n         privObj.setRowFilterExpression(\"key > 10\");\n         needRewritePrivObjs.add(privObj);\n", "projectName": "apache.hive", "bugLineNum": 122, "bugNodeStartChar": 5292, "bugNodeLength": 46, "fixLineNum": 122, "fixNodeStartChar": 5292, "fixNodeLength": 102, "sourceBeforeFix": "privObj.getObjectName().equals(\"masking_test\")", "sourceAfterFix": "privObj.getObjectName().equals(\"masking_test\") || privObj.getObjectName().startsWith(\"masking_test_n\")"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "6340a81f487c407f63cf2a00e39b2f905328d74e", "fixCommitParentSHA1": "4a0814b7f2edccb98f028a1528fc45c31a0d286f", "bugFilePath": "itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java", "fixPatch": "diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\nindex d883e4b..92c1292 100644\n--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\n+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\n@@ -119,7 +119,7 @@\n       List<HivePrivilegeObject> privObjs) throws SemanticException {\n     List<HivePrivilegeObject> needRewritePrivObjs = new ArrayList<>(); \n     for (HivePrivilegeObject privObj : privObjs) {\n-      if (privObj.getObjectName().equals(\"masking_test\")) {\n+      if (privObj.getObjectName().equals(\"masking_test\") || privObj.getObjectName().startsWith(\"masking_test_n\")) {\n         privObj.setRowFilterExpression(\"key % 2 = 0 and key < 10\");\n         List<String> cellValueTransformers = new ArrayList<>();\n         for (String columnName : privObj.getColumns()) {\n@@ -131,7 +131,7 @@\n         }\n         privObj.setCellValueTransformers(cellValueTransformers);\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_view\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_view\") || privObj.getObjectName().startsWith(\"masking_test_view_n\")) {\n         privObj.setRowFilterExpression(\"key > 6\");\n         List<String> cellValueTransformers = new ArrayList<>();\n         for (String columnName : privObj.getColumns()) {\n@@ -143,14 +143,14 @@\n         }\n         privObj.setCellValueTransformers(cellValueTransformers);\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_subq\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_subq\") || privObj.getObjectName().startsWith(\"masking_test_subq_n\")) {\n         privObj\n-            .setRowFilterExpression(\"key in (select key from src where src.key = masking_test_subq.key)\");\n+            .setRowFilterExpression(\"key in (select key from src where src.key = \" + privObj.getObjectName() + \".key)\");\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_acid_no_masking\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_acid_no_masking\") || privObj.getObjectName().startsWith(\"masking_acid_no_masking_n\")) {\n         // testing acid usage when no masking/filtering is present\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_druid\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_druid\") || privObj.getObjectName().startsWith(\"masking_test_druid_n\")) {\n         // testing druid queries row filtering is present\n         privObj.setRowFilterExpression(\"key > 10\");\n         needRewritePrivObjs.add(privObj);\n", "projectName": "apache.hive", "bugLineNum": 134, "bugNodeStartChar": 5848, "bugNodeLength": 51, "fixLineNum": 134, "fixNodeStartChar": 5848, "fixNodeLength": 112, "sourceBeforeFix": "privObj.getObjectName().equals(\"masking_test_view\")", "sourceAfterFix": "privObj.getObjectName().equals(\"masking_test_view\") || privObj.getObjectName().startsWith(\"masking_test_view_n\")"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "6340a81f487c407f63cf2a00e39b2f905328d74e", "fixCommitParentSHA1": "4a0814b7f2edccb98f028a1528fc45c31a0d286f", "bugFilePath": "itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java", "fixPatch": "diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\nindex d883e4b..92c1292 100644\n--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\n+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\n@@ -119,7 +119,7 @@\n       List<HivePrivilegeObject> privObjs) throws SemanticException {\n     List<HivePrivilegeObject> needRewritePrivObjs = new ArrayList<>(); \n     for (HivePrivilegeObject privObj : privObjs) {\n-      if (privObj.getObjectName().equals(\"masking_test\")) {\n+      if (privObj.getObjectName().equals(\"masking_test\") || privObj.getObjectName().startsWith(\"masking_test_n\")) {\n         privObj.setRowFilterExpression(\"key % 2 = 0 and key < 10\");\n         List<String> cellValueTransformers = new ArrayList<>();\n         for (String columnName : privObj.getColumns()) {\n@@ -131,7 +131,7 @@\n         }\n         privObj.setCellValueTransformers(cellValueTransformers);\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_view\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_view\") || privObj.getObjectName().startsWith(\"masking_test_view_n\")) {\n         privObj.setRowFilterExpression(\"key > 6\");\n         List<String> cellValueTransformers = new ArrayList<>();\n         for (String columnName : privObj.getColumns()) {\n@@ -143,14 +143,14 @@\n         }\n         privObj.setCellValueTransformers(cellValueTransformers);\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_subq\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_subq\") || privObj.getObjectName().startsWith(\"masking_test_subq_n\")) {\n         privObj\n-            .setRowFilterExpression(\"key in (select key from src where src.key = masking_test_subq.key)\");\n+            .setRowFilterExpression(\"key in (select key from src where src.key = \" + privObj.getObjectName() + \".key)\");\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_acid_no_masking\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_acid_no_masking\") || privObj.getObjectName().startsWith(\"masking_acid_no_masking_n\")) {\n         // testing acid usage when no masking/filtering is present\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_druid\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_druid\") || privObj.getObjectName().startsWith(\"masking_test_druid_n\")) {\n         // testing druid queries row filtering is present\n         privObj.setRowFilterExpression(\"key > 10\");\n         needRewritePrivObjs.add(privObj);\n", "projectName": "apache.hive", "bugLineNum": 146, "bugNodeStartChar": 6383, "bugNodeLength": 51, "fixLineNum": 146, "fixNodeStartChar": 6383, "fixNodeLength": 112, "sourceBeforeFix": "privObj.getObjectName().equals(\"masking_test_subq\")", "sourceAfterFix": "privObj.getObjectName().equals(\"masking_test_subq\") || privObj.getObjectName().startsWith(\"masking_test_subq_n\")"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "6340a81f487c407f63cf2a00e39b2f905328d74e", "fixCommitParentSHA1": "4a0814b7f2edccb98f028a1528fc45c31a0d286f", "bugFilePath": "itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java", "fixPatch": "diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\nindex d883e4b..92c1292 100644\n--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\n+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\n@@ -119,7 +119,7 @@\n       List<HivePrivilegeObject> privObjs) throws SemanticException {\n     List<HivePrivilegeObject> needRewritePrivObjs = new ArrayList<>(); \n     for (HivePrivilegeObject privObj : privObjs) {\n-      if (privObj.getObjectName().equals(\"masking_test\")) {\n+      if (privObj.getObjectName().equals(\"masking_test\") || privObj.getObjectName().startsWith(\"masking_test_n\")) {\n         privObj.setRowFilterExpression(\"key % 2 = 0 and key < 10\");\n         List<String> cellValueTransformers = new ArrayList<>();\n         for (String columnName : privObj.getColumns()) {\n@@ -131,7 +131,7 @@\n         }\n         privObj.setCellValueTransformers(cellValueTransformers);\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_view\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_view\") || privObj.getObjectName().startsWith(\"masking_test_view_n\")) {\n         privObj.setRowFilterExpression(\"key > 6\");\n         List<String> cellValueTransformers = new ArrayList<>();\n         for (String columnName : privObj.getColumns()) {\n@@ -143,14 +143,14 @@\n         }\n         privObj.setCellValueTransformers(cellValueTransformers);\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_subq\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_subq\") || privObj.getObjectName().startsWith(\"masking_test_subq_n\")) {\n         privObj\n-            .setRowFilterExpression(\"key in (select key from src where src.key = masking_test_subq.key)\");\n+            .setRowFilterExpression(\"key in (select key from src where src.key = \" + privObj.getObjectName() + \".key)\");\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_acid_no_masking\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_acid_no_masking\") || privObj.getObjectName().startsWith(\"masking_acid_no_masking_n\")) {\n         // testing acid usage when no masking/filtering is present\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_druid\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_druid\") || privObj.getObjectName().startsWith(\"masking_test_druid_n\")) {\n         // testing druid queries row filtering is present\n         privObj.setRowFilterExpression(\"key > 10\");\n         needRewritePrivObjs.add(privObj);\n", "projectName": "apache.hive", "bugLineNum": 150, "bugNodeStartChar": 6620, "bugNodeLength": 57, "fixLineNum": 150, "fixNodeStartChar": 6620, "fixNodeLength": 124, "sourceBeforeFix": "privObj.getObjectName().equals(\"masking_acid_no_masking\")", "sourceAfterFix": "privObj.getObjectName().equals(\"masking_acid_no_masking\") || privObj.getObjectName().startsWith(\"masking_acid_no_masking_n\")"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "6340a81f487c407f63cf2a00e39b2f905328d74e", "fixCommitParentSHA1": "4a0814b7f2edccb98f028a1528fc45c31a0d286f", "bugFilePath": "itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java", "fixPatch": "diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\nindex d883e4b..92c1292 100644\n--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\n+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAuthorizationValidatorForTest.java\n@@ -119,7 +119,7 @@\n       List<HivePrivilegeObject> privObjs) throws SemanticException {\n     List<HivePrivilegeObject> needRewritePrivObjs = new ArrayList<>(); \n     for (HivePrivilegeObject privObj : privObjs) {\n-      if (privObj.getObjectName().equals(\"masking_test\")) {\n+      if (privObj.getObjectName().equals(\"masking_test\") || privObj.getObjectName().startsWith(\"masking_test_n\")) {\n         privObj.setRowFilterExpression(\"key % 2 = 0 and key < 10\");\n         List<String> cellValueTransformers = new ArrayList<>();\n         for (String columnName : privObj.getColumns()) {\n@@ -131,7 +131,7 @@\n         }\n         privObj.setCellValueTransformers(cellValueTransformers);\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_view\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_view\") || privObj.getObjectName().startsWith(\"masking_test_view_n\")) {\n         privObj.setRowFilterExpression(\"key > 6\");\n         List<String> cellValueTransformers = new ArrayList<>();\n         for (String columnName : privObj.getColumns()) {\n@@ -143,14 +143,14 @@\n         }\n         privObj.setCellValueTransformers(cellValueTransformers);\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_subq\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_subq\") || privObj.getObjectName().startsWith(\"masking_test_subq_n\")) {\n         privObj\n-            .setRowFilterExpression(\"key in (select key from src where src.key = masking_test_subq.key)\");\n+            .setRowFilterExpression(\"key in (select key from src where src.key = \" + privObj.getObjectName() + \".key)\");\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_acid_no_masking\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_acid_no_masking\") || privObj.getObjectName().startsWith(\"masking_acid_no_masking_n\")) {\n         // testing acid usage when no masking/filtering is present\n         needRewritePrivObjs.add(privObj);\n-      } else if (privObj.getObjectName().equals(\"masking_test_druid\")) {\n+      } else if (privObj.getObjectName().equals(\"masking_test_druid\") || privObj.getObjectName().startsWith(\"masking_test_druid_n\")) {\n         // testing druid queries row filtering is present\n         privObj.setRowFilterExpression(\"key > 10\");\n         needRewritePrivObjs.add(privObj);\n", "projectName": "apache.hive", "bugLineNum": 153, "bugNodeStartChar": 6807, "bugNodeLength": 52, "fixLineNum": 153, "fixNodeStartChar": 6807, "fixNodeLength": 114, "sourceBeforeFix": "privObj.getObjectName().equals(\"masking_test_druid\")", "sourceAfterFix": "privObj.getObjectName().equals(\"masking_test_druid\") || privObj.getObjectName().startsWith(\"masking_test_druid_n\")"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "eb91fa49db99396c3cf0e79d02962dbbaef55e06", "fixCommitParentSHA1": "d3bba71666c41f13e8efb007d7cd20accde0b296", "bugFilePath": "service/src/java/org/apache/hive/http/LlapServlet.java", "fixPatch": "diff --git a/service/src/java/org/apache/hive/http/LlapServlet.java b/service/src/java/org/apache/hive/http/LlapServlet.java\nindex a0eb409..2bc59b5 100644\n--- a/service/src/java/org/apache/hive/http/LlapServlet.java\n+++ b/service/src/java/org/apache/hive/http/LlapServlet.java\n@@ -34,7 +34,7 @@\n @SuppressWarnings(\"serial\")\n public class LlapServlet extends HttpServlet {\n \n-  private static final Log LOG = LogFactory.getLog(JMXJsonServlet.class);\n+  private static final Log LOG = LogFactory.getLog(LlapServlet.class);\n \n   /**\n    * Initialize this servlet.\n", "projectName": "apache.hive", "bugLineNum": 37, "bugNodeStartChar": 1470, "bugNodeLength": 20, "fixLineNum": 37, "fixNodeStartChar": 1470, "fixNodeLength": 17, "sourceBeforeFix": "JMXJsonServlet.class", "sourceAfterFix": "LlapServlet.class"}, {"bugType": "OVERLOAD_METHOD_DELETED_ARGS", "fixCommitSHA1": "59ada6a131a7c551ba6288b211e108acb5806dc8", "fixCommitParentSHA1": "798ff7d2443f4477c9fb02ad871511b152217829", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java\nindex 31041af..18a27c4 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/StatsOptimizer.java\n@@ -919,7 +919,7 @@\n         partNames.add(part.getName());\n       }\n       AcidUtils.TableSnapshot tableSnapshot =\n-          AcidUtils.getTableSnapshot(hive.getConf(), tbl, true);\n+          AcidUtils.getTableSnapshot(hive.getConf(), tbl);\n \n       Map<String, List<ColumnStatisticsObj>> result = hive.getMSC().getPartitionColumnStatistics(\n           tbl.getDbName(), tbl.getTableName(), partNames, Lists.newArrayList(colName),\n", "projectName": "apache.hive", "bugLineNum": 922, "bugNodeStartChar": 41977, "bugNodeLength": 53, "fixLineNum": 922, "fixNodeStartChar": 41977, "fixNodeLength": 47, "sourceBeforeFix": "AcidUtils.getTableSnapshot(hive.getConf(),tbl,true)", "sourceAfterFix": "AcidUtils.getTableSnapshot(hive.getConf(),tbl)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "45faab2f4c6252971d7145e46aa336784e28d602", "fixCommitParentSHA1": "2d8e48c75414cb03d36290f97413124dea8337bb", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java\nindex 4611ce9..a1f549a 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java\n@@ -214,7 +214,7 @@\n     system.registerUDF(\"rand\", UDFRand.class, false);\n     system.registerGenericUDF(\"abs\", GenericUDFAbs.class);\n     system.registerGenericUDF(\"sq_count_check\", GenericUDFSQCountCheck.class);\n-    system.registerGenericUDF(\"enforce_constraint\", GenericUDFEnforceNotNullConstraint.class);\n+    system.registerGenericUDF(\"enforce_constraint\", GenericUDFEnforceConstraint.class);\n     system.registerGenericUDF(\"pmod\", GenericUDFPosMod.class);\n \n     system.registerUDF(\"ln\", UDFLn.class, false);\n", "projectName": "apache.hive", "bugLineNum": 217, "bugNodeStartChar": 10439, "bugNodeLength": 40, "fixLineNum": 217, "fixNodeStartChar": 10439, "fixNodeLength": 33, "sourceBeforeFix": "GenericUDFEnforceNotNullConstraint.class", "sourceAfterFix": "GenericUDFEnforceConstraint.class"}, {"bugType": "SWAP_BOOLEAN_LITERAL", "fixCommitSHA1": "9559306c3698a453609fe1ea47fddf219ca397b3", "fixCommitParentSHA1": "b98fb1f1aa47210aa240b1c22b32525828b47df1", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinTypeCheckCtx.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinTypeCheckCtx.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinTypeCheckCtx.java\nindex 4e42197..871518c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinTypeCheckCtx.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/JoinTypeCheckCtx.java\n@@ -54,7 +54,7 @@\n   public JoinTypeCheckCtx(RowResolver leftRR, RowResolver rightRR, JoinType hiveJoinType)\n       throws SemanticException {\n     super(RowResolver.getCombinedRR(leftRR, rightRR), true, false, false, false, false, false, false, false,\n-        false, false);\n+        true, false);\n     this.inputRRLst = ImmutableList.of(leftRR, rightRR);\n     this.outerJoin = (hiveJoinType == JoinType.LEFTOUTER) || (hiveJoinType == JoinType.RIGHTOUTER)\n         || (hiveJoinType == JoinType.FULLOUTER);\n", "projectName": "apache.hive", "bugLineNum": 56, "bugNodeStartChar": 1991, "bugNodeLength": 127, "fixLineNum": 56, "fixNodeStartChar": 1991, "fixNodeLength": 126, "sourceBeforeFix": "super(RowResolver.getCombinedRR(leftRR,rightRR),true,false,false,false,false,false,false,false,false,false); ", "sourceAfterFix": "super(RowResolver.getCombinedRR(leftRR,rightRR),true,false,false,false,false,false,false,false,true,false); "}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "32b8994480ec94cb1f28ba9cd295cd85cc7fe064", "fixCommitParentSHA1": "419593e70565923eb52c704b2bb58007476aa404", "bugFilePath": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java", "fixPatch": "diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java\nindex 2b8c493..9537647 100644\n--- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java\n+++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java\n@@ -178,7 +178,7 @@\n           if (aggregateData == null) {\n             aggregateData = newData.deepCopy();\n           } else {\n-            aggregateData.setAvgColLen(Math.min(aggregateData.getAvgColLen(),\n+            aggregateData.setAvgColLen(Math.max(aggregateData.getAvgColLen(),\n                 newData.getAvgColLen()));\n             aggregateData.setMaxColLen(Math.max(aggregateData.getMaxColLen(),\n                 newData.getMaxColLen()));\n", "projectName": "apache.hive", "bugLineNum": 181, "bugNodeStartChar": 8303, "bugNodeLength": 78, "fixLineNum": 181, "fixNodeStartChar": 8303, "fixNodeLength": 78, "sourceBeforeFix": "Math.min(aggregateData.getAvgColLen(),newData.getAvgColLen())", "sourceAfterFix": "Math.max(aggregateData.getAvgColLen(),newData.getAvgColLen())"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "32b8994480ec94cb1f28ba9cd295cd85cc7fe064", "fixCommitParentSHA1": "419593e70565923eb52c704b2bb58007476aa404", "bugFilePath": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java", "fixPatch": "diff --git a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java\nindex 2b8c493..9537647 100644\n--- a/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java\n+++ b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/aggr/StringColumnStatsAggregator.java\n@@ -178,7 +178,7 @@\n           if (aggregateData == null) {\n             aggregateData = newData.deepCopy();\n           } else {\n-            aggregateData.setAvgColLen(Math.min(aggregateData.getAvgColLen(),\n+            aggregateData.setAvgColLen(Math.max(aggregateData.getAvgColLen(),\n                 newData.getAvgColLen()));\n             aggregateData.setMaxColLen(Math.max(aggregateData.getMaxColLen(),\n                 newData.getMaxColLen()));\n", "projectName": "apache.hive", "bugLineNum": 181, "bugNodeStartChar": 8303, "bugNodeLength": 78, "fixLineNum": 181, "fixNodeStartChar": 8303, "fixNodeLength": 78, "sourceBeforeFix": "Math.min(aggregateData.getAvgColLen(),newData.getAvgColLen())", "sourceAfterFix": "Math.max(aggregateData.getAvgColLen(),newData.getAvgColLen())"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "123e2eb64c4d5a57a8841576ffae66c0dcd2afa1", "fixCommitParentSHA1": "6bde1ed74a68d1a1ffccf127b5f06049da7062f2", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java\nindex f53739d..39ff591 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java\n@@ -337,7 +337,7 @@\n         // If it is a floor <date> operator, we need to rewrite it\n         childRexNodeLst = rewriteFloorDateChildren(calciteOp, childRexNodeLst);\n       }\n-      expr = cluster.getRexBuilder().makeCall(calciteOp, childRexNodeLst);\n+      expr = cluster.getRexBuilder().makeCall(retType, calciteOp, childRexNodeLst);\n     } else {\n       retType = expr.getType();\n     }\n", "projectName": "apache.hive", "bugLineNum": 340, "bugNodeStartChar": 16595, "bugNodeLength": 60, "fixLineNum": 340, "fixNodeStartChar": 16595, "fixNodeLength": 69, "sourceBeforeFix": "cluster.getRexBuilder().makeCall(calciteOp,childRexNodeLst)", "sourceAfterFix": "cluster.getRexBuilder().makeCall(retType,calciteOp,childRexNodeLst)"}, {"bugType": "CHANGE_CALLER_IN_FUNCTION_CALL", "fixCommitSHA1": "c9a67786483edaf7480cc73cdec2e492a7f16f46", "fixCommitParentSHA1": "68d35fd60aa8ccde66d759e296ad73fa44c4b353", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java\nindex 1103d35..d80c8f5 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java\n@@ -61,7 +61,7 @@\n     String[] partKeyTypes = pcolTypes.trim().split(\":\");\n \n     if (partSpec.size() != partKeyTypes.length) {\n-        throw new HiveException(\"Internal error : Partition Spec size, \" + partProps.size() +\n+        throw new HiveException(\"Internal error : Partition Spec size, \" + partSpec.size() +\n                 \" doesn't match partition key definition size, \" + partKeyTypes.length);\n     }\n     boolean hasVC = vcs != null && !vcs.isEmpty();\n", "projectName": "apache.hive", "bugLineNum": 64, "bugNodeStartChar": 2936, "bugNodeLength": 16, "fixLineNum": 64, "fixNodeStartChar": 2936, "fixNodeLength": 15, "sourceBeforeFix": "partProps.size()", "sourceAfterFix": "partSpec.size()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "c9a67786483edaf7480cc73cdec2e492a7f16f46", "fixCommitParentSHA1": "68d35fd60aa8ccde66d759e296ad73fa44c4b353", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java\nindex 1103d35..d80c8f5 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartExprEvalUtils.java\n@@ -61,7 +61,7 @@\n     String[] partKeyTypes = pcolTypes.trim().split(\":\");\n \n     if (partSpec.size() != partKeyTypes.length) {\n-        throw new HiveException(\"Internal error : Partition Spec size, \" + partProps.size() +\n+        throw new HiveException(\"Internal error : Partition Spec size, \" + partSpec.size() +\n                 \" doesn't match partition key definition size, \" + partKeyTypes.length);\n     }\n     boolean hasVC = vcs != null && !vcs.isEmpty();\n", "projectName": "apache.hive", "bugLineNum": 64, "bugNodeStartChar": 2936, "bugNodeLength": 16, "fixLineNum": 64, "fixNodeStartChar": 2936, "fixNodeLength": 15, "sourceBeforeFix": "partProps.size()", "sourceAfterFix": "partSpec.size()"}, {"bugType": "SWAP_BOOLEAN_LITERAL", "fixCommitSHA1": "63bda8c7049725b63ee7e81c684abf8f996f69d4", "fixCommitParentSHA1": "d9717f5824cc382b819eabf14c03040ae4b14fac", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java\nindex 9412e5f..e24760b 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java\n@@ -1112,8 +1112,8 @@\n     long max = HiveConf.getLongVar(context.parseContext.getConf(),\n             HiveConf.ConfVars.HIVECONVERTJOINMAXSHUFFLESIZE);\n     if (max < 1) {\n-      // Max is disabled, we can safely return true\n-      return true;\n+      // Max is disabled, we can safely return false\n+      return false;\n     }\n     // Evaluate\n     ReduceSinkOperator rsOp = (ReduceSinkOperator) joinOp.getParentOperators().get(position);\n", "projectName": "apache.hive", "bugLineNum": 1116, "bugNodeStartChar": 48539, "bugNodeLength": 12, "fixLineNum": 1116, "fixNodeStartChar": 48540, "fixNodeLength": 13, "sourceBeforeFix": "return true; ", "sourceAfterFix": "return false; "}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "f0b0cc268e5fcb59391bbf4f4773f6f92aed6dab", "fixCommitParentSHA1": "ff30a1ebf6a470ce66d7bc7cb8ed452ac956b347", "bugFilePath": "service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java", "fixPatch": "diff --git a/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java b/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java\nindex 1a2be8b..773dd51 100644\n--- a/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java\n+++ b/service/src/java/org/apache/hive/service/cli/operation/SQLOperation.java\n@@ -257,11 +257,12 @@\n       /**\n        * If the operation was cancelled by another thread, or the execution timed out, Driver#run\n        * may return a non-zero response code. We will simply return if the operation state is\n-       * CANCELED, TIMEDOUT or CLOSED, otherwise throw an exception\n+       * CANCELED, TIMEDOUT, CLOSED or FINISHED, otherwise throw an exception\n        */\n       if ((getStatus().getState() == OperationState.CANCELED)\n           || (getStatus().getState() == OperationState.TIMEDOUT)\n-          || (getStatus().getState() == OperationState.CLOSED)) {\n+          || (getStatus().getState() == OperationState.CLOSED)\n+          || (getStatus().getState() == OperationState.FINISHED)) {\n         LOG.warn(\"Ignore exception in terminal state\", e);\n         return;\n       }\n", "projectName": "apache.hive", "bugLineNum": 262, "bugNodeStartChar": 10935, "bugNodeLength": 179, "fixLineNum": 262, "fixNodeStartChar": 10935, "fixNodeLength": 244, "sourceBeforeFix": "(getStatus().getState() == OperationState.CANCELED) || (getStatus().getState() == OperationState.TIMEDOUT) || (getStatus().getState() == OperationState.CLOSED)", "sourceAfterFix": "(getStatus().getState() == OperationState.CANCELED) || (getStatus().getState() == OperationState.TIMEDOUT) || (getStatus().getState() == OperationState.CLOSED)|| (getStatus().getState() == OperationState.FINISHED)"}, {"bugType": "SWAP_ARGUMENTS", "fixCommitSHA1": "efa5b54542e6ccd1357433f3ae340e94efdd9f6e", "fixCommitParentSHA1": "4a14cfc01b6c06817899290b6d2c5f0849cb44dd", "bugFilePath": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "fixPatch": "diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\nindex b43b4df..5b353c4 100644\n--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n@@ -2031,7 +2031,7 @@\n             transactionalListenerResponses =\n                 MetaStoreListenerNotifier.notifyEvent(transactionalListeners,\n                                                       EventType.DROP_TABLE,\n-                                                      new DropTableEvent(tbl, deleteData, true, this),\n+                                                      new DropTableEvent(tbl, true, deleteData, this),\n                                                       envContext);\n           }\n           success = ms.commitTransaction();\n@@ -2051,7 +2051,7 @@\n         if (!listeners.isEmpty()) {\n           MetaStoreListenerNotifier.notifyEvent(listeners,\n                                                 EventType.DROP_TABLE,\n-                                                new DropTableEvent(tbl, deleteData, success, this),\n+                                                new DropTableEvent(tbl, success, deleteData, this),\n                                                 envContext,\n                                                 transactionalListenerResponses, ms);\n         }\n", "projectName": "apache.hive", "bugLineNum": 2034, "bugNodeStartChar": 79703, "bugNodeLength": 47, "fixLineNum": 2034, "fixNodeStartChar": 79703, "fixNodeLength": 47, "sourceBeforeFix": "new DropTableEvent(tbl,deleteData,true,this)", "sourceAfterFix": "new DropTableEvent(tbl,true,deleteData,this)"}, {"bugType": "SWAP_ARGUMENTS", "fixCommitSHA1": "efa5b54542e6ccd1357433f3ae340e94efdd9f6e", "fixCommitParentSHA1": "4a14cfc01b6c06817899290b6d2c5f0849cb44dd", "bugFilePath": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "fixPatch": "diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\nindex b43b4df..5b353c4 100644\n--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n@@ -2031,7 +2031,7 @@\n             transactionalListenerResponses =\n                 MetaStoreListenerNotifier.notifyEvent(transactionalListeners,\n                                                       EventType.DROP_TABLE,\n-                                                      new DropTableEvent(tbl, deleteData, true, this),\n+                                                      new DropTableEvent(tbl, true, deleteData, this),\n                                                       envContext);\n           }\n           success = ms.commitTransaction();\n@@ -2051,7 +2051,7 @@\n         if (!listeners.isEmpty()) {\n           MetaStoreListenerNotifier.notifyEvent(listeners,\n                                                 EventType.DROP_TABLE,\n-                                                new DropTableEvent(tbl, deleteData, success, this),\n+                                                new DropTableEvent(tbl, success, deleteData, this),\n                                                 envContext,\n                                                 transactionalListenerResponses, ms);\n         }\n", "projectName": "apache.hive", "bugLineNum": 2054, "bugNodeStartChar": 80560, "bugNodeLength": 50, "fixLineNum": 2054, "fixNodeStartChar": 80560, "fixNodeLength": 50, "sourceBeforeFix": "new DropTableEvent(tbl,deleteData,success,this)", "sourceAfterFix": "new DropTableEvent(tbl,success,deleteData,this)"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "c3aba153cb92d71d41554a0714a6694980f9b363", "fixCommitParentSHA1": "7629dbd5b722bc190d741f73c70bba2bd7daa87d", "bugFilePath": "common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/CodahaleReporter.java", "fixPatch": "diff --git a/common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/CodahaleReporter.java b/common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/CodahaleReporter.java\nindex ba4d8e4..9424f28 100644\n--- a/common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/CodahaleReporter.java\n+++ b/common/src/java/org/apache/hadoop/hive/common/metrics/metrics2/CodahaleReporter.java\n@@ -20,7 +20,7 @@\n import com.codahale.metrics.Reporter;\n import java.io.Closeable;\n \n-interface CodahaleReporter extends Closeable, Reporter {\n+public interface CodahaleReporter extends Closeable, Reporter {\n \n   /**\n    * Start the reporter.\n", "projectName": "apache.hive", "bugLineNum": 23, "bugNodeStartChar": 929, "bugNodeLength": 119, "fixLineNum": 23, "fixNodeStartChar": 929, "fixNodeLength": 126, "sourceBeforeFix": "0", "sourceAfterFix": "1"}, {"bugType": "OVERLOAD_METHOD_DELETED_ARGS", "fixCommitSHA1": "7cc741c6146774d47d77e6c2ab213f4d2cd6b049", "fixCommitParentSHA1": "693b8565950bd5a49b9145c59d05ca6fd3300af6", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java\nindex 530b364..37c3a96 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java\n@@ -960,7 +960,7 @@\n     fsp2.configureDynPartPath(dirName, !conf.isMmTable() && isUnionDp ? unionPath : null);\n     Utilities.LOG14535.info(\"creating new paths \" + System.identityHashCode(fsp2) + \" for \"\n         + dirName + \", childSpec \" + unionPath + \": tmpPath \" + fsp2.getTmpPath()\n-        + \", task path \" + fsp2.getTaskOutputTempPath(), new Exception());\n+        + \", task path \" + fsp2.getTaskOutputTempPath());\n     if(!conf.getDpSortState().equals(DPSortState.PARTITION_BUCKET_SORTED)) {\n       createBucketFiles(fsp2);\n       valToPaths.put(dirName, fsp2);\n", "projectName": "apache.hive", "bugLineNum": 961, "bugNodeStartChar": 38437, "bugNodeLength": 243, "fixLineNum": 961, "fixNodeStartChar": 38437, "fixNodeLength": 226, "sourceBeforeFix": "Utilities.LOG14535.info(\"creating new paths \" + System.identityHashCode(fsp2) + \" for \"+ dirName+ \", childSpec \"+ unionPath+ \": tmpPath \"+ fsp2.getTmpPath()+ \", task path \"+ fsp2.getTaskOutputTempPath(),new Exception())", "sourceAfterFix": "Utilities.LOG14535.info(\"creating new paths \" + System.identityHashCode(fsp2) + \" for \"+ dirName+ \", childSpec \"+ unionPath+ \": tmpPath \"+ fsp2.getTmpPath()+ \", task path \"+ fsp2.getTaskOutputTempPath())"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "53f03358377f3dde21f58e6c841142c6db8a9c32", "fixCommitParentSHA1": "6ca79e3aa2b5a8812e5c4aaee80c4115e2b9def8", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java\nindex ecac85c..b70dbd8 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java\n@@ -314,7 +314,7 @@\n         LOG.warn(\"User has specified \" + queueName + \" queue; ignoring the setting\");\n         queueName = null;\n         hasQueue = false;\n-        conf.set(\"tez.queue.name\", null);\n+        conf.unset(\"tez.queue.name\");\n       }\n       default: // All good.\n       }\n", "projectName": "apache.hive", "bugLineNum": 317, "bugNodeStartChar": 12518, "bugNodeLength": 32, "fixLineNum": 317, "fixNodeStartChar": 12518, "fixNodeLength": 28, "sourceBeforeFix": "conf.set(\"tez.queue.name\",null)", "sourceAfterFix": "conf.unset(\"tez.queue.name\")"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "1aad3be28ffd06bf56dc743d4738fe850723cca4", "fixCommitParentSHA1": "bddf5a7a974fcfc4f350123f561da5f4ddcf43e0", "bugFilePath": "druid-handler/src/test/org/apache/hadoop/hive/ql/io/DruidRecordWriterTest.java", "fixPatch": "diff --git a/druid-handler/src/test/org/apache/hadoop/hive/ql/io/DruidRecordWriterTest.java b/druid-handler/src/test/org/apache/hadoop/hive/ql/io/DruidRecordWriterTest.java\nindex b1310b6..f72a735 100644\n--- a/druid-handler/src/test/org/apache/hadoop/hive/ql/io/DruidRecordWriterTest.java\n+++ b/druid-handler/src/test/org/apache/hadoop/hive/ql/io/DruidRecordWriterTest.java\n@@ -231,7 +231,7 @@\n   @Test\n   public void testSerDesr() throws IOException {\n     String segment = \"{\\\"dataSource\\\":\\\"datasource2015\\\",\\\"interval\\\":\\\"2015-06-01T00:00:00.000-04:00/2015-06-02T00:00:00.000-04:00\\\",\\\"version\\\":\\\"2016-11-04T19:24:01.732-04:00\\\",\\\"loadSpec\\\":{\\\"type\\\":\\\"hdfs\\\",\\\"path\\\":\\\"hdfs://cn105-10.l42scl.hortonworks.com:8020/apps/hive/warehouse/druid.db/.hive-staging_hive_2016-11-04_19-23-50_168_1550339856804207572-1/_task_tmp.-ext-10002/_tmp.000000_0/datasource2015/20150601T000000.000-0400_20150602T000000.000-0400/2016-11-04T19_24_01.732-04_00/0/index.zip\\\"},\\\"dimensions\\\":\\\"dimension1\\\",\\\"metrics\\\":\\\"bigint\\\",\\\"shardSpec\\\":{\\\"type\\\":\\\"linear\\\",\\\"partitionNum\\\":0},\\\"binaryVersion\\\":9,\\\"size\\\":1765,\\\"identifier\\\":\\\"datasource2015_2015-06-01T00:00:00.000-04:00_2015-06-02T00:00:00.000-04:00_2016-11-04T19:24:01.732-04:00\\\"}\";\n-    DataSegment dataSegment = objectMapper.readerFor(DataSegment.class)\n+    DataSegment dataSegment = objectMapper.reader(DataSegment.class)\n             .readValue(segment);\n     Assert.assertTrue(dataSegment.getDataSource().equals(\"datasource2015\"));\n   }\n", "projectName": "apache.hive", "bugLineNum": 234, "bugNodeStartChar": 10606, "bugNodeLength": 41, "fixLineNum": 234, "fixNodeStartChar": 10606, "fixNodeLength": 38, "sourceBeforeFix": "objectMapper.readerFor(DataSegment.class)", "sourceAfterFix": "objectMapper.reader(DataSegment.class)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "1aad3be28ffd06bf56dc743d4738fe850723cca4", "fixCommitParentSHA1": "bddf5a7a974fcfc4f350123f561da5f4ddcf43e0", "bugFilePath": "druid-handler/src/test/org/apache/hadoop/hive/ql/io/DruidRecordWriterTest.java", "fixPatch": "diff --git a/druid-handler/src/test/org/apache/hadoop/hive/ql/io/DruidRecordWriterTest.java b/druid-handler/src/test/org/apache/hadoop/hive/ql/io/DruidRecordWriterTest.java\nindex b1310b6..f72a735 100644\n--- a/druid-handler/src/test/org/apache/hadoop/hive/ql/io/DruidRecordWriterTest.java\n+++ b/druid-handler/src/test/org/apache/hadoop/hive/ql/io/DruidRecordWriterTest.java\n@@ -231,7 +231,7 @@\n   @Test\n   public void testSerDesr() throws IOException {\n     String segment = \"{\\\"dataSource\\\":\\\"datasource2015\\\",\\\"interval\\\":\\\"2015-06-01T00:00:00.000-04:00/2015-06-02T00:00:00.000-04:00\\\",\\\"version\\\":\\\"2016-11-04T19:24:01.732-04:00\\\",\\\"loadSpec\\\":{\\\"type\\\":\\\"hdfs\\\",\\\"path\\\":\\\"hdfs://cn105-10.l42scl.hortonworks.com:8020/apps/hive/warehouse/druid.db/.hive-staging_hive_2016-11-04_19-23-50_168_1550339856804207572-1/_task_tmp.-ext-10002/_tmp.000000_0/datasource2015/20150601T000000.000-0400_20150602T000000.000-0400/2016-11-04T19_24_01.732-04_00/0/index.zip\\\"},\\\"dimensions\\\":\\\"dimension1\\\",\\\"metrics\\\":\\\"bigint\\\",\\\"shardSpec\\\":{\\\"type\\\":\\\"linear\\\",\\\"partitionNum\\\":0},\\\"binaryVersion\\\":9,\\\"size\\\":1765,\\\"identifier\\\":\\\"datasource2015_2015-06-01T00:00:00.000-04:00_2015-06-02T00:00:00.000-04:00_2016-11-04T19:24:01.732-04:00\\\"}\";\n-    DataSegment dataSegment = objectMapper.readerFor(DataSegment.class)\n+    DataSegment dataSegment = objectMapper.reader(DataSegment.class)\n             .readValue(segment);\n     Assert.assertTrue(dataSegment.getDataSource().equals(\"datasource2015\"));\n   }\n", "projectName": "apache.hive", "bugLineNum": 234, "bugNodeStartChar": 10606, "bugNodeLength": 41, "fixLineNum": 234, "fixNodeStartChar": 10606, "fixNodeLength": 38, "sourceBeforeFix": "objectMapper.readerFor(DataSegment.class)", "sourceAfterFix": "objectMapper.reader(DataSegment.class)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "9589126100375c54a8a6e41e271438bce6e07b0f", "fixCommitParentSHA1": "84faae00731313964c06d223afe27dc6a91847b6", "bugFilePath": "llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java", "fixPatch": "diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java\nindex 419043a..f6531e8 100644\n--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java\n+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/SerDeEncodedDataReader.java\n@@ -1475,8 +1475,7 @@\n         Path path, StructObjectInspector oi) throws IOException {\n       // TODO: this is currently broken. We need to set memory manager to a bogus implementation\n       //       to avoid problems with memory manager actually tracking the usage.\n-      return OrcFile.createWriter(path, createOrcWriterOptions(\n-          sourceOi, conf, cacheWriter, allocSize));\n+      return OrcFile.createWriter(path, createOrcWriterOptions(oi, conf, cacheWriter, allocSize));\n     }\n   }\n \n", "projectName": "apache.hive", "bugLineNum": 1478, "bugNodeStartChar": 60757, "bugNodeLength": 73, "fixLineNum": 1478, "fixNodeStartChar": 60757, "fixNodeLength": 56, "sourceBeforeFix": "createOrcWriterOptions(sourceOi,conf,cacheWriter,allocSize)", "sourceAfterFix": "createOrcWriterOptions(oi,conf,cacheWriter,allocSize)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "cea954f5c93bc8830c9358ad16d3c39aaf9c9de8", "fixCommitParentSHA1": "936df7a15a3ce323300cabe7b2ebb90e22f2069d", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\nindex 8f5542b..47feeaf 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n@@ -825,9 +825,9 @@\n \n     qb.getParseInfo().setSrcForAlias(alias, tableTree);\n \n-    // if alias to CTE contains the alias, we do not do the translation because\n+    // if alias to CTE contains the table name, we do not do the translation because\n     // cte is actually a subquery.\n-    if (!this.aliasToCTEs.containsKey(alias)) {\n+    if (!this.aliasToCTEs.containsKey(tabIdName)) {\n       unparseTranslator.addTableNameTranslation(tableTree, SessionState.get().getCurrentDatabase());\n       if (aliasIndex != 0) {\n         unparseTranslator.addIdentifierTranslation((ASTNode) tabref.getChild(aliasIndex));\n", "projectName": "apache.hive", "bugLineNum": 830, "bugNodeStartChar": 36326, "bugNodeLength": 35, "fixLineNum": 830, "fixNodeStartChar": 36326, "fixNodeLength": 39, "sourceBeforeFix": "this.aliasToCTEs.containsKey(alias)", "sourceAfterFix": "this.aliasToCTEs.containsKey(tabIdName)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "b87311df5630fab6a5a5f57a056511f4c09eaab1", "fixCommitParentSHA1": "0a26c4885046891ba4339e8dae830a74a7e226d5", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/PartitionColumnsSeparator.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/PartitionColumnsSeparator.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/PartitionColumnsSeparator.java\nindex aea28e2..beb1884 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/PartitionColumnsSeparator.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/PartitionColumnsSeparator.java\n@@ -64,7 +64,7 @@\n  */\n public class PartitionColumnsSeparator extends Transform {\n \n-  private static final Log LOG = LogFactory.getLog(PointLookupOptimizer.class);\n+  private static final Log LOG = LogFactory.getLog(PartitionColumnsSeparator.class);\n   private static final String IN_UDF =\n     GenericUDFIn.class.getAnnotation(Description.class).name();\n   private static final String STRUCT_UDF =\n", "projectName": "apache.hive", "bugLineNum": 67, "bugNodeStartChar": 3040, "bugNodeLength": 26, "fixLineNum": 67, "fixNodeStartChar": 3040, "fixNodeLength": 31, "sourceBeforeFix": "PointLookupOptimizer.class", "sourceAfterFix": "PartitionColumnsSeparator.class"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "27fb87cfcea241c2d7961baf68e84ce97f2dee7a", "fixCommitParentSHA1": "345353c0ea5d3ddda9f6d89cbf8cd0e92726fcb6", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java\nindex d6852dc..eb4488a 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java\n@@ -98,7 +98,9 @@\n     \n     String columnType = colType.get(0);\n \n-    if (columnType.equalsIgnoreCase(\"long\")) {\n+    if (columnType.equalsIgnoreCase(\"long\") || columnType.equalsIgnoreCase(\"tinyint\")\n+            || columnType.equalsIgnoreCase(\"smallint\") || columnType.equalsIgnoreCase(\"int\")\n+            || columnType.equalsIgnoreCase(\"bigint\")) {\n       LongColumnStatsData longStats = new LongColumnStatsData();\n       longStats.setNumNullsIsSet(false);\n       longStats.setNumDVsIsSet(false);\n@@ -123,7 +125,7 @@\n       }\n       statsData.setLongStats(longStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"double\")) {\n+    } else if (columnType.equalsIgnoreCase(\"double\") || columnType.equalsIgnoreCase(\"float\")) {\n       DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n       doubleStats.setNumNullsIsSet(false);\n       doubleStats.setNumDVsIsSet(false);\n@@ -147,7 +149,8 @@\n       }\n       statsData.setDoubleStats(doubleStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"string\")) {\n+    } else if (columnType.equalsIgnoreCase(\"string\") || columnType.toLowerCase().startsWith(\"char\")\n+              || columnType.toLowerCase().startsWith(\"varchar\")) { //char(x),varchar(x) types\n       StringColumnStatsData stringStats = new StringColumnStatsData();\n       stringStats.setMaxColLenIsSet(false);\n       stringStats.setAvgColLenIsSet(false);\n@@ -213,7 +216,7 @@\n       }\n       statsData.setBinaryStats(binaryStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"decimal\")) {\n+    } else if (columnType.toLowerCase().startsWith(\"decimal\")) { //decimal(a,b) type\n       DecimalColumnStatsData decimalStats = new DecimalColumnStatsData();\n       decimalStats.setNumNullsIsSet(false);\n       decimalStats.setNumDVsIsSet(false);\n@@ -241,7 +244,8 @@\n       }\n       statsData.setDecimalStats(decimalStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"date\")) {\n+    } else if (columnType.equalsIgnoreCase(\"date\")\n+            || columnType.equalsIgnoreCase(\"timestamp\")) {\n       DateColumnStatsData dateStats = new DateColumnStatsData();\n       Map<String, String> mapProp = work.getMapProp();\n       for (Entry<String, String> entry : mapProp.entrySet()) {\n", "projectName": "apache.hive", "bugLineNum": 101, "bugNodeStartChar": 4227, "bugNodeLength": 35, "fixLineNum": 101, "fixNodeStartChar": 4227, "fixNodeLength": 223, "sourceBeforeFix": "columnType.equalsIgnoreCase(\"long\")", "sourceAfterFix": "columnType.equalsIgnoreCase(\"long\") || columnType.equalsIgnoreCase(\"tinyint\") || columnType.equalsIgnoreCase(\"smallint\")|| columnType.equalsIgnoreCase(\"int\")|| columnType.equalsIgnoreCase(\"bigint\")"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "27fb87cfcea241c2d7961baf68e84ce97f2dee7a", "fixCommitParentSHA1": "345353c0ea5d3ddda9f6d89cbf8cd0e92726fcb6", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java\nindex d6852dc..eb4488a 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java\n@@ -98,7 +98,9 @@\n     \n     String columnType = colType.get(0);\n \n-    if (columnType.equalsIgnoreCase(\"long\")) {\n+    if (columnType.equalsIgnoreCase(\"long\") || columnType.equalsIgnoreCase(\"tinyint\")\n+            || columnType.equalsIgnoreCase(\"smallint\") || columnType.equalsIgnoreCase(\"int\")\n+            || columnType.equalsIgnoreCase(\"bigint\")) {\n       LongColumnStatsData longStats = new LongColumnStatsData();\n       longStats.setNumNullsIsSet(false);\n       longStats.setNumDVsIsSet(false);\n@@ -123,7 +125,7 @@\n       }\n       statsData.setLongStats(longStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"double\")) {\n+    } else if (columnType.equalsIgnoreCase(\"double\") || columnType.equalsIgnoreCase(\"float\")) {\n       DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n       doubleStats.setNumNullsIsSet(false);\n       doubleStats.setNumDVsIsSet(false);\n@@ -147,7 +149,8 @@\n       }\n       statsData.setDoubleStats(doubleStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"string\")) {\n+    } else if (columnType.equalsIgnoreCase(\"string\") || columnType.toLowerCase().startsWith(\"char\")\n+              || columnType.toLowerCase().startsWith(\"varchar\")) { //char(x),varchar(x) types\n       StringColumnStatsData stringStats = new StringColumnStatsData();\n       stringStats.setMaxColLenIsSet(false);\n       stringStats.setAvgColLenIsSet(false);\n@@ -213,7 +216,7 @@\n       }\n       statsData.setBinaryStats(binaryStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"decimal\")) {\n+    } else if (columnType.toLowerCase().startsWith(\"decimal\")) { //decimal(a,b) type\n       DecimalColumnStatsData decimalStats = new DecimalColumnStatsData();\n       decimalStats.setNumNullsIsSet(false);\n       decimalStats.setNumDVsIsSet(false);\n@@ -241,7 +244,8 @@\n       }\n       statsData.setDecimalStats(decimalStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"date\")) {\n+    } else if (columnType.equalsIgnoreCase(\"date\")\n+            || columnType.equalsIgnoreCase(\"timestamp\")) {\n       DateColumnStatsData dateStats = new DateColumnStatsData();\n       Map<String, String> mapProp = work.getMapProp();\n       for (Entry<String, String> entry : mapProp.entrySet()) {\n", "projectName": "apache.hive", "bugLineNum": 126, "bugNodeStartChar": 5282, "bugNodeLength": 37, "fixLineNum": 126, "fixNodeStartChar": 5282, "fixNodeLength": 77, "sourceBeforeFix": "columnType.equalsIgnoreCase(\"double\")", "sourceAfterFix": "columnType.equalsIgnoreCase(\"double\") || columnType.equalsIgnoreCase(\"float\")"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "27fb87cfcea241c2d7961baf68e84ce97f2dee7a", "fixCommitParentSHA1": "345353c0ea5d3ddda9f6d89cbf8cd0e92726fcb6", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java\nindex d6852dc..eb4488a 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java\n@@ -98,7 +98,9 @@\n     \n     String columnType = colType.get(0);\n \n-    if (columnType.equalsIgnoreCase(\"long\")) {\n+    if (columnType.equalsIgnoreCase(\"long\") || columnType.equalsIgnoreCase(\"tinyint\")\n+            || columnType.equalsIgnoreCase(\"smallint\") || columnType.equalsIgnoreCase(\"int\")\n+            || columnType.equalsIgnoreCase(\"bigint\")) {\n       LongColumnStatsData longStats = new LongColumnStatsData();\n       longStats.setNumNullsIsSet(false);\n       longStats.setNumDVsIsSet(false);\n@@ -123,7 +125,7 @@\n       }\n       statsData.setLongStats(longStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"double\")) {\n+    } else if (columnType.equalsIgnoreCase(\"double\") || columnType.equalsIgnoreCase(\"float\")) {\n       DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n       doubleStats.setNumNullsIsSet(false);\n       doubleStats.setNumDVsIsSet(false);\n@@ -147,7 +149,8 @@\n       }\n       statsData.setDoubleStats(doubleStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"string\")) {\n+    } else if (columnType.equalsIgnoreCase(\"string\") || columnType.toLowerCase().startsWith(\"char\")\n+              || columnType.toLowerCase().startsWith(\"varchar\")) { //char(x),varchar(x) types\n       StringColumnStatsData stringStats = new StringColumnStatsData();\n       stringStats.setMaxColLenIsSet(false);\n       stringStats.setAvgColLenIsSet(false);\n@@ -213,7 +216,7 @@\n       }\n       statsData.setBinaryStats(binaryStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"decimal\")) {\n+    } else if (columnType.toLowerCase().startsWith(\"decimal\")) { //decimal(a,b) type\n       DecimalColumnStatsData decimalStats = new DecimalColumnStatsData();\n       decimalStats.setNumNullsIsSet(false);\n       decimalStats.setNumDVsIsSet(false);\n@@ -241,7 +244,8 @@\n       }\n       statsData.setDecimalStats(decimalStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"date\")) {\n+    } else if (columnType.equalsIgnoreCase(\"date\")\n+            || columnType.equalsIgnoreCase(\"timestamp\")) {\n       DateColumnStatsData dateStats = new DateColumnStatsData();\n       Map<String, String> mapProp = work.getMapProp();\n       for (Entry<String, String> entry : mapProp.entrySet()) {\n", "projectName": "apache.hive", "bugLineNum": 150, "bugNodeStartChar": 6372, "bugNodeLength": 37, "fixLineNum": 150, "fixNodeStartChar": 6372, "fixNodeLength": 148, "sourceBeforeFix": "columnType.equalsIgnoreCase(\"string\")", "sourceAfterFix": "columnType.equalsIgnoreCase(\"string\") || columnType.toLowerCase().startsWith(\"char\") || columnType.toLowerCase().startsWith(\"varchar\")"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "27fb87cfcea241c2d7961baf68e84ce97f2dee7a", "fixCommitParentSHA1": "345353c0ea5d3ddda9f6d89cbf8cd0e92726fcb6", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java\nindex d6852dc..eb4488a 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ColumnStatsUpdateTask.java\n@@ -98,7 +98,9 @@\n     \n     String columnType = colType.get(0);\n \n-    if (columnType.equalsIgnoreCase(\"long\")) {\n+    if (columnType.equalsIgnoreCase(\"long\") || columnType.equalsIgnoreCase(\"tinyint\")\n+            || columnType.equalsIgnoreCase(\"smallint\") || columnType.equalsIgnoreCase(\"int\")\n+            || columnType.equalsIgnoreCase(\"bigint\")) {\n       LongColumnStatsData longStats = new LongColumnStatsData();\n       longStats.setNumNullsIsSet(false);\n       longStats.setNumDVsIsSet(false);\n@@ -123,7 +125,7 @@\n       }\n       statsData.setLongStats(longStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"double\")) {\n+    } else if (columnType.equalsIgnoreCase(\"double\") || columnType.equalsIgnoreCase(\"float\")) {\n       DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n       doubleStats.setNumNullsIsSet(false);\n       doubleStats.setNumDVsIsSet(false);\n@@ -147,7 +149,8 @@\n       }\n       statsData.setDoubleStats(doubleStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"string\")) {\n+    } else if (columnType.equalsIgnoreCase(\"string\") || columnType.toLowerCase().startsWith(\"char\")\n+              || columnType.toLowerCase().startsWith(\"varchar\")) { //char(x),varchar(x) types\n       StringColumnStatsData stringStats = new StringColumnStatsData();\n       stringStats.setMaxColLenIsSet(false);\n       stringStats.setAvgColLenIsSet(false);\n@@ -213,7 +216,7 @@\n       }\n       statsData.setBinaryStats(binaryStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"decimal\")) {\n+    } else if (columnType.toLowerCase().startsWith(\"decimal\")) { //decimal(a,b) type\n       DecimalColumnStatsData decimalStats = new DecimalColumnStatsData();\n       decimalStats.setNumNullsIsSet(false);\n       decimalStats.setNumDVsIsSet(false);\n@@ -241,7 +244,8 @@\n       }\n       statsData.setDecimalStats(decimalStats);\n       statsObj.setStatsData(statsData);\n-    } else if (columnType.equalsIgnoreCase(\"date\")) {\n+    } else if (columnType.equalsIgnoreCase(\"date\")\n+            || columnType.equalsIgnoreCase(\"timestamp\")) {\n       DateColumnStatsData dateStats = new DateColumnStatsData();\n       Map<String, String> mapProp = work.getMapProp();\n       for (Entry<String, String> entry : mapProp.entrySet()) {\n", "projectName": "apache.hive", "bugLineNum": 244, "bugNodeStartChar": 10700, "bugNodeLength": 35, "fixLineNum": 244, "fixNodeStartChar": 10700, "fixNodeLength": 91, "sourceBeforeFix": "columnType.equalsIgnoreCase(\"date\")", "sourceAfterFix": "columnType.equalsIgnoreCase(\"date\") || columnType.equalsIgnoreCase(\"timestamp\")"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "dcfb1ed2788a4c497bc251ab777c2d04652fa20c", "fixCommitParentSHA1": "2f40ac3aa423af827c1978c9be977a7b2ce5e5c8", "bugFilePath": "druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java", "fixPatch": "diff --git a/druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java b/druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java\nindex cc4a8ef..787cd52 100644\n--- a/druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java\n+++ b/druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java\n@@ -254,7 +254,7 @@\n       }\n \n       intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),\n-              timeList.get(0).getValue().getMaxTime().getMillis()));\n+              timeList.get(0).getValue().getMaxTime().getMillis(), ISOChronology.getInstanceUTC()));\n     } else {\n       intervals.addAll(query.getIntervals());\n     }\n@@ -289,13 +289,13 @@\n         final long expectedRange = rangeSize - currTime;\n         if (interval.getEndMillis() - startTime >= expectedRange) {\n           endTime = startTime + expectedRange;\n-          currentIntervals.add(new Interval(startTime, endTime));\n+          currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));\n           startTime = endTime;\n           currTime = 0;\n           break;\n         }\n         endTime = interval.getEndMillis();\n-        currentIntervals.add(new Interval(startTime, endTime));\n+        currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));\n         currTime += (endTime - startTime);\n         startTime = intervals.get(++posIntervals).getStartMillis();\n       }\n", "projectName": "apache.hive", "bugLineNum": 256, "bugNodeStartChar": 10996, "bugNodeLength": 132, "fixLineNum": 256, "fixNodeStartChar": 10996, "fixNodeLength": 164, "sourceBeforeFix": "new Interval(timeList.get(0).getValue().getMinTime().getMillis(),timeList.get(0).getValue().getMaxTime().getMillis())", "sourceAfterFix": "new Interval(timeList.get(0).getValue().getMinTime().getMillis(),timeList.get(0).getValue().getMaxTime().getMillis(),ISOChronology.getInstanceUTC())"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "dcfb1ed2788a4c497bc251ab777c2d04652fa20c", "fixCommitParentSHA1": "2f40ac3aa423af827c1978c9be977a7b2ce5e5c8", "bugFilePath": "druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java", "fixPatch": "diff --git a/druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java b/druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java\nindex cc4a8ef..787cd52 100644\n--- a/druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java\n+++ b/druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java\n@@ -254,7 +254,7 @@\n       }\n \n       intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),\n-              timeList.get(0).getValue().getMaxTime().getMillis()));\n+              timeList.get(0).getValue().getMaxTime().getMillis(), ISOChronology.getInstanceUTC()));\n     } else {\n       intervals.addAll(query.getIntervals());\n     }\n@@ -289,13 +289,13 @@\n         final long expectedRange = rangeSize - currTime;\n         if (interval.getEndMillis() - startTime >= expectedRange) {\n           endTime = startTime + expectedRange;\n-          currentIntervals.add(new Interval(startTime, endTime));\n+          currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));\n           startTime = endTime;\n           currTime = 0;\n           break;\n         }\n         endTime = interval.getEndMillis();\n-        currentIntervals.add(new Interval(startTime, endTime));\n+        currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));\n         currTime += (endTime - startTime);\n         startTime = intervals.get(++posIntervals).getStartMillis();\n       }\n", "projectName": "apache.hive", "bugLineNum": 292, "bugNodeStartChar": 12826, "bugNodeLength": 32, "fixLineNum": 292, "fixNodeStartChar": 12826, "fixNodeLength": 64, "sourceBeforeFix": "new Interval(startTime,endTime)", "sourceAfterFix": "new Interval(startTime,endTime,ISOChronology.getInstanceUTC())"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "dcfb1ed2788a4c497bc251ab777c2d04652fa20c", "fixCommitParentSHA1": "2f40ac3aa423af827c1978c9be977a7b2ce5e5c8", "bugFilePath": "druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java", "fixPatch": "diff --git a/druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java b/druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java\nindex cc4a8ef..787cd52 100644\n--- a/druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java\n+++ b/druid-handler/src/java/org/apache/hadoop/hive/druid/HiveDruidQueryBasedInputFormat.java\n@@ -254,7 +254,7 @@\n       }\n \n       intervals.add(new Interval(timeList.get(0).getValue().getMinTime().getMillis(),\n-              timeList.get(0).getValue().getMaxTime().getMillis()));\n+              timeList.get(0).getValue().getMaxTime().getMillis(), ISOChronology.getInstanceUTC()));\n     } else {\n       intervals.addAll(query.getIntervals());\n     }\n@@ -289,13 +289,13 @@\n         final long expectedRange = rangeSize - currTime;\n         if (interval.getEndMillis() - startTime >= expectedRange) {\n           endTime = startTime + expectedRange;\n-          currentIntervals.add(new Interval(startTime, endTime));\n+          currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));\n           startTime = endTime;\n           currTime = 0;\n           break;\n         }\n         endTime = interval.getEndMillis();\n-        currentIntervals.add(new Interval(startTime, endTime));\n+        currentIntervals.add(new Interval(startTime, endTime, ISOChronology.getInstanceUTC()));\n         currTime += (endTime - startTime);\n         startTime = intervals.get(++posIntervals).getStartMillis();\n       }\n", "projectName": "apache.hive", "bugLineNum": 298, "bugNodeStartChar": 13015, "bugNodeLength": 32, "fixLineNum": 298, "fixNodeStartChar": 13015, "fixNodeLength": 64, "sourceBeforeFix": "new Interval(startTime,endTime)", "sourceAfterFix": "new Interval(startTime,endTime,ISOChronology.getInstanceUTC())"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "9ada10f883aef37b9a87984fe499cdc04e4f1f06", "fixCommitParentSHA1": "fea5c84ce230c4d08f1d581952c9a4ce300e43c7", "bugFilePath": "beeline/src/java/org/apache/hive/beeline/Commands.java", "fixPatch": "diff --git a/beeline/src/java/org/apache/hive/beeline/Commands.java b/beeline/src/java/org/apache/hive/beeline/Commands.java\nindex 3a204c0..387861b 100644\n--- a/beeline/src/java/org/apache/hive/beeline/Commands.java\n+++ b/beeline/src/java/org/apache/hive/beeline/Commands.java\n@@ -1316,7 +1316,7 @@\n     Properties props = new Properties();\n     if (url != null) {\n       String saveUrl = getUrlToUse(url);\n-      props.setProperty(JdbcConnectionParams.PROPERTY_URL, url);\n+      props.setProperty(JdbcConnectionParams.PROPERTY_URL, saveUrl);\n     }\n \n     String value = null;\n", "projectName": "apache.hive", "bugLineNum": 1319, "bugNodeStartChar": 38847, "bugNodeLength": 57, "fixLineNum": 1319, "fixNodeStartChar": 38847, "fixNodeLength": 61, "sourceBeforeFix": "props.setProperty(JdbcConnectionParams.PROPERTY_URL,url)", "sourceAfterFix": "props.setProperty(JdbcConnectionParams.PROPERTY_URL,saveUrl)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "e1626ef3aeecb1ccf7acf4056fb7bcf23243be25", "fixCommitParentSHA1": "a354507ccf650a2b304a2ea76797e7846564e620", "bugFilePath": "llap-common/src/test/org/apache/hadoop/hive/llap/tez/TestConverters.java", "fixPatch": "diff --git a/llap-common/src/test/org/apache/hadoop/hive/llap/tez/TestConverters.java b/llap-common/src/test/org/apache/hadoop/hive/llap/tez/TestConverters.java\nindex 349ee14..1df6df0 100644\n--- a/llap-common/src/test/org/apache/hadoop/hive/llap/tez/TestConverters.java\n+++ b/llap-common/src/test/org/apache/hadoop/hive/llap/tez/TestConverters.java\n@@ -42,7 +42,7 @@\n \n public class TestConverters {\n \n-  @Test(timeout = 5000)\n+  @Test(timeout = 10000)\n   public void testTaskSpecToFragmentSpec() {\n     ByteBuffer procBb = ByteBuffer.allocate(4);\n     procBb.putInt(0, 200);\n@@ -98,7 +98,7 @@\n \n   }\n \n-  @Test (timeout = 5000)\n+  @Test (timeout = 10000)\n   public void testFragmentSpecToTaskSpec() {\n \n     ByteBuffer procBb = ByteBuffer.allocate(4);\n@@ -142,7 +142,7 @@\n \n     SignableVertexSpec vertexProto = builder.build();\n \n-    TaskSpec taskSpec = Converters.getTaskSpecfromProto(vertexProto, 0, 0, null);\n+    TaskSpec taskSpec = Converters.getTaskSpecfromProto(vertexProto, 0, 0, tezTaskAttemptId);\n \n     assertEquals(\"dagName\", taskSpec.getDAGName());\n     assertEquals(\"vertexName\", taskSpec.getVertexName());\n", "projectName": "apache.hive", "bugLineNum": 45, "bugNodeStartChar": 1837, "bugNodeLength": 14, "fixLineNum": 45, "fixNodeStartChar": 1837, "fixNodeLength": 15, "sourceBeforeFix": "timeout=5000", "sourceAfterFix": "timeout=10000"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "e1626ef3aeecb1ccf7acf4056fb7bcf23243be25", "fixCommitParentSHA1": "a354507ccf650a2b304a2ea76797e7846564e620", "bugFilePath": "llap-common/src/test/org/apache/hadoop/hive/llap/tez/TestConverters.java", "fixPatch": "diff --git a/llap-common/src/test/org/apache/hadoop/hive/llap/tez/TestConverters.java b/llap-common/src/test/org/apache/hadoop/hive/llap/tez/TestConverters.java\nindex 349ee14..1df6df0 100644\n--- a/llap-common/src/test/org/apache/hadoop/hive/llap/tez/TestConverters.java\n+++ b/llap-common/src/test/org/apache/hadoop/hive/llap/tez/TestConverters.java\n@@ -42,7 +42,7 @@\n \n public class TestConverters {\n \n-  @Test(timeout = 5000)\n+  @Test(timeout = 10000)\n   public void testTaskSpecToFragmentSpec() {\n     ByteBuffer procBb = ByteBuffer.allocate(4);\n     procBb.putInt(0, 200);\n@@ -98,7 +98,7 @@\n \n   }\n \n-  @Test (timeout = 5000)\n+  @Test (timeout = 10000)\n   public void testFragmentSpecToTaskSpec() {\n \n     ByteBuffer procBb = ByteBuffer.allocate(4);\n@@ -142,7 +142,7 @@\n \n     SignableVertexSpec vertexProto = builder.build();\n \n-    TaskSpec taskSpec = Converters.getTaskSpecfromProto(vertexProto, 0, 0, null);\n+    TaskSpec taskSpec = Converters.getTaskSpecfromProto(vertexProto, 0, 0, tezTaskAttemptId);\n \n     assertEquals(\"dagName\", taskSpec.getDAGName());\n     assertEquals(\"vertexName\", taskSpec.getVertexName());\n", "projectName": "apache.hive", "bugLineNum": 101, "bugNodeStartChar": 4746, "bugNodeLength": 14, "fixLineNum": 101, "fixNodeStartChar": 4746, "fixNodeLength": 15, "sourceBeforeFix": "timeout=5000", "sourceAfterFix": "timeout=10000"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "64454faa3bbc2c1d709779762d9f93a15a1b8c15", "fixCommitParentSHA1": "ff38cfb76e91a21ab10077d7ac62c6828514b0ad", "bugFilePath": "itests/hive-unit/src/test/java/org/apache/hive/service/cli/session/TestHiveSessionImpl.java", "fixPatch": "diff --git a/itests/hive-unit/src/test/java/org/apache/hive/service/cli/session/TestHiveSessionImpl.java b/itests/hive-unit/src/test/java/org/apache/hive/service/cli/session/TestHiveSessionImpl.java\nindex c9e6a13..d58a913 100644\n--- a/itests/hive-unit/src/test/java/org/apache/hive/service/cli/session/TestHiveSessionImpl.java\n+++ b/itests/hive-unit/src/test/java/org/apache/hive/service/cli/session/TestHiveSessionImpl.java\n@@ -70,7 +70,7 @@\n     Map<String, String> confOverlay = new HashMap<String, String>();\n     String hql = \"drop table if exists table_not_exists\";\n     Mockito.when(operationManager.newExecuteStatementOperation(same(session), eq(hql),\n-        (Map<String, String>)Mockito.any(), eq(true), eq(0))).thenReturn(operation);\n+        (Map<String, String>)Mockito.any(), eq(true), eq(0L))).thenReturn(operation);\n \n     try {\n \n", "projectName": "apache.hive", "bugLineNum": 73, "bugNodeStartChar": 2887, "bugNodeLength": 5, "fixLineNum": 73, "fixNodeStartChar": 2887, "fixNodeLength": 6, "sourceBeforeFix": "eq(0)", "sourceAfterFix": "eq(0L)"}, {"bugType": "MORE_SPECIFIC_IF", "fixCommitSHA1": "298644f6618b992dbb3352e26e51792edfb08605", "fixCommitParentSHA1": "57d666a27e7a6fc82244c921d074b2194d3b8f1f", "bugFilePath": "metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java", "fixPatch": "diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java b/metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java\nindex 3c125e0..c0d9c0c 100644\n--- a/metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java\n+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/RetryingMetaStoreClient.java\n@@ -176,13 +176,15 @@\n           // TODO: most protocol exceptions are probably unrecoverable... throw?\n           caughtException = (TException)t;\n         } else if ((t instanceof MetaException) && t.getMessage().matches(\n-            \"(?s).*(JDO[a-zA-Z]*|TProtocol|TTransport)Exception.*\")) {\n+            \"(?s).*(JDO[a-zA-Z]*|TProtocol|TTransport)Exception.*\") &&\n+            !t.getMessage().contains(\"java.sql.SQLIntegrityConstraintViolationException\")) {\n           caughtException = (MetaException)t;\n         } else {\n           throw t;\n         }\n       } catch (MetaException e) {\n-        if (e.getMessage().matches(\"(?s).*(IO|TTransport)Exception.*\")) {\n+        if (e.getMessage().matches(\"(?s).*(IO|TTransport)Exception.*\") &&\n+            !e.getMessage().contains(\"java.sql.SQLIntegrityConstraintViolationException\")) {\n           caughtException = e;\n         } else {\n           throw e;\n", "projectName": "apache.hive", "bugLineNum": 178, "bugNodeStartChar": 7476, "bugNodeLength": 123, "fixLineNum": 178, "fixNodeStartChar": 7476, "fixNodeLength": 216, "sourceBeforeFix": "(t instanceof MetaException) && t.getMessage().matches(\"(?s).*(JDO[a-zA-Z]*|TProtocol|TTransport)Exception.*\")", "sourceAfterFix": "(t instanceof MetaException) && t.getMessage().matches(\"(?s).*(JDO[a-zA-Z]*|TProtocol|TTransport)Exception.*\") && !t.getMessage().contains(\"java.sql.SQLIntegrityConstraintViolationException\")"}, {"bugType": "CHANGE_OPERATOR", "fixCommitSHA1": "4eb960305f6cf30aa6e1011ee09388b1ab4c4fd9", "fixCommitParentSHA1": "da82819bc112589e0d96874947c942e834681ed2", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java\nindex 41507b1..a8ed74c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java\n@@ -83,7 +83,7 @@\n \n   public static <T> T findSingleOperatorUpstreamJoinAccounted(Operator<?> start, Class<T> clazz) {\n     Set<T> found = findOperatorsUpstreamJoinAccounted(start, clazz, new HashSet<T>());\n-    return found.size() == 1 ? found.iterator().next(): null;\n+    return found.size() >= 1 ? found.iterator().next(): null;\n   }\n \n   public static <T> Set<T> findOperatorsUpstream(Collection<Operator<?>> starts, Class<T> clazz) {\n", "projectName": "apache.hive", "bugLineNum": 86, "bugNodeStartChar": 3084, "bugNodeLength": 17, "fixLineNum": 86, "fixNodeStartChar": 3084, "fixNodeLength": 17, "sourceBeforeFix": "found.size() == 1", "sourceAfterFix": "found.size() >= 1"}, {"bugType": "CHANGE_CALLER_IN_FUNCTION_CALL", "fixCommitSHA1": "8c0d148cd20199c647b2550ae51068f531ba42a3", "fixCommitParentSHA1": "531652e681e57aafdcfd7929c603131eebf8890a", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveReduceExpressionsRule.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveReduceExpressionsRule.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveReduceExpressionsRule.java\nindex 8f15ec7..1d2c4cc 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveReduceExpressionsRule.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveReduceExpressionsRule.java\n@@ -582,7 +582,7 @@\n       }\n       node = super.visitCall(call);\n       if (node != call) {\n-        node = RexUtil.simplify(rexBuilder, node);\n+        node = HiveRexUtil.simplify(rexBuilder, node);\n       }\n       return node;\n     }\n", "projectName": "apache.hive", "bugLineNum": 585, "bugNodeStartChar": 22451, "bugNodeLength": 34, "fixLineNum": 585, "fixNodeStartChar": 22451, "fixNodeLength": 38, "sourceBeforeFix": "RexUtil.simplify(rexBuilder,node)", "sourceAfterFix": "HiveRexUtil.simplify(rexBuilder,node)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "8c0d148cd20199c647b2550ae51068f531ba42a3", "fixCommitParentSHA1": "531652e681e57aafdcfd7929c603131eebf8890a", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveReduceExpressionsRule.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveReduceExpressionsRule.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveReduceExpressionsRule.java\nindex 8f15ec7..1d2c4cc 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveReduceExpressionsRule.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/rules/HiveReduceExpressionsRule.java\n@@ -582,7 +582,7 @@\n       }\n       node = super.visitCall(call);\n       if (node != call) {\n-        node = RexUtil.simplify(rexBuilder, node);\n+        node = HiveRexUtil.simplify(rexBuilder, node);\n       }\n       return node;\n     }\n", "projectName": "apache.hive", "bugLineNum": 585, "bugNodeStartChar": 22451, "bugNodeLength": 34, "fixLineNum": 585, "fixNodeStartChar": 22451, "fixNodeLength": 38, "sourceBeforeFix": "RexUtil.simplify(rexBuilder,node)", "sourceAfterFix": "HiveRexUtil.simplify(rexBuilder,node)"}, {"bugType": "SWAP_ARGUMENTS", "fixCommitSHA1": "0a526774d5a012e0f1cb1adef2a5e3d982e39395", "fixCommitParentSHA1": "8260a7975ccae12786bbcc1435f6880a28a97f2c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java\nindex 9c929af..ee4f4ea 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/calcite/translator/RexNodeConverter.java\n@@ -455,7 +455,7 @@\n         // An alternative would be to throw CboSemanticException and fall back\n         // to no CBO.\n         RelDataType relType = cluster.getTypeFactory().createSqlType(SqlTypeName.DECIMAL,\n-            bd.scale(), unscaled.toString().length());\n+            unscaled.toString().length(), bd.scale());\n         calciteLiteral = rexBuilder.makeExactLiteral(bd, relType);\n       }\n       break;\n", "projectName": "apache.hive", "bugLineNum": 457, "bugNodeStartChar": 20262, "bugNodeLength": 113, "fixLineNum": 457, "fixNodeStartChar": 20262, "fixNodeLength": 113, "sourceBeforeFix": "cluster.getTypeFactory().createSqlType(SqlTypeName.DECIMAL,bd.scale(),unscaled.toString().length())", "sourceAfterFix": "cluster.getTypeFactory().createSqlType(SqlTypeName.DECIMAL,unscaled.toString().length(),bd.scale())"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "9deed11a655f8ae5c308c6389025fd38983e0f8e", "fixCommitParentSHA1": "53cf9ec025d5d8ce45f1ca0277023f839881f466", "bugFilePath": "llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java", "fixPatch": "diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java\nindex fc66254..bb9f341 100644\n--- a/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java\n+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/LlapTaskReporter.java\n@@ -350,13 +350,15 @@\n       float progress = 0;\n       if (task.hasInitialized()) {\n         progress = task.getProgress();\n+        // TODO HIVE-12449. Make use of progress notifications once Hive starts sending them out.\n+        // progressNotified = task.getAndClearProgressNotification();\n         if (sendCounters) {\n           // send these potentially large objects at longer intervals to avoid overloading the AM\n           counters = task.getCounters();\n           stats = task.getTaskStatistics();\n         }\n       }\n-      return new TaskStatusUpdateEvent(counters, progress, stats);\n+      return new TaskStatusUpdateEvent(counters, progress, stats, true);\n     }\n \n     /**\n", "projectName": "apache.hive", "bugLineNum": 359, "bugNodeStartChar": 14641, "bugNodeLength": 52, "fixLineNum": 359, "fixNodeStartChar": 14641, "fixNodeLength": 58, "sourceBeforeFix": "new TaskStatusUpdateEvent(counters,progress,stats)", "sourceAfterFix": "new TaskStatusUpdateEvent(counters,progress,stats,true)"}, {"bugType": "MORE_SPECIFIC_IF", "fixCommitSHA1": "1806dbce41bddc3cbb6741ca87c05f5fe83cb3f4", "fixCommitParentSHA1": "b2efd1a695ddaf073bbe0bf311075e542e1b865b", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\nindex 4ebdf90..e1a0c4a 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n@@ -9643,7 +9643,7 @@\n          * as Join conditions\n          */\n         Set<String> dests = qb.getParseInfo().getClauseNames();\n-        if ( dests.size() == 1 ) {\n+        if ( dests.size() == 1 && joinTree.getNoOuterJoin()) {\n           String dest = dests.iterator().next();\n           ASTNode whereClause = qb.getParseInfo().getWhrForClause(dest);\n           if ( whereClause != null ) {\n", "projectName": "apache.hive", "bugLineNum": 9646, "bugNodeStartChar": 392536, "bugNodeLength": 17, "fixLineNum": 9646, "fixNodeStartChar": 392536, "fixNodeLength": 46, "sourceBeforeFix": "dests.size() == 1", "sourceAfterFix": "dests.size() == 1 && joinTree.getNoOuterJoin()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "39a82524ce0dd6d5d30629a9ea02a8bedde2cfd3", "fixCommitParentSHA1": "f679a5e19da55e1ef90179ea06ae999582601588", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HiveSplitGenerator.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HiveSplitGenerator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HiveSplitGenerator.java\nindex 532d242..8ebfe69 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HiveSplitGenerator.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/HiveSplitGenerator.java\n@@ -140,7 +140,7 @@\n               TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_MIN_SIZE,\n               TezMapReduceSplitsGrouper.TEZ_GROUPING_SPLIT_MIN_SIZE_DEFAULT);\n           final long preferredSplitSize = Math.min(blockSize / 2, minGrouping);\n-          HiveConf.setLongVar(conf, HiveConf.ConfVars.MAPREDMINSPLITSIZE, preferredSplitSize);\n+          HiveConf.setLongVar(jobConf, HiveConf.ConfVars.MAPREDMINSPLITSIZE, preferredSplitSize);\n           LOG.info(\"The preferred split size is \" + preferredSplitSize);\n         }\n \n", "projectName": "apache.hive", "bugLineNum": 143, "bugNodeStartChar": 6502, "bugNodeLength": 83, "fixLineNum": 143, "fixNodeStartChar": 6502, "fixNodeLength": 86, "sourceBeforeFix": "HiveConf.setLongVar(conf,HiveConf.ConfVars.MAPREDMINSPLITSIZE,preferredSplitSize)", "sourceAfterFix": "HiveConf.setLongVar(jobConf,HiveConf.ConfVars.MAPREDMINSPLITSIZE,preferredSplitSize)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "de1fe68b90ff9e29a21095035c7ed02dbbf35f26", "fixCommitParentSHA1": "1357f6338796600fe37b81bb11600ad56da3d4e2", "bugFilePath": "jdbc/src/java/org/apache/hive/jdbc/ZooKeeperHiveClientHelper.java", "fixPatch": "diff --git a/jdbc/src/java/org/apache/hive/jdbc/ZooKeeperHiveClientHelper.java b/jdbc/src/java/org/apache/hive/jdbc/ZooKeeperHiveClientHelper.java\nindex 7195515..1ca77a1 100644\n--- a/jdbc/src/java/org/apache/hive/jdbc/ZooKeeperHiveClientHelper.java\n+++ b/jdbc/src/java/org/apache/hive/jdbc/ZooKeeperHiveClientHelper.java\n@@ -159,7 +159,7 @@\n         }\n         // KERBEROS\n         // If delegation token is passed from the client side, do not set the principal\n-        if (matcher.group(2).equalsIgnoreCase(\"hive.server2.authentication.kerberos.principal\")\n+        if (matcher.group(1).equalsIgnoreCase(\"hive.server2.authentication.kerberos.principal\")\n             && !(connParams.getSessionVars().containsKey(JdbcConnectionParams.AUTH_TYPE) && connParams\n                 .getSessionVars().get(JdbcConnectionParams.AUTH_TYPE)\n                 .equalsIgnoreCase(JdbcConnectionParams.AUTH_TOKEN))\n", "projectName": "apache.hive", "bugLineNum": 162, "bugNodeStartChar": 7421, "bugNodeLength": 16, "fixLineNum": 162, "fixNodeStartChar": 7421, "fixNodeLength": 16, "sourceBeforeFix": "matcher.group(2)", "sourceAfterFix": "matcher.group(1)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "2db6ccb1f28236997b4aebab501b4d1be0d776fe", "fixCommitParentSHA1": "cc9efdde16ecd49129ead1a1e2e67ff26962160a", "bugFilePath": "llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java", "fixPatch": "diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java\nindex 157f9d3..5c95086 100644\n--- a/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java\n+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java\n@@ -217,7 +217,7 @@\n               serviceConsumerMetadata, envMap, startedInputsMap, taskReporter, executor,\n               objectRegistry,\n               pid,\n-              executionContext, memoryAvailable);\n+              executionContext, memoryAvailable, false);\n         }\n       }\n       if (taskRunner == null) {\n", "projectName": "apache.hive", "bugLineNum": 214, "bugNodeStartChar": 9651, "bugNodeLength": 318, "fixLineNum": 214, "fixNodeStartChar": 9651, "fixNodeLength": 325, "sourceBeforeFix": "new TezTaskRunner2(conf,taskUgi,fragmentInfo.getLocalDirs(),taskSpec,request.getAppAttemptNumber(),serviceConsumerMetadata,envMap,startedInputsMap,taskReporter,executor,objectRegistry,pid,executionContext,memoryAvailable)", "sourceAfterFix": "new TezTaskRunner2(conf,taskUgi,fragmentInfo.getLocalDirs(),taskSpec,request.getAppAttemptNumber(),serviceConsumerMetadata,envMap,startedInputsMap,taskReporter,executor,objectRegistry,pid,executionContext,memoryAvailable,false)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "3c1eae0c44450ebbf26c88066de871ea7f479981", "fixCommitParentSHA1": "1030ae719e2bbea47d8f2835244589621ab126fb", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNvl.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNvl.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNvl.java\nindex 625b466..0a16da8 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNvl.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFNvl.java\n@@ -42,7 +42,7 @@\n     returnOIResolver = new GenericUDFUtils.ReturnObjectInspectorResolver(true);\n     if (!(returnOIResolver.update(arguments[0]) && returnOIResolver\n         .update(arguments[1]))) {\n-      throw new UDFArgumentTypeException(2,\n+      throw new UDFArgumentTypeException(1,\n           \"The first and seconds arguments of function NLV should have the same type, \"\n           + \"but they are different: \\\"\" + arguments[0].getTypeName()\n           + \"\\\" and \\\"\" + arguments[1].getTypeName() + \"\\\"\");\n", "projectName": "apache.hive", "bugLineNum": 45, "bugNodeStartChar": 2090, "bugNodeLength": 250, "fixLineNum": 45, "fixNodeStartChar": 2090, "fixNodeLength": 250, "sourceBeforeFix": "new UDFArgumentTypeException(2,\"The first and seconds arguments of function NLV should have the same type, \" + \"but they are different: \\\"\" + arguments[0].getTypeName() + \"\\\" and \\\"\"+ arguments[1].getTypeName()+ \"\\\"\")", "sourceAfterFix": "new UDFArgumentTypeException(1,\"The first and seconds arguments of function NLV should have the same type, \" + \"but they are different: \\\"\" + arguments[0].getTypeName() + \"\\\" and \\\"\"+ arguments[1].getTypeName()+ \"\\\"\")"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "7e53685310fa22abd12307e141511184fa6ede3a", "fixCommitParentSHA1": "70631bb4cff0c0cbd7055e843e091bfd4fae8e4e", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java\nindex 2e77bc4..9546191 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java\n@@ -283,7 +283,7 @@\n   public void setOutputFormatClass(Class<? extends HiveOutputFormat> outputFormatClass) {\n     this.outputFormatClass = outputFormatClass;\n     tPartition.getSd().setOutputFormat(HiveFileFormatUtils\n-        .getOutputFormatSubstitute(outputFormatClass).toString());\n+        .getOutputFormatSubstitute(outputFormatClass).getName());\n   }\n \n   final public Class<? extends InputFormat> getInputFormatClass()\n", "projectName": "apache.hive", "bugLineNum": 285, "bugNodeStartChar": 9880, "bugNodeLength": 84, "fixLineNum": 285, "fixNodeStartChar": 9880, "fixNodeLength": 83, "sourceBeforeFix": "HiveFileFormatUtils.getOutputFormatSubstitute(outputFormatClass).toString()", "sourceAfterFix": "HiveFileFormatUtils.getOutputFormatSubstitute(outputFormatClass).getName()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "7e53685310fa22abd12307e141511184fa6ede3a", "fixCommitParentSHA1": "70631bb4cff0c0cbd7055e843e091bfd4fae8e4e", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java\nindex 2e77bc4..9546191 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Partition.java\n@@ -283,7 +283,7 @@\n   public void setOutputFormatClass(Class<? extends HiveOutputFormat> outputFormatClass) {\n     this.outputFormatClass = outputFormatClass;\n     tPartition.getSd().setOutputFormat(HiveFileFormatUtils\n-        .getOutputFormatSubstitute(outputFormatClass).toString());\n+        .getOutputFormatSubstitute(outputFormatClass).getName());\n   }\n \n   final public Class<? extends InputFormat> getInputFormatClass()\n", "projectName": "apache.hive", "bugLineNum": 285, "bugNodeStartChar": 9880, "bugNodeLength": 84, "fixLineNum": 285, "fixNodeStartChar": 9880, "fixNodeLength": 83, "sourceBeforeFix": "HiveFileFormatUtils.getOutputFormatSubstitute(outputFormatClass).toString()", "sourceAfterFix": "HiveFileFormatUtils.getOutputFormatSubstitute(outputFormatClass).getName()"}, {"bugType": "MORE_SPECIFIC_IF", "fixCommitSHA1": "d3a879a7ed73895620c2dbc6f6ea2b321ecc6574", "fixCommitParentSHA1": "89c02bf5f57f2bcadf6dd329c47b8b7074b2b75b", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\nindex 424f4fa..cc85f31 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java\n@@ -410,7 +410,7 @@\n               HiveSemanticAnalyzerHook.class);\n \n       // Do semantic analysis and plan generation\n-      if (saHooks != null) {\n+      if (saHooks != null && !saHooks.isEmpty()) {\n         HiveSemanticAnalyzerHookContext hookCtx = new HiveSemanticAnalyzerHookContextImpl();\n         hookCtx.setConf(conf);\n         hookCtx.setUserName(userName);\n", "projectName": "apache.hive", "bugLineNum": 413, "bugNodeStartChar": 14658, "bugNodeLength": 15, "fixLineNum": 413, "fixNodeStartChar": 14658, "fixNodeLength": 37, "sourceBeforeFix": "saHooks != null", "sourceAfterFix": "saHooks != null && !saHooks.isEmpty()"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "9c64f9378143b6804c384f0333014b8f1cbd8edd", "fixCommitParentSHA1": "97b4750c6314eea9025b426e4df73f795b601927", "bugFilePath": "serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java", "fixPatch": "diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java b/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java\nindex 40ede1a..c65174e 100644\n--- a/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java\n+++ b/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java\n@@ -555,6 +555,6 @@\n   }\n \n   public static Text transformTextFromUTF8(Text text, Charset targetCharset) {\n-    return new Text(new String(text.getBytes()).getBytes(targetCharset));\n+    return new Text(new String(text.getBytes(), 0, text.getLength()).getBytes(targetCharset));\n   }\n }\n", "projectName": "apache.hive", "bugLineNum": 558, "bugNodeStartChar": 18569, "bugNodeLength": 27, "fixLineNum": 558, "fixNodeStartChar": 18569, "fixNodeLength": 48, "sourceBeforeFix": "new String(text.getBytes())", "sourceAfterFix": "new String(text.getBytes(),0,text.getLength())"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "03eca7708ef2b1d9bdf3a4265639dd3d5541ca74", "fixCommitParentSHA1": "b09fcfb558c6cdb451a76924ea2c14f64282c61b", "bugFilePath": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/MetadataListStructObjectInspector.java", "fixPatch": "diff --git a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/MetadataListStructObjectInspector.java b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/MetadataListStructObjectInspector.java\nindex e68325f..5a2beb9 100644\n--- a/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/MetadataListStructObjectInspector.java\n+++ b/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/MetadataListStructObjectInspector.java\n@@ -49,7 +49,7 @@\n       List<String> columnNames) {\n     ArrayList<List<String>> key = new ArrayList<List<String>>(1);\n     key.add(columnNames);\n-    MetadataListStructObjectInspector result = cached.get(columnNames);\n+    MetadataListStructObjectInspector result = cached.get(key);\n     if (result == null) {\n       result = new MetadataListStructObjectInspector(columnNames);\n       MetadataListStructObjectInspector prev = cached.putIfAbsent(key, result);\n", "projectName": "apache.hive", "bugLineNum": 52, "bugNodeStartChar": 2223, "bugNodeLength": 23, "fixLineNum": 52, "fixNodeStartChar": 2223, "fixNodeLength": 15, "sourceBeforeFix": "cached.get(columnNames)", "sourceAfterFix": "cached.get(key)"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "3bf41faa04284b1f50adcd7da50dbb74664a8396", "fixCommitParentSHA1": "ce736af2a5025a4bb07b39362b064bd64aecdeef", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java\nindex d9c16dc..8c4b6ea 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorColumnSetInfo.java\n@@ -126,7 +126,8 @@\n       doubleIndices[doubleIndicesIndex] = addIndex;\n       indexLookup[addIndex].setDouble(doubleIndicesIndex);\n       ++doubleIndicesIndex;\n-    } else if (VectorizationContext.isStringFamily(outputType)) {\n+    } else if (VectorizationContext.isStringFamily(outputType) ||\n+        outputType.equalsIgnoreCase(\"binary\")) {\n       stringIndices[stringIndicesIndex]= addIndex;\n       indexLookup[addIndex].setString(stringIndicesIndex);\n       ++stringIndicesIndex;\n", "projectName": "apache.hive", "bugLineNum": 129, "bugNodeStartChar": 3780, "bugNodeLength": 47, "fixLineNum": 129, "fixNodeStartChar": 3780, "fixNodeLength": 96, "sourceBeforeFix": "VectorizationContext.isStringFamily(outputType)", "sourceAfterFix": "VectorizationContext.isStringFamily(outputType) || outputType.equalsIgnoreCase(\"binary\")"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "1d4261b1f58bf1a79df2caa11146869f2b47ecf3", "fixCommitParentSHA1": "6d5c403d3aa62500d2640137bfc0306af6f0a582", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java\nindex cdf4f49..bde9fc2 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java\n@@ -385,10 +385,10 @@\n         result = evaluatePredicateBloomFilter(predicate, predObj, bloomFilter, hasNull);\n       }\n       // in case failed conversion, return the default YES_NO_NULL truth value\n-    } catch (NumberFormatException nfe) {\n+    } catch (Exception e) {\n       if (LOG.isWarnEnabled()) {\n-        LOG.warn(\"NumberFormatException when type matching predicate object\" +\n-            \" and statistics object. Exception: \" + ExceptionUtils.getStackTrace(nfe));\n+        LOG.warn(\"Exception when evaluating predicate. Skipping ORC PPD.\" +\n+            \" Exception: \" + ExceptionUtils.getStackTrace(e));\n       }\n       result = hasNull ? TruthValue.YES_NO_NULL : TruthValue.YES_NO;\n     }\n", "projectName": "apache.hive", "bugLineNum": 388, "bugNodeStartChar": 15009, "bugNodeLength": 25, "fixLineNum": 388, "fixNodeStartChar": 15009, "fixNodeLength": 11, "sourceBeforeFix": "NumberFormatException nfe", "sourceAfterFix": "Exception e"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "1d4261b1f58bf1a79df2caa11146869f2b47ecf3", "fixCommitParentSHA1": "6d5c403d3aa62500d2640137bfc0306af6f0a582", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java b/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java\nindex d61af63..a451bfb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java\n@@ -128,7 +128,7 @@\n         case ORC:\n           // adapt base type to what orc needs\n           if (literal instanceof Integer) {\n-            return Long.valueOf(literal.toString());\n+            return ((Number) literal).longValue();\n           }\n           return literal;\n         case PARQUET:\n@@ -157,9 +157,9 @@\n           Integer)) {\n         return literalList;\n       }\n-      List<Object> result = new ArrayList<Object>();\n+      List<Object> result = new ArrayList<Object>(literalList.size());\n       for (Object o : literalList) {\n-        result.add(Long.valueOf(o.toString()));\n+        result.add(((Number) o).longValue());\n       }\n       return result;\n     }\n@@ -1114,7 +1114,7 @@\n       } else if (literal instanceof Byte ||\n           literal instanceof Short ||\n           literal instanceof Integer) {\n-        return Long.valueOf(literal.toString());\n+        return ((Number) literal).longValue();\n       } else if (literal instanceof Float) {\n         // to avoid change in precision when upcasting float to double\n         // we convert the literal to string and parse it as double. (HIVE-8460)\n", "projectName": "apache.hive", "bugLineNum": 160, "bugNodeStartChar": 5774, "bugNodeLength": 23, "fixLineNum": 160, "fixNodeStartChar": 5774, "fixNodeLength": 41, "sourceBeforeFix": "new ArrayList<Object>()", "sourceAfterFix": "new ArrayList<Object>(literalList.size())"}, {"bugType": "CHANGE_OPERATOR", "fixCommitSHA1": "74194eed2e9b2755ec9828178ed378040b42f0d1", "fixCommitParentSHA1": "7ebf999e2fdee5de00a145653a1e58de53650602", "bugFilePath": "llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataProducer.java", "fixPatch": "diff --git a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataProducer.java b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataProducer.java\nindex f450302..93f4257 100644\n--- a/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataProducer.java\n+++ b/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataProducer.java\n@@ -126,7 +126,7 @@\n         metadata = getOrReadFileMetadata();\n         int bufferSize = metadata.getCompressionBufferSize();\n         int minAllocSize = HiveConf.getIntVar(conf, HiveConf.ConfVars.LLAP_ORC_CACHE_MIN_ALLOC);\n-        if (bufferSize != minAllocSize) {\n+        if (bufferSize < minAllocSize) {\n           throw new IOException(\"ORC compression buffer size (\" + bufferSize + \") is smaller than\" +\n               \" LLAP low-level cache minimum allocation size (\" + minAllocSize + \"). Decrease the\" +\n               \" value for \" + HiveConf.ConfVars.LLAP_ORC_CACHE_MIN_ALLOC.toString());\n", "projectName": "apache.hive", "bugLineNum": 129, "bugNodeStartChar": 5147, "bugNodeLength": 26, "fixLineNum": 129, "fixNodeStartChar": 5147, "fixNodeLength": 25, "sourceBeforeFix": "bufferSize != minAllocSize", "sourceAfterFix": "bufferSize < minAllocSize"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "85c0bc256211160178d49bead3f228b78a21e1c4", "fixCommitParentSHA1": "465c5777c9807b92fc039f0f3b8c3affd56882bb", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java\nindex e2db7b1..0073812 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java\n@@ -89,7 +89,7 @@\n     if (arguments[1].getCategory() != ObjectInspector.Category.PRIMITIVE) {\n       throw new UDFArgumentTypeException(1,\n         \"Only primitive type arguments are accepted but \"\n-        + arguments[2].getTypeName() + \" is passed. as second arguments\");\n+        + arguments[1].getTypeName() + \" is passed. as second arguments\");\n     }\n \n     inputType1 = ((PrimitiveObjectInspector) arguments[0]).getPrimitiveCategory();\n", "projectName": "apache.hive", "bugLineNum": 92, "bugNodeStartChar": 4491, "bugNodeLength": 12, "fixLineNum": 92, "fixNodeStartChar": 4491, "fixNodeLength": 12, "sourceBeforeFix": "arguments[2]", "sourceAfterFix": "arguments[1]"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "85c0bc256211160178d49bead3f228b78a21e1c4", "fixCommitParentSHA1": "465c5777c9807b92fc039f0f3b8c3affd56882bb", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java\nindex cb13060..290a5bc 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java\n@@ -89,7 +89,7 @@\n     if (arguments[1].getCategory() != ObjectInspector.Category.PRIMITIVE) {\n       throw new UDFArgumentTypeException(1,\n           \"Only primitive type arguments are accepted but \"\n-              + arguments[2].getTypeName() + \" is passed. as second arguments\");\n+              + arguments[1].getTypeName() + \" is passed. as second arguments\");\n     }\n \n     inputType1 = ((PrimitiveObjectInspector) arguments[0]).getPrimitiveCategory();\n", "projectName": "apache.hive", "bugLineNum": 92, "bugNodeStartChar": 4515, "bugNodeLength": 12, "fixLineNum": 92, "fixNodeStartChar": 4515, "fixNodeLength": 12, "sourceBeforeFix": "arguments[2]", "sourceAfterFix": "arguments[1]"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "fb5ffe6bfddb8dda8a2b1a1a25e48abe0a853e30", "fixCommitParentSHA1": "92cadc6e619294234a996b4c0bf74b363224269c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java\nindex d0dae2b..4809f9b 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java\n@@ -53,7 +53,7 @@\n     OriginalType annotation = type.getOriginalType();\n     if (annotation == OriginalType.LIST) {\n       return HiveCollectionConverter.forList(type, parent, index);\n-    } else if (annotation == OriginalType.MAP) {\n+    } else if (annotation == OriginalType.MAP || annotation == OriginalType.MAP_KEY_VALUE) {\n       return HiveCollectionConverter.forMap(type, parent, index);\n     }\n \n", "projectName": "apache.hive", "bugLineNum": 56, "bugNodeStartChar": 1855, "bugNodeLength": 30, "fixLineNum": 56, "fixNodeStartChar": 1855, "fixNodeLength": 74, "sourceBeforeFix": "annotation == OriginalType.MAP", "sourceAfterFix": "annotation == OriginalType.MAP || annotation == OriginalType.MAP_KEY_VALUE"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "fb5ffe6bfddb8dda8a2b1a1a25e48abe0a853e30", "fixCommitParentSHA1": "92cadc6e619294234a996b4c0bf74b363224269c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java\nindex d0dae2b..4809f9b 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java\n@@ -53,7 +53,7 @@\n     OriginalType annotation = type.getOriginalType();\n     if (annotation == OriginalType.LIST) {\n       return HiveCollectionConverter.forList(type, parent, index);\n-    } else if (annotation == OriginalType.MAP) {\n+    } else if (annotation == OriginalType.MAP || annotation == OriginalType.MAP_KEY_VALUE) {\n       return HiveCollectionConverter.forMap(type, parent, index);\n     }\n \n", "projectName": "apache.hive", "bugLineNum": 56, "bugNodeStartChar": 1855, "bugNodeLength": 30, "fixLineNum": 56, "fixNodeStartChar": 1855, "fixNodeLength": 74, "sourceBeforeFix": "annotation == OriginalType.MAP", "sourceAfterFix": "annotation == OriginalType.MAP || annotation == OriginalType.MAP_KEY_VALUE"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "39c82b017cbbc7239c90e792acd9d0e4efdb44de", "fixCommitParentSHA1": "a8dfec5d8afb60252af94ee261509edb24259fd2", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java\nindex c4f04cb..c735be1 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java\n@@ -129,7 +129,7 @@\n       rowsRet = work.getLimit() >= 0 ? Math.min(work.getLimit() - totalRows, maxRows) : maxRows;\n     }\n     try {\n-      if (rowsRet <= 0) {\n+      if (rowsRet <= 0 || work.getLimit() == totalRows) {\n         fetch.clearFetchContext();\n         return false;\n       }\n", "projectName": "apache.hive", "bugLineNum": 132, "bugNodeStartChar": 4309, "bugNodeLength": 12, "fixLineNum": 132, "fixNodeStartChar": 4309, "fixNodeLength": 44, "sourceBeforeFix": "rowsRet <= 0", "sourceAfterFix": "rowsRet <= 0 || work.getLimit() == totalRows"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "39c82b017cbbc7239c90e792acd9d0e4efdb44de", "fixCommitParentSHA1": "a8dfec5d8afb60252af94ee261509edb24259fd2", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java\nindex c4f04cb..c735be1 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java\n@@ -129,7 +129,7 @@\n       rowsRet = work.getLimit() >= 0 ? Math.min(work.getLimit() - totalRows, maxRows) : maxRows;\n     }\n     try {\n-      if (rowsRet <= 0) {\n+      if (rowsRet <= 0 || work.getLimit() == totalRows) {\n         fetch.clearFetchContext();\n         return false;\n       }\n", "projectName": "apache.hive", "bugLineNum": 132, "bugNodeStartChar": 4309, "bugNodeLength": 12, "fixLineNum": 132, "fixNodeStartChar": 4309, "fixNodeLength": 44, "sourceBeforeFix": "rowsRet <= 0", "sourceAfterFix": "rowsRet <= 0 || work.getLimit() == totalRows"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "6e4e6d32757138712bcba38d7ab1ed8ae97cdcb5", "fixCommitParentSHA1": "a19c501787a2f68a716307dbed2ed7333f448e57", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java\nindex a056822..ffd14e0 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java\n@@ -216,7 +216,7 @@\n     @Override\n     public StructField getStructFieldRef(String s) {\n       for(StructField field: fields) {\n-        if (field.getFieldName().equals(s)) {\n+        if (field.getFieldName().equalsIgnoreCase(s)) {\n           return field;\n         }\n       }\n@@ -304,7 +304,7 @@\n         for(int i = 0; i < fields.size(); ++i) {\n           StructField left = other.get(i);\n           StructField right = fields.get(i);\n-          if (!(left.getFieldName().equals(right.getFieldName()) &&\n+          if (!(left.getFieldName().equalsIgnoreCase(right.getFieldName()) &&\n                 left.getFieldObjectInspector().equals\n                     (right.getFieldObjectInspector()))) {\n             return false;\n", "projectName": "apache.hive", "bugLineNum": 219, "bugNodeStartChar": 6556, "bugNodeLength": 30, "fixLineNum": 219, "fixNodeStartChar": 6556, "fixNodeLength": 40, "sourceBeforeFix": "field.getFieldName().equals(s)", "sourceAfterFix": "field.getFieldName().equalsIgnoreCase(s)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "6e4e6d32757138712bcba38d7ab1ed8ae97cdcb5", "fixCommitParentSHA1": "a19c501787a2f68a716307dbed2ed7333f448e57", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java\nindex a056822..ffd14e0 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java\n@@ -216,7 +216,7 @@\n     @Override\n     public StructField getStructFieldRef(String s) {\n       for(StructField field: fields) {\n-        if (field.getFieldName().equals(s)) {\n+        if (field.getFieldName().equalsIgnoreCase(s)) {\n           return field;\n         }\n       }\n@@ -304,7 +304,7 @@\n         for(int i = 0; i < fields.size(); ++i) {\n           StructField left = other.get(i);\n           StructField right = fields.get(i);\n-          if (!(left.getFieldName().equals(right.getFieldName()) &&\n+          if (!(left.getFieldName().equalsIgnoreCase(right.getFieldName()) &&\n                 left.getFieldObjectInspector().equals\n                     (right.getFieldObjectInspector()))) {\n             return false;\n", "projectName": "apache.hive", "bugLineNum": 219, "bugNodeStartChar": 6556, "bugNodeLength": 30, "fixLineNum": 219, "fixNodeStartChar": 6556, "fixNodeLength": 40, "sourceBeforeFix": "field.getFieldName().equals(s)", "sourceAfterFix": "field.getFieldName().equalsIgnoreCase(s)"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "6e4e6d32757138712bcba38d7ab1ed8ae97cdcb5", "fixCommitParentSHA1": "a19c501787a2f68a716307dbed2ed7333f448e57", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java\nindex a056822..ffd14e0 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java\n@@ -216,7 +216,7 @@\n     @Override\n     public StructField getStructFieldRef(String s) {\n       for(StructField field: fields) {\n-        if (field.getFieldName().equals(s)) {\n+        if (field.getFieldName().equalsIgnoreCase(s)) {\n           return field;\n         }\n       }\n@@ -304,7 +304,7 @@\n         for(int i = 0; i < fields.size(); ++i) {\n           StructField left = other.get(i);\n           StructField right = fields.get(i);\n-          if (!(left.getFieldName().equals(right.getFieldName()) &&\n+          if (!(left.getFieldName().equalsIgnoreCase(right.getFieldName()) &&\n                 left.getFieldObjectInspector().equals\n                     (right.getFieldObjectInspector()))) {\n             return false;\n", "projectName": "apache.hive", "bugLineNum": 307, "bugNodeStartChar": 9011, "bugNodeLength": 48, "fixLineNum": 307, "fixNodeStartChar": 9011, "fixNodeLength": 58, "sourceBeforeFix": "left.getFieldName().equals(right.getFieldName())", "sourceAfterFix": "left.getFieldName().equalsIgnoreCase(right.getFieldName())"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "6e4e6d32757138712bcba38d7ab1ed8ae97cdcb5", "fixCommitParentSHA1": "a19c501787a2f68a716307dbed2ed7333f448e57", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java\nindex a056822..ffd14e0 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java\n@@ -216,7 +216,7 @@\n     @Override\n     public StructField getStructFieldRef(String s) {\n       for(StructField field: fields) {\n-        if (field.getFieldName().equals(s)) {\n+        if (field.getFieldName().equalsIgnoreCase(s)) {\n           return field;\n         }\n       }\n@@ -304,7 +304,7 @@\n         for(int i = 0; i < fields.size(); ++i) {\n           StructField left = other.get(i);\n           StructField right = fields.get(i);\n-          if (!(left.getFieldName().equals(right.getFieldName()) &&\n+          if (!(left.getFieldName().equalsIgnoreCase(right.getFieldName()) &&\n                 left.getFieldObjectInspector().equals\n                     (right.getFieldObjectInspector()))) {\n             return false;\n", "projectName": "apache.hive", "bugLineNum": 307, "bugNodeStartChar": 9011, "bugNodeLength": 48, "fixLineNum": 307, "fixNodeStartChar": 9011, "fixNodeLength": 58, "sourceBeforeFix": "left.getFieldName().equals(right.getFieldName())", "sourceAfterFix": "left.getFieldName().equalsIgnoreCase(right.getFieldName())"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "28f046ef4ac16e7d260dfd505400c329030ebbe8", "fixCommitParentSHA1": "577c841433cf3bbff94f00ae557c23c12586c7ba", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\nindex 590b828..15c86d2 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n@@ -8289,7 +8289,7 @@\n       }\n     }\n \n-    if ( targetCondn == null ) {\n+    if ( targetCondn == null || (nodeCondn.size() != targetCondn.size())) {\n       return new ObjectPair(-1, null);\n     }\n \n", "projectName": "apache.hive", "bugLineNum": 8292, "bugNodeStartChar": 337753, "bugNodeLength": 19, "fixLineNum": 8292, "fixNodeStartChar": 337753, "fixNodeLength": 63, "sourceBeforeFix": "targetCondn == null", "sourceAfterFix": "targetCondn == null || (nodeCondn.size() != targetCondn.size())"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "28f046ef4ac16e7d260dfd505400c329030ebbe8", "fixCommitParentSHA1": "577c841433cf3bbff94f00ae557c23c12586c7ba", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\nindex 590b828..15c86d2 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n@@ -8289,7 +8289,7 @@\n       }\n     }\n \n-    if ( targetCondn == null ) {\n+    if ( targetCondn == null || (nodeCondn.size() != targetCondn.size())) {\n       return new ObjectPair(-1, null);\n     }\n \n", "projectName": "apache.hive", "bugLineNum": 8292, "bugNodeStartChar": 337753, "bugNodeLength": 19, "fixLineNum": 8292, "fixNodeStartChar": 337753, "fixNodeLength": 63, "sourceBeforeFix": "targetCondn == null", "sourceAfterFix": "targetCondn == null || (nodeCondn.size() != targetCondn.size())"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "856d677bd206104de06ed33649576ad7ae90a6d2", "fixCommitParentSHA1": "9ddc0a3f2afc320bf2c6a7838aa2aaf7c96cda0e", "bugFilePath": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "fixPatch": "diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java\nindex a8411c9..20dbafe 100644\n--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java\n+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java\n@@ -375,9 +375,9 @@\n     METASTORECONNECTURLKEY(\"javax.jdo.option.ConnectionURL\",\n         \"jdbc:derby:;databaseName=metastore_db;create=true\",\n         \"JDBC connect string for a JDBC metastore\"),\n-    HMSHANDLERATTEMPTS(\"hive.hmshandler.retry.attempts\", 1,\n+    HMSHANDLERATTEMPTS(\"hive.hmshandler.retry.attempts\", 10,\n         \"The number of times to retry a HMSHandler call if there were a connection error.\"),\n-    HMSHANDLERINTERVAL(\"hive.hmshandler.retry.interval\", \"1000ms\",\n+    HMSHANDLERINTERVAL(\"hive.hmshandler.retry.interval\", \"2000ms\",\n         new TimeValidator(TimeUnit.MILLISECONDS), \"The time between HMSHandler retry attempts on failure.\"),\n     HMSHANDLERFORCERELOADCONF(\"hive.hmshandler.force.reload.conf\", false,\n         \"Whether to force reloading of the HMSHandler configuration (including\\n\" +\n", "projectName": "apache.hive", "bugLineNum": 378, "bugNodeStartChar": 21693, "bugNodeLength": 147, "fixLineNum": 378, "fixNodeStartChar": 21693, "fixNodeLength": 148, "sourceBeforeFix": "HMSHANDLERATTEMPTS(\"hive.hmshandler.retry.attempts\",1,\"The number of times to retry a HMSHandler call if there were a connection error.\")", "sourceAfterFix": "HMSHANDLERATTEMPTS(\"hive.hmshandler.retry.attempts\",10,\"The number of times to retry a HMSHandler call if there were a connection error.\")"}, {"bugType": "MORE_SPECIFIC_IF", "fixCommitSHA1": "0872cece78c1ed649fda87bdb69a23eaf30ccd71", "fixCommitParentSHA1": "6ad3c32b82d12c2d81adba629d916aa20e16e0ee", "bugFilePath": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java", "fixPatch": "diff --git a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java\nindex 2f624df..63909b8 100644\n--- a/hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java\n+++ b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/common/HCatUtil.java\n@@ -84,7 +84,8 @@\n   private static volatile HiveClientCache hiveClientCache;\n \n   public static boolean checkJobContextIfRunningFromBackend(JobContext j) {\n-    if (j.getConfiguration().get(\"mapred.task.id\", \"\").equals(\"\") &&\n+    if (j.getConfiguration().get(\"pig.job.converted.fetch\", \"\").equals(\"\") &&\n+          j.getConfiguration().get(\"mapred.task.id\", \"\").equals(\"\") &&\n         !(\"true\".equals(j.getConfiguration().get(\"pig.illustrating\")))) {\n       return false;\n     }\n", "projectName": "apache.hive", "bugLineNum": 87, "bugNodeStartChar": 3745, "bugNodeLength": 131, "fixLineNum": 87, "fixNodeStartChar": 3745, "fixNodeLength": 211, "sourceBeforeFix": "j.getConfiguration().get(\"mapred.task.id\",\"\").equals(\"\") && !(\"true\".equals(j.getConfiguration().get(\"pig.illustrating\")))", "sourceAfterFix": "j.getConfiguration().get(\"pig.job.converted.fetch\",\"\").equals(\"\") && j.getConfiguration().get(\"mapred.task.id\",\"\").equals(\"\") && !(\"true\".equals(j.getConfiguration().get(\"pig.illustrating\")))"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "40a36841e2ed139a2a8714ccdafcaee49693f48f", "fixCommitParentSHA1": "32019c10bbd7215aba0856f50dfa1b80fdb59088", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\nindex 48d334a..3c83a7c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n@@ -12423,7 +12423,7 @@\n                   + \" does not have the field \" + field));\n         }\n         if (!lInfo.getInternalName().equals(rInfo.getInternalName())) {\n-          throw new SemanticException(generateErrorMessage(tabref,\n+          throw new OptiqSemanticException(generateErrorMessage(tabref,\n               \"Schema of both sides of union should match: field \" + field + \":\"\n                   + \" appears on the left side of the UNION at column position: \"\n                   + getPositionFromInternalName(lInfo.getInternalName())\n@@ -12435,7 +12435,7 @@\n         TypeInfo commonTypeInfo = FunctionRegistry.getCommonClassForUnionAll(lInfo.getType(),\n             rInfo.getType());\n         if (commonTypeInfo == null) {\n-          throw new SemanticException(generateErrorMessage(tabref,\n+          throw new OptiqSemanticException(generateErrorMessage(tabref,\n               \"Schema of both sides of union should match: Column \" + field + \" is of type \"\n                   + lInfo.getType().getTypeName() + \" on first table and type \"\n                   + rInfo.getType().getTypeName() + \" on second table\"));\n@@ -13293,7 +13293,7 @@\n                 grpbyExpr, new TypeCheckCtx(groupByInputRowResolver));\n             ExprNodeDesc grpbyExprNDesc = astToExprNDescMap.get(grpbyExpr);\n             if (grpbyExprNDesc == null)\n-              throw new RuntimeException(\"Invalid Column Reference: \" + grpbyExpr.dump());\n+              throw new OptiqSemanticException(\"Invalid Column Reference: \" + grpbyExpr.dump());\n \n             addToGBExpr(groupByOutputRowResolver, groupByInputRowResolver, grpbyExpr,\n                 grpbyExprNDesc, gbExprNDescLst, outputColumnNames);\n", "projectName": "apache.hive", "bugLineNum": 12426, "bugNodeStartChar": 493659, "bugNodeLength": 507, "fixLineNum": 12426, "fixNodeStartChar": 493659, "fixNodeLength": 512, "sourceBeforeFix": "new SemanticException(generateErrorMessage(tabref,\"Schema of both sides of union should match: field \" + field + \":\"+ \" appears on the left side of the UNION at column position: \"+ getPositionFromInternalName(lInfo.getInternalName())+ \", and on the right side of the UNION at column position: \"+ getPositionFromInternalName(rInfo.getInternalName())+ \". Column positions should match for a UNION\"))", "sourceAfterFix": "new OptiqSemanticException(generateErrorMessage(tabref,\"Schema of both sides of union should match: field \" + field + \":\"+ \" appears on the left side of the UNION at column position: \"+ getPositionFromInternalName(lInfo.getInternalName())+ \", and on the right side of the UNION at column position: \"+ getPositionFromInternalName(rInfo.getInternalName())+ \". Column positions should match for a UNION\"))"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "40a36841e2ed139a2a8714ccdafcaee49693f48f", "fixCommitParentSHA1": "32019c10bbd7215aba0856f50dfa1b80fdb59088", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\nindex 48d334a..3c83a7c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n@@ -12423,7 +12423,7 @@\n                   + \" does not have the field \" + field));\n         }\n         if (!lInfo.getInternalName().equals(rInfo.getInternalName())) {\n-          throw new SemanticException(generateErrorMessage(tabref,\n+          throw new OptiqSemanticException(generateErrorMessage(tabref,\n               \"Schema of both sides of union should match: field \" + field + \":\"\n                   + \" appears on the left side of the UNION at column position: \"\n                   + getPositionFromInternalName(lInfo.getInternalName())\n@@ -12435,7 +12435,7 @@\n         TypeInfo commonTypeInfo = FunctionRegistry.getCommonClassForUnionAll(lInfo.getType(),\n             rInfo.getType());\n         if (commonTypeInfo == null) {\n-          throw new SemanticException(generateErrorMessage(tabref,\n+          throw new OptiqSemanticException(generateErrorMessage(tabref,\n               \"Schema of both sides of union should match: Column \" + field + \" is of type \"\n                   + lInfo.getType().getTypeName() + \" on first table and type \"\n                   + rInfo.getType().getTypeName() + \" on second table\"));\n@@ -13293,7 +13293,7 @@\n                 grpbyExpr, new TypeCheckCtx(groupByInputRowResolver));\n             ExprNodeDesc grpbyExprNDesc = astToExprNDescMap.get(grpbyExpr);\n             if (grpbyExprNDesc == null)\n-              throw new RuntimeException(\"Invalid Column Reference: \" + grpbyExpr.dump());\n+              throw new OptiqSemanticException(\"Invalid Column Reference: \" + grpbyExpr.dump());\n \n             addToGBExpr(groupByOutputRowResolver, groupByInputRowResolver, grpbyExpr,\n                 grpbyExprNDesc, gbExprNDescLst, outputColumnNames);\n", "projectName": "apache.hive", "bugLineNum": 12438, "bugNodeStartChar": 494412, "bugNodeLength": 296, "fixLineNum": 12438, "fixNodeStartChar": 494412, "fixNodeLength": 301, "sourceBeforeFix": "new SemanticException(generateErrorMessage(tabref,\"Schema of both sides of union should match: Column \" + field + \" is of type \"+ lInfo.getType().getTypeName()+ \" on first table and type \"+ rInfo.getType().getTypeName()+ \" on second table\"))", "sourceAfterFix": "new OptiqSemanticException(generateErrorMessage(tabref,\"Schema of both sides of union should match: Column \" + field + \" is of type \"+ lInfo.getType().getTypeName()+ \" on first table and type \"+ rInfo.getType().getTypeName()+ \" on second table\"))"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "40a36841e2ed139a2a8714ccdafcaee49693f48f", "fixCommitParentSHA1": "32019c10bbd7215aba0856f50dfa1b80fdb59088", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\nindex 48d334a..3c83a7c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n@@ -12423,7 +12423,7 @@\n                   + \" does not have the field \" + field));\n         }\n         if (!lInfo.getInternalName().equals(rInfo.getInternalName())) {\n-          throw new SemanticException(generateErrorMessage(tabref,\n+          throw new OptiqSemanticException(generateErrorMessage(tabref,\n               \"Schema of both sides of union should match: field \" + field + \":\"\n                   + \" appears on the left side of the UNION at column position: \"\n                   + getPositionFromInternalName(lInfo.getInternalName())\n@@ -12435,7 +12435,7 @@\n         TypeInfo commonTypeInfo = FunctionRegistry.getCommonClassForUnionAll(lInfo.getType(),\n             rInfo.getType());\n         if (commonTypeInfo == null) {\n-          throw new SemanticException(generateErrorMessage(tabref,\n+          throw new OptiqSemanticException(generateErrorMessage(tabref,\n               \"Schema of both sides of union should match: Column \" + field + \" is of type \"\n                   + lInfo.getType().getTypeName() + \" on first table and type \"\n                   + rInfo.getType().getTypeName() + \" on second table\"));\n@@ -13293,7 +13293,7 @@\n                 grpbyExpr, new TypeCheckCtx(groupByInputRowResolver));\n             ExprNodeDesc grpbyExprNDesc = astToExprNDescMap.get(grpbyExpr);\n             if (grpbyExprNDesc == null)\n-              throw new RuntimeException(\"Invalid Column Reference: \" + grpbyExpr.dump());\n+              throw new OptiqSemanticException(\"Invalid Column Reference: \" + grpbyExpr.dump());\n \n             addToGBExpr(groupByOutputRowResolver, groupByInputRowResolver, grpbyExpr,\n                 grpbyExprNDesc, gbExprNDescLst, outputColumnNames);\n", "projectName": "apache.hive", "bugLineNum": 13296, "bugNodeStartChar": 531869, "bugNodeLength": 69, "fixLineNum": 13296, "fixNodeStartChar": 531869, "fixNodeLength": 75, "sourceBeforeFix": "new RuntimeException(\"Invalid Column Reference: \" + grpbyExpr.dump())", "sourceAfterFix": "new OptiqSemanticException(\"Invalid Column Reference: \" + grpbyExpr.dump())"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "cd7f13802b17a581105bb4d0aec2f8a72669108d", "fixCommitParentSHA1": "c9407a471887f91fffd786ec2ad2b3699f6d3522", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java\nindex 94db145..8043686 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java\n@@ -162,7 +162,7 @@\n \n     List<Path> inputPaths;\n     try {\n-      inputPaths = Utilities.getInputPaths(jobConf, mapWork, emptyScratchDir, ctx);\n+      inputPaths = Utilities.getInputPaths(jobConf, mapWork, emptyScratchDir, ctx, false);\n     } catch (Exception e2) {\n       e2.printStackTrace();\n       return -1;\n", "projectName": "apache.hive", "bugLineNum": 165, "bugNodeStartChar": 6110, "bugNodeLength": 63, "fixLineNum": 165, "fixNodeStartChar": 6110, "fixNodeLength": 70, "sourceBeforeFix": "Utilities.getInputPaths(jobConf,mapWork,emptyScratchDir,ctx)", "sourceAfterFix": "Utilities.getInputPaths(jobConf,mapWork,emptyScratchDir,ctx,false)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "28830d90dc3031fa489a6cfb8ef05c535184c09c", "fixCommitParentSHA1": "19402b588cb72a351bbc2b4623a799c4e6031331", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/processors/CommandUtil.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/processors/CommandUtil.java b/ql/src/java/org/apache/hadoop/hive/ql/processors/CommandUtil.java\nindex 7eceb21..87c8fbb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/processors/CommandUtil.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/processors/CommandUtil.java\n@@ -68,7 +68,7 @@\n   static void authorizeCommandThrowEx(SessionState ss, HiveOperationType type,\n       List<String> command) throws HiveAuthzPluginException, HiveAccessControlException {\n     HivePrivilegeObject commandObj = HivePrivilegeObject.createHivePrivilegeObject(command);\n-    ss.getAuthorizerV2().checkPrivileges(type, Arrays.asList(commandObj), null);\n+    ss.getAuthorizerV2().checkPrivileges(type, Arrays.asList(commandObj), null, null);\n   }\n \n \n", "projectName": "apache.hive", "bugLineNum": 71, "bugNodeStartChar": 2652, "bugNodeLength": 75, "fixLineNum": 71, "fixNodeStartChar": 2652, "fixNodeLength": 81, "sourceBeforeFix": "ss.getAuthorizerV2().checkPrivileges(type,Arrays.asList(commandObj),null)", "sourceAfterFix": "ss.getAuthorizerV2().checkPrivileges(type,Arrays.asList(commandObj),null,null)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "4633bbc0240015345b63f1ca91afffcb6c727838", "fixCommitParentSHA1": "ad33c56db0ba40c18c18ac4827271bbf14ea99ab", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java\nindex 2a809cf..ee6f7d7 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkClient.java\n@@ -56,7 +56,7 @@\n \n   private static String sparkHome = \"/home/xzhang/apache/spark\";\n   \n-  private static int reducerCount = 5;\n+  private static int reducerCount = 1;\n   \n   private static String execMem = \"1g\";\n   private static String execJvmOpts = \"\";\n", "projectName": "apache.hive", "bugLineNum": 59, "bugNodeStartChar": 2273, "bugNodeLength": 16, "fixLineNum": 59, "fixNodeStartChar": 2273, "fixNodeLength": 16, "sourceBeforeFix": "reducerCount=5", "sourceAfterFix": "reducerCount=1"}, {"bugType": "MORE_SPECIFIC_IF", "fixCommitSHA1": "77407490afdacb115dcd80dc3905fcf831e7f38c", "fixCommitParentSHA1": "93ca31b010885065df670552f52497143b2a3c83", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java\nindex 041639d..0d0ac41 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java\n@@ -271,7 +271,7 @@\n   public void closeAndOpen(TezSessionState sessionState, HiveConf conf)\n       throws Exception {\n     HiveConf sessionConf = sessionState.getConf();\n-    if (sessionConf.get(\"tez.queue.name\") != null) {\n+    if (sessionConf != null && sessionConf.get(\"tez.queue.name\") != null) {\n       conf.set(\"tez.queue.name\", sessionConf.get(\"tez.queue.name\"));\n     }\n     close(sessionState);\n", "projectName": "apache.hive", "bugLineNum": 274, "bugNodeStartChar": 9869, "bugNodeLength": 41, "fixLineNum": 274, "fixNodeStartChar": 9869, "fixNodeLength": 64, "sourceBeforeFix": "sessionConf.get(\"tez.queue.name\") != null", "sourceAfterFix": "sessionConf != null && sessionConf.get(\"tez.queue.name\") != null"}, {"bugType": "SWAP_BOOLEAN_LITERAL", "fixCommitSHA1": "77407490afdacb115dcd80dc3905fcf831e7f38c", "fixCommitParentSHA1": "93ca31b010885065df670552f52497143b2a3c83", "bugFilePath": "ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java", "fixPatch": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java\nindex 1af92cf..3d55a7c 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java\n@@ -203,7 +203,7 @@\n     task.submit(conf, dag, path, appLr, sessionState);\n     // validate close/reopen\n     verify(sessionState, times(1)).open(any(HiveConf.class));\n-    verify(sessionState, times(1)).close(eq(true));\n+    verify(sessionState, times(1)).close(eq(false));  // now uses pool after HIVE-7043\n     verify(session, times(2)).submitDAG(any(DAG.class));\n   }\n \n", "projectName": "apache.hive", "bugLineNum": 206, "bugNodeStartChar": 7184, "bugNodeLength": 8, "fixLineNum": 206, "fixNodeStartChar": 7184, "fixNodeLength": 9, "sourceBeforeFix": "eq(true)", "sourceAfterFix": "eq(false)"}, {"bugType": "CHANGE_OPERATOR", "fixCommitSHA1": "21810014f007080b96d6fa46deeef760ec542934", "fixCommitParentSHA1": "e7a708738fd815ae0676b42d111c325086ed59a2", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java\nindex 45f9d3d..0ccc3ad 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java\n@@ -754,7 +754,7 @@\n               // eliminate stripes that doesn't satisfy the predicate condition\n               includeStripe = new boolean[stripes.size()];\n               for(int i=0; i < stripes.size(); ++i) {\n-                includeStripe[i] = (i > stripeStats.size()) ||\n+                includeStripe[i] = (i >= stripeStats.size()) ||\n                     isStripeSatisfyPredicate(stripeStats.get(i), sarg,\n                                              filterColumns);\n                 if (LOG.isDebugEnabled() && !includeStripe[i]) {\n", "projectName": "apache.hive", "bugLineNum": 757, "bugNodeStartChar": 27723, "bugNodeLength": 22, "fixLineNum": 757, "fixNodeStartChar": 27723, "fixNodeLength": 23, "sourceBeforeFix": "i > stripeStats.size()", "sourceAfterFix": "i >= stripeStats.size()"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "8bbd614034d1949d4f0f787d56c510848eaaa787", "fixCommitParentSHA1": "8e55a16169d3ef6a9e01596ddb6063b6e625029a", "bugFilePath": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java", "fixPatch": "diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java\nindex 16d54c6..4763cb9 100644\n--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java\n+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java\n@@ -795,7 +795,7 @@\n     // Number of async threads\n     HIVE_SERVER2_ASYNC_EXEC_THREADS(\"hive.server2.async.exec.threads\", 100),\n     // Number of seconds HiveServer2 shutdown will wait for async threads to terminate\n-    HIVE_SERVER2_ASYNC_EXEC_SHUTDOWN_TIMEOUT(\"hive.server2.async.exec.shutdown.timeout\", 10L),\n+    HIVE_SERVER2_ASYNC_EXEC_SHUTDOWN_TIMEOUT(\"hive.server2.async.exec.shutdown.timeout\", 10),\n     // Size of the wait queue for async thread pool in HiveServer2.\n     // After hitting this limit, the async thread pool will reject new requests.\n     HIVE_SERVER2_ASYNC_EXEC_WAIT_QUEUE_SIZE(\"hive.server2.async.exec.wait.queue.size\", 100),\n", "projectName": "apache.hive", "bugLineNum": 798, "bugNodeStartChar": 44071, "bugNodeLength": 89, "fixLineNum": 798, "fixNodeStartChar": 44071, "fixNodeLength": 88, "sourceBeforeFix": "HIVE_SERVER2_ASYNC_EXEC_SHUTDOWN_TIMEOUT(\"hive.server2.async.exec.shutdown.timeout\",10L)", "sourceAfterFix": "HIVE_SERVER2_ASYNC_EXEC_SHUTDOWN_TIMEOUT(\"hive.server2.async.exec.shutdown.timeout\",10)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "8e55a16169d3ef6a9e01596ddb6063b6e625029a", "fixCommitParentSHA1": "e5856ddb1b6efa9f7433af242fc4488fe684584f", "bugFilePath": "service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java", "fixPatch": "diff --git a/service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java b/service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java\nindex 85abba7..5342214 100644\n--- a/service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java\n+++ b/service/src/java/org/apache/hive/service/auth/LdapAuthenticationProviderImpl.java\n@@ -70,7 +70,7 @@\n       DirContext ctx = new InitialDirContext(env);\n       ctx.close();\n     } catch (NamingException e) {\n-      throw new AuthenticationException(\"Error validating LDAP user\");\n+      throw new AuthenticationException(\"Error validating LDAP user\", e);\n     }\n   return;\n   }\n", "projectName": "apache.hive", "bugLineNum": 73, "bugNodeStartChar": 2651, "bugNodeLength": 57, "fixLineNum": 73, "fixNodeStartChar": 2651, "fixNodeLength": 60, "sourceBeforeFix": "new AuthenticationException(\"Error validating LDAP user\")", "sourceAfterFix": "new AuthenticationException(\"Error validating LDAP user\",e)"}, {"bugType": "MORE_SPECIFIC_IF", "fixCommitSHA1": "6aed5d407b8b3293402995430d606029112b44c2", "fixCommitParentSHA1": "327f14da6752d9057269e0adbbfdc04d000323da", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java\nindex b9456e8..74d1c05 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java\n@@ -377,7 +377,7 @@\n         if (work.getLoadTableDesc() != null &&\n             !work.getLoadTableDesc().getReplace()) {\n           String originalValue = parameters.get(statType);\n-          if (originalValue != null) {\n+          if (originalValue != null && !originalValue.equals(\"-1\")) {\n             longValue += Long.parseLong(originalValue);\n           }\n         }\n", "projectName": "apache.hive", "bugLineNum": 380, "bugNodeStartChar": 14076, "bugNodeLength": 21, "fixLineNum": 380, "fixNodeStartChar": 14076, "fixNodeLength": 52, "sourceBeforeFix": "originalValue != null", "sourceAfterFix": "originalValue != null && !originalValue.equals(\"-1\")"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "a0ecc248be51f8b5b518174457ac9c9875179c45", "fixCommitParentSHA1": "9636394c4ae98ff9ed46277be3e6c07fa3c43c7f", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java\nindex 7dbd96f..5834af7 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java\n@@ -32,7 +32,7 @@\n public class ASTNode extends CommonTree implements Node,Serializable {\n   private static final long serialVersionUID = 1L;\n \n-  private ASTNodeOrigin origin;\n+  private transient ASTNodeOrigin origin;\n \n   public ASTNode() {\n   }\n", "projectName": "apache.hive", "bugLineNum": 35, "bugNodeStartChar": 1198, "bugNodeLength": 29, "fixLineNum": 35, "fixNodeStartChar": 1198, "fixNodeLength": 39, "sourceBeforeFix": "2", "sourceAfterFix": "130"}, {"bugType": "MORE_SPECIFIC_IF", "fixCommitSHA1": "369e76ceaaa61354716708061801b48cf75b386d", "fixCommitParentSHA1": "8f73d2e6490454f2777b3a2c9a248f627e7118b8", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java\nindex c39c46b..d5b35d0 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java\n@@ -227,7 +227,9 @@\n \n   @Override\n   public void closeOp(boolean abort) throws HiveException {\n-    if (mapJoinTables != null) {\n+    if ((this.getExecContext().getLocalWork() != null\n+        && this.getExecContext().getLocalWork().getInputFileChangeSensitive())\n+        && mapJoinTables != null) {\n       for (MapJoinTableContainer tableContainer : mapJoinTables) {\n         if (tableContainer != null) {\n           tableContainer.clear();\n", "projectName": "apache.hive", "bugLineNum": 230, "bugNodeStartChar": 8202, "bugNodeLength": 21, "fixLineNum": 230, "fixNodeStartChar": 8202, "fixNodeLength": 157, "sourceBeforeFix": "mapJoinTables != null", "sourceAfterFix": "(this.getExecContext().getLocalWork() != null && this.getExecContext().getLocalWork().getInputFileChangeSensitive()) && mapJoinTables != null"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "fc9a05a1e4ace9cce5940b7dc606a8bea763a3f9", "fixCommitParentSHA1": "83d0c134bffb9b8953caf659e767ee4c2f594f64", "bugFilePath": "jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java", "fixPatch": "diff --git a/jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java b/jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java\nindex c658dbd..bb36337 100644\n--- a/jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java\n+++ b/jdbc/src/test/org/apache/hive/jdbc/TestJdbcDriver2.java\n@@ -789,7 +789,7 @@\n   }\n \n   public void testMetaDataGetTables() throws SQLException {\n-    getTablesTest(TableType.MANAGED_TABLE.toString(), TableType.VIRTUAL_VIEW.toString());\n+    getTablesTest(ClassicTableTypes.TABLE.toString(), ClassicTableTypes.VIEW.toString());\n   }\n \n   public  void testMetaDataGetTablesHive() throws SQLException {\n@@ -807,6 +807,13 @@\n     getTablesTest(ClassicTableTypes.TABLE.toString(), ClassicTableTypes.VIEW.toString());\n   }\n \n+  /**\n+   * Test the type returned for pre-created table type table and view type\n+   * table\n+   * @param tableTypeName expected table type\n+   * @param viewTypeName expected view type\n+   * @throws SQLException\n+   */\n   private void getTablesTest(String tableTypeName, String viewTypeName) throws SQLException {\n     Map<String, Object[]> tests = new HashMap<String, Object[]>();\n     tests.put(\"test%jdbc%\", new Object[]{\"testhivejdbcdriver_table\"\n@@ -887,10 +894,14 @@\n     rs.close();\n   }\n \n+  //test default table types returned in\n+  // Connection.getMetaData().getTableTypes()\n   public void testMetaDataGetTableTypes() throws SQLException {\n-    metaDataGetTableTypeTest(new HiveTableTypeMapping().getTableTypeNames());\n+    metaDataGetTableTypeTest(new ClassicTableTypeMapping().getTableTypeNames());\n   }\n \n+  //test default table types returned in\n+  // Connection.getMetaData().getTableTypes() when type config is set to \"HIVE\"\n   public void testMetaDataGetHiveTableTypes() throws SQLException {\n     Statement stmt = con.createStatement();\n     stmt.execute(\"set \" + HiveConf.ConfVars.HIVE_SERVER2_TABLE_TYPE_MAPPING.varname +\n@@ -899,6 +910,8 @@\n     metaDataGetTableTypeTest(new HiveTableTypeMapping().getTableTypeNames());\n   }\n \n+  //test default table types returned in\n+  // Connection.getMetaData().getTableTypes() when type config is set to \"CLASSIC\"\n   public void testMetaDataGetClassicTableTypes() throws SQLException {\n     Statement stmt = con.createStatement();\n     stmt.execute(\"set \" + HiveConf.ConfVars.HIVE_SERVER2_TABLE_TYPE_MAPPING.varname +\n@@ -907,6 +920,12 @@\n     metaDataGetTableTypeTest(new ClassicTableTypeMapping().getTableTypeNames());\n   }\n \n+  /**\n+   * Test if Connection.getMetaData().getTableTypes() returns expected\n+   *  tabletypes\n+   * @param tabletypes expected table types\n+   * @throws SQLException\n+   */\n   private void metaDataGetTableTypeTest(Set<String> tabletypes)\n       throws SQLException {\n     ResultSet rs = (ResultSet)con.getMetaData().getTableTypes();\n", "projectName": "apache.hive", "bugLineNum": 891, "bugNodeStartChar": 32570, "bugNodeLength": 26, "fixLineNum": 891, "fixNodeStartChar": 32570, "fixNodeLength": 29, "sourceBeforeFix": "new HiveTableTypeMapping()", "sourceAfterFix": "new ClassicTableTypeMapping()"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "364ae9baf3824dc4e0e56b2b681f8302731e8cdd", "fixCommitParentSHA1": "6b386d4b81f63bc3afd05888529550572da04c06", "bugFilePath": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "fixPatch": "diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\nindex 6c2aeb4..bad5ea2 100644\n--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n@@ -3104,7 +3104,7 @@\n       try {\n         statsObj = getMS().getTableColumnStatistics(dbName, tableName, colName);\n       } finally {\n-        endFunction(\"get_column_statistics_by_table: \", statsObj != null);\n+        endFunction(\"get_column_statistics_by_table: \", statsObj != null, null);\n       }\n       return statsObj;\n     }\n@@ -3126,7 +3126,7 @@\n         statsObj = getMS().getPartitionColumnStatistics(dbName, tableName, convertedPartName,\n                                                             partVals, colName);\n       } finally {\n-        endFunction(\"get_column_statistics_by_partition: \", statsObj != null);\n+        endFunction(\"get_column_statistics_by_partition: \", statsObj != null, null);\n       }\n       return statsObj;\n    }\n@@ -3165,7 +3165,7 @@\n         ret = getMS().updateTableColumnStatistics(colStats);\n         return ret;\n       } finally {\n-        endFunction(\"write_column_statistics: \", ret != false);\n+        endFunction(\"write_column_statistics: \", ret != false, null);\n       }\n     }\n \n@@ -3211,7 +3211,7 @@\n         ret = getMS().updatePartitionColumnStatistics(colStats, partVals);\n         return ret;\n       } finally {\n-        endFunction(\"write_partition_column_statistics: \", ret != false);\n+        endFunction(\"write_partition_column_statistics: \", ret != false, null);\n       }\n     }\n \n@@ -3234,7 +3234,7 @@\n         ret = getMS().deletePartitionColumnStatistics(dbName, tableName,\n                                                       convertedPartName, partVals, colName);\n       } finally {\n-        endFunction(\"delete_column_statistics_by_partition: \", ret != false);\n+        endFunction(\"delete_column_statistics_by_partition: \", ret != false, null);\n       }\n       return ret;\n     }\n@@ -3256,7 +3256,7 @@\n       try {\n         ret = getMS().deleteTableColumnStatistics(dbName, tableName, colName);\n       } finally {\n-        endFunction(\"delete_column_statistics_by_table: \", ret != false);\n+        endFunction(\"delete_column_statistics_by_table: \", ret != false, null);\n       }\n       return ret;\n    }\n", "projectName": "apache.hive", "bugLineNum": 3107, "bugNodeStartChar": 111095, "bugNodeLength": 65, "fixLineNum": 3107, "fixNodeStartChar": 111095, "fixNodeLength": 71, "sourceBeforeFix": "endFunction(\"get_column_statistics_by_table: \",statsObj != null)", "sourceAfterFix": "endFunction(\"get_column_statistics_by_table: \",statsObj != null,null)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "364ae9baf3824dc4e0e56b2b681f8302731e8cdd", "fixCommitParentSHA1": "6b386d4b81f63bc3afd05888529550572da04c06", "bugFilePath": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "fixPatch": "diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\nindex 6c2aeb4..bad5ea2 100644\n--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n@@ -3104,7 +3104,7 @@\n       try {\n         statsObj = getMS().getTableColumnStatistics(dbName, tableName, colName);\n       } finally {\n-        endFunction(\"get_column_statistics_by_table: \", statsObj != null);\n+        endFunction(\"get_column_statistics_by_table: \", statsObj != null, null);\n       }\n       return statsObj;\n     }\n@@ -3126,7 +3126,7 @@\n         statsObj = getMS().getPartitionColumnStatistics(dbName, tableName, convertedPartName,\n                                                             partVals, colName);\n       } finally {\n-        endFunction(\"get_column_statistics_by_partition: \", statsObj != null);\n+        endFunction(\"get_column_statistics_by_partition: \", statsObj != null, null);\n       }\n       return statsObj;\n    }\n@@ -3165,7 +3165,7 @@\n         ret = getMS().updateTableColumnStatistics(colStats);\n         return ret;\n       } finally {\n-        endFunction(\"write_column_statistics: \", ret != false);\n+        endFunction(\"write_column_statistics: \", ret != false, null);\n       }\n     }\n \n@@ -3211,7 +3211,7 @@\n         ret = getMS().updatePartitionColumnStatistics(colStats, partVals);\n         return ret;\n       } finally {\n-        endFunction(\"write_partition_column_statistics: \", ret != false);\n+        endFunction(\"write_partition_column_statistics: \", ret != false, null);\n       }\n     }\n \n@@ -3234,7 +3234,7 @@\n         ret = getMS().deletePartitionColumnStatistics(dbName, tableName,\n                                                       convertedPartName, partVals, colName);\n       } finally {\n-        endFunction(\"delete_column_statistics_by_partition: \", ret != false);\n+        endFunction(\"delete_column_statistics_by_partition: \", ret != false, null);\n       }\n       return ret;\n     }\n@@ -3256,7 +3256,7 @@\n       try {\n         ret = getMS().deleteTableColumnStatistics(dbName, tableName, colName);\n       } finally {\n-        endFunction(\"delete_column_statistics_by_table: \", ret != false);\n+        endFunction(\"delete_column_statistics_by_table: \", ret != false, null);\n       }\n       return ret;\n    }\n", "projectName": "apache.hive", "bugLineNum": 3129, "bugNodeStartChar": 112144, "bugNodeLength": 69, "fixLineNum": 3129, "fixNodeStartChar": 112144, "fixNodeLength": 75, "sourceBeforeFix": "endFunction(\"get_column_statistics_by_partition: \",statsObj != null)", "sourceAfterFix": "endFunction(\"get_column_statistics_by_partition: \",statsObj != null,null)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "364ae9baf3824dc4e0e56b2b681f8302731e8cdd", "fixCommitParentSHA1": "6b386d4b81f63bc3afd05888529550572da04c06", "bugFilePath": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "fixPatch": "diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\nindex 6c2aeb4..bad5ea2 100644\n--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n@@ -3104,7 +3104,7 @@\n       try {\n         statsObj = getMS().getTableColumnStatistics(dbName, tableName, colName);\n       } finally {\n-        endFunction(\"get_column_statistics_by_table: \", statsObj != null);\n+        endFunction(\"get_column_statistics_by_table: \", statsObj != null, null);\n       }\n       return statsObj;\n     }\n@@ -3126,7 +3126,7 @@\n         statsObj = getMS().getPartitionColumnStatistics(dbName, tableName, convertedPartName,\n                                                             partVals, colName);\n       } finally {\n-        endFunction(\"get_column_statistics_by_partition: \", statsObj != null);\n+        endFunction(\"get_column_statistics_by_partition: \", statsObj != null, null);\n       }\n       return statsObj;\n    }\n@@ -3165,7 +3165,7 @@\n         ret = getMS().updateTableColumnStatistics(colStats);\n         return ret;\n       } finally {\n-        endFunction(\"write_column_statistics: \", ret != false);\n+        endFunction(\"write_column_statistics: \", ret != false, null);\n       }\n     }\n \n@@ -3211,7 +3211,7 @@\n         ret = getMS().updatePartitionColumnStatistics(colStats, partVals);\n         return ret;\n       } finally {\n-        endFunction(\"write_partition_column_statistics: \", ret != false);\n+        endFunction(\"write_partition_column_statistics: \", ret != false, null);\n       }\n     }\n \n@@ -3234,7 +3234,7 @@\n         ret = getMS().deletePartitionColumnStatistics(dbName, tableName,\n                                                       convertedPartName, partVals, colName);\n       } finally {\n-        endFunction(\"delete_column_statistics_by_partition: \", ret != false);\n+        endFunction(\"delete_column_statistics_by_partition: \", ret != false, null);\n       }\n       return ret;\n     }\n@@ -3256,7 +3256,7 @@\n       try {\n         ret = getMS().deleteTableColumnStatistics(dbName, tableName, colName);\n       } finally {\n-        endFunction(\"delete_column_statistics_by_table: \", ret != false);\n+        endFunction(\"delete_column_statistics_by_table: \", ret != false, null);\n       }\n       return ret;\n    }\n", "projectName": "apache.hive", "bugLineNum": 3168, "bugNodeStartChar": 113450, "bugNodeLength": 54, "fixLineNum": 3168, "fixNodeStartChar": 113450, "fixNodeLength": 60, "sourceBeforeFix": "endFunction(\"write_column_statistics: \",ret != false)", "sourceAfterFix": "endFunction(\"write_column_statistics: \",ret != false,null)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "364ae9baf3824dc4e0e56b2b681f8302731e8cdd", "fixCommitParentSHA1": "6b386d4b81f63bc3afd05888529550572da04c06", "bugFilePath": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "fixPatch": "diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\nindex 6c2aeb4..bad5ea2 100644\n--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n@@ -3104,7 +3104,7 @@\n       try {\n         statsObj = getMS().getTableColumnStatistics(dbName, tableName, colName);\n       } finally {\n-        endFunction(\"get_column_statistics_by_table: \", statsObj != null);\n+        endFunction(\"get_column_statistics_by_table: \", statsObj != null, null);\n       }\n       return statsObj;\n     }\n@@ -3126,7 +3126,7 @@\n         statsObj = getMS().getPartitionColumnStatistics(dbName, tableName, convertedPartName,\n                                                             partVals, colName);\n       } finally {\n-        endFunction(\"get_column_statistics_by_partition: \", statsObj != null);\n+        endFunction(\"get_column_statistics_by_partition: \", statsObj != null, null);\n       }\n       return statsObj;\n    }\n@@ -3165,7 +3165,7 @@\n         ret = getMS().updateTableColumnStatistics(colStats);\n         return ret;\n       } finally {\n-        endFunction(\"write_column_statistics: \", ret != false);\n+        endFunction(\"write_column_statistics: \", ret != false, null);\n       }\n     }\n \n@@ -3211,7 +3211,7 @@\n         ret = getMS().updatePartitionColumnStatistics(colStats, partVals);\n         return ret;\n       } finally {\n-        endFunction(\"write_partition_column_statistics: \", ret != false);\n+        endFunction(\"write_partition_column_statistics: \", ret != false, null);\n       }\n     }\n \n@@ -3234,7 +3234,7 @@\n         ret = getMS().deletePartitionColumnStatistics(dbName, tableName,\n                                                       convertedPartName, partVals, colName);\n       } finally {\n-        endFunction(\"delete_column_statistics_by_partition: \", ret != false);\n+        endFunction(\"delete_column_statistics_by_partition: \", ret != false, null);\n       }\n       return ret;\n     }\n@@ -3256,7 +3256,7 @@\n       try {\n         ret = getMS().deleteTableColumnStatistics(dbName, tableName, colName);\n       } finally {\n-        endFunction(\"delete_column_statistics_by_table: \", ret != false);\n+        endFunction(\"delete_column_statistics_by_table: \", ret != false, null);\n       }\n       return ret;\n    }\n", "projectName": "apache.hive", "bugLineNum": 3214, "bugNodeStartChar": 115014, "bugNodeLength": 64, "fixLineNum": 3214, "fixNodeStartChar": 115014, "fixNodeLength": 70, "sourceBeforeFix": "endFunction(\"write_partition_column_statistics: \",ret != false)", "sourceAfterFix": "endFunction(\"write_partition_column_statistics: \",ret != false,null)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "364ae9baf3824dc4e0e56b2b681f8302731e8cdd", "fixCommitParentSHA1": "6b386d4b81f63bc3afd05888529550572da04c06", "bugFilePath": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "fixPatch": "diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\nindex 6c2aeb4..bad5ea2 100644\n--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n@@ -3104,7 +3104,7 @@\n       try {\n         statsObj = getMS().getTableColumnStatistics(dbName, tableName, colName);\n       } finally {\n-        endFunction(\"get_column_statistics_by_table: \", statsObj != null);\n+        endFunction(\"get_column_statistics_by_table: \", statsObj != null, null);\n       }\n       return statsObj;\n     }\n@@ -3126,7 +3126,7 @@\n         statsObj = getMS().getPartitionColumnStatistics(dbName, tableName, convertedPartName,\n                                                             partVals, colName);\n       } finally {\n-        endFunction(\"get_column_statistics_by_partition: \", statsObj != null);\n+        endFunction(\"get_column_statistics_by_partition: \", statsObj != null, null);\n       }\n       return statsObj;\n    }\n@@ -3165,7 +3165,7 @@\n         ret = getMS().updateTableColumnStatistics(colStats);\n         return ret;\n       } finally {\n-        endFunction(\"write_column_statistics: \", ret != false);\n+        endFunction(\"write_column_statistics: \", ret != false, null);\n       }\n     }\n \n@@ -3211,7 +3211,7 @@\n         ret = getMS().updatePartitionColumnStatistics(colStats, partVals);\n         return ret;\n       } finally {\n-        endFunction(\"write_partition_column_statistics: \", ret != false);\n+        endFunction(\"write_partition_column_statistics: \", ret != false, null);\n       }\n     }\n \n@@ -3234,7 +3234,7 @@\n         ret = getMS().deletePartitionColumnStatistics(dbName, tableName,\n                                                       convertedPartName, partVals, colName);\n       } finally {\n-        endFunction(\"delete_column_statistics_by_partition: \", ret != false);\n+        endFunction(\"delete_column_statistics_by_partition: \", ret != false, null);\n       }\n       return ret;\n     }\n@@ -3256,7 +3256,7 @@\n       try {\n         ret = getMS().deleteTableColumnStatistics(dbName, tableName, colName);\n       } finally {\n-        endFunction(\"delete_column_statistics_by_table: \", ret != false);\n+        endFunction(\"delete_column_statistics_by_table: \", ret != false, null);\n       }\n       return ret;\n    }\n", "projectName": "apache.hive", "bugLineNum": 3237, "bugNodeStartChar": 116074, "bugNodeLength": 68, "fixLineNum": 3237, "fixNodeStartChar": 116074, "fixNodeLength": 74, "sourceBeforeFix": "endFunction(\"delete_column_statistics_by_partition: \",ret != false)", "sourceAfterFix": "endFunction(\"delete_column_statistics_by_partition: \",ret != false,null)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "364ae9baf3824dc4e0e56b2b681f8302731e8cdd", "fixCommitParentSHA1": "6b386d4b81f63bc3afd05888529550572da04c06", "bugFilePath": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java", "fixPatch": "diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\nindex 6c2aeb4..bad5ea2 100644\n--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java\n@@ -3104,7 +3104,7 @@\n       try {\n         statsObj = getMS().getTableColumnStatistics(dbName, tableName, colName);\n       } finally {\n-        endFunction(\"get_column_statistics_by_table: \", statsObj != null);\n+        endFunction(\"get_column_statistics_by_table: \", statsObj != null, null);\n       }\n       return statsObj;\n     }\n@@ -3126,7 +3126,7 @@\n         statsObj = getMS().getPartitionColumnStatistics(dbName, tableName, convertedPartName,\n                                                             partVals, colName);\n       } finally {\n-        endFunction(\"get_column_statistics_by_partition: \", statsObj != null);\n+        endFunction(\"get_column_statistics_by_partition: \", statsObj != null, null);\n       }\n       return statsObj;\n    }\n@@ -3165,7 +3165,7 @@\n         ret = getMS().updateTableColumnStatistics(colStats);\n         return ret;\n       } finally {\n-        endFunction(\"write_column_statistics: \", ret != false);\n+        endFunction(\"write_column_statistics: \", ret != false, null);\n       }\n     }\n \n@@ -3211,7 +3211,7 @@\n         ret = getMS().updatePartitionColumnStatistics(colStats, partVals);\n         return ret;\n       } finally {\n-        endFunction(\"write_partition_column_statistics: \", ret != false);\n+        endFunction(\"write_partition_column_statistics: \", ret != false, null);\n       }\n     }\n \n@@ -3234,7 +3234,7 @@\n         ret = getMS().deletePartitionColumnStatistics(dbName, tableName,\n                                                       convertedPartName, partVals, colName);\n       } finally {\n-        endFunction(\"delete_column_statistics_by_partition: \", ret != false);\n+        endFunction(\"delete_column_statistics_by_partition: \", ret != false, null);\n       }\n       return ret;\n     }\n@@ -3256,7 +3256,7 @@\n       try {\n         ret = getMS().deleteTableColumnStatistics(dbName, tableName, colName);\n       } finally {\n-        endFunction(\"delete_column_statistics_by_table: \", ret != false);\n+        endFunction(\"delete_column_statistics_by_table: \", ret != false, null);\n       }\n       return ret;\n    }\n", "projectName": "apache.hive", "bugLineNum": 3259, "bugNodeStartChar": 116839, "bugNodeLength": 64, "fixLineNum": 3259, "fixNodeStartChar": 116839, "fixNodeLength": 70, "sourceBeforeFix": "endFunction(\"delete_column_statistics_by_table: \",ret != false)", "sourceAfterFix": "endFunction(\"delete_column_statistics_by_table: \",ret != false,null)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "81f85b9930f3b49433a04a0ffa6b1e3113eb8d40", "fixCommitParentSHA1": "1a78fcc9476c797673d3b3f92b794dc8e022541d", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\nindex 0e7e82b..876c28c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n@@ -3081,7 +3081,7 @@\n     // them\n     for (String destination : dests) {\n \n-      getReduceValuesForReduceSinkNoMapAgg(parseInfo, dest, reduceSinkInputRowResolver,\n+      getReduceValuesForReduceSinkNoMapAgg(parseInfo, destination, reduceSinkInputRowResolver,\n           reduceSinkOutputRowResolver, outputValueColumnNames, reduceValues);\n \n       // Need to pass all of the columns used in the where clauses as reduce values\n", "projectName": "apache.hive", "bugLineNum": 3084, "bugNodeStartChar": 124914, "bugNodeLength": 158, "fixLineNum": 3084, "fixNodeStartChar": 124914, "fixNodeLength": 165, "sourceBeforeFix": "getReduceValuesForReduceSinkNoMapAgg(parseInfo,dest,reduceSinkInputRowResolver,reduceSinkOutputRowResolver,outputValueColumnNames,reduceValues)", "sourceAfterFix": "getReduceValuesForReduceSinkNoMapAgg(parseInfo,destination,reduceSinkInputRowResolver,reduceSinkOutputRowResolver,outputValueColumnNames,reduceValues)"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "dac442889f82a7f84ee1e74746027845c263b8ea", "fixCommitParentSHA1": "ca5e0ed8edd780a0e54c43e67f7222fcd48aef0c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java\nindex 6660a91..c80403d 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java\n@@ -1166,12 +1166,12 @@\n           bucketCols = part.getBucketCols();\n           inputFormatClass = part.getInputFormatClass();\n           isArchived = ArchiveUtils.isArchived(part);\n-          tblPartLoc = part.getDataLocation().toString();\n+          tblPartLoc = part.getPartitionPath().toString();\n         }\n       } else {\n         inputFormatClass = tblObj.getInputFormatClass();\n         bucketCols = tblObj.getBucketCols();\n-        tblPartLoc = tblObj.getDataLocation().toString();\n+        tblPartLoc = tblObj.getPath().toString();\n       }\n \n       // throw a HiveException for non-rcfile.\n", "projectName": "apache.hive", "bugLineNum": 1169, "bugNodeStartChar": 45523, "bugNodeLength": 22, "fixLineNum": 1169, "fixNodeStartChar": 45523, "fixNodeLength": 23, "sourceBeforeFix": "part.getDataLocation()", "sourceAfterFix": "part.getPartitionPath()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "dac442889f82a7f84ee1e74746027845c263b8ea", "fixCommitParentSHA1": "ca5e0ed8edd780a0e54c43e67f7222fcd48aef0c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java\nindex 6660a91..c80403d 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java\n@@ -1166,12 +1166,12 @@\n           bucketCols = part.getBucketCols();\n           inputFormatClass = part.getInputFormatClass();\n           isArchived = ArchiveUtils.isArchived(part);\n-          tblPartLoc = part.getDataLocation().toString();\n+          tblPartLoc = part.getPartitionPath().toString();\n         }\n       } else {\n         inputFormatClass = tblObj.getInputFormatClass();\n         bucketCols = tblObj.getBucketCols();\n-        tblPartLoc = tblObj.getDataLocation().toString();\n+        tblPartLoc = tblObj.getPath().toString();\n       }\n \n       // throw a HiveException for non-rcfile.\n", "projectName": "apache.hive", "bugLineNum": 1169, "bugNodeStartChar": 45523, "bugNodeLength": 22, "fixLineNum": 1169, "fixNodeStartChar": 45523, "fixNodeLength": 23, "sourceBeforeFix": "part.getDataLocation()", "sourceAfterFix": "part.getPartitionPath()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "dac442889f82a7f84ee1e74746027845c263b8ea", "fixCommitParentSHA1": "ca5e0ed8edd780a0e54c43e67f7222fcd48aef0c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java\nindex 6660a91..c80403d 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java\n@@ -1166,12 +1166,12 @@\n           bucketCols = part.getBucketCols();\n           inputFormatClass = part.getInputFormatClass();\n           isArchived = ArchiveUtils.isArchived(part);\n-          tblPartLoc = part.getDataLocation().toString();\n+          tblPartLoc = part.getPartitionPath().toString();\n         }\n       } else {\n         inputFormatClass = tblObj.getInputFormatClass();\n         bucketCols = tblObj.getBucketCols();\n-        tblPartLoc = tblObj.getDataLocation().toString();\n+        tblPartLoc = tblObj.getPath().toString();\n       }\n \n       // throw a HiveException for non-rcfile.\n", "projectName": "apache.hive", "bugLineNum": 1174, "bugNodeStartChar": 45706, "bugNodeLength": 24, "fixLineNum": 1174, "fixNodeStartChar": 45706, "fixNodeLength": 16, "sourceBeforeFix": "tblObj.getDataLocation()", "sourceAfterFix": "tblObj.getPath()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "dac442889f82a7f84ee1e74746027845c263b8ea", "fixCommitParentSHA1": "ca5e0ed8edd780a0e54c43e67f7222fcd48aef0c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java\nindex 6660a91..c80403d 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java\n@@ -1166,12 +1166,12 @@\n           bucketCols = part.getBucketCols();\n           inputFormatClass = part.getInputFormatClass();\n           isArchived = ArchiveUtils.isArchived(part);\n-          tblPartLoc = part.getDataLocation().toString();\n+          tblPartLoc = part.getPartitionPath().toString();\n         }\n       } else {\n         inputFormatClass = tblObj.getInputFormatClass();\n         bucketCols = tblObj.getBucketCols();\n-        tblPartLoc = tblObj.getDataLocation().toString();\n+        tblPartLoc = tblObj.getPath().toString();\n       }\n \n       // throw a HiveException for non-rcfile.\n", "projectName": "apache.hive", "bugLineNum": 1174, "bugNodeStartChar": 45706, "bugNodeLength": 24, "fixLineNum": 1174, "fixNodeStartChar": 45706, "fixNodeLength": 16, "sourceBeforeFix": "tblObj.getDataLocation()", "sourceAfterFix": "tblObj.getPath()"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "0649166c8fbaf1615c77f8ef37bbb3e385d6a83e", "fixCommitParentSHA1": "41206e4d3f687599d125a6fb7c80547a39f85152", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java\nindex 3001575..5a8118c 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java\n@@ -309,7 +309,8 @@\n               Warehouse.makePartPath(addPartitionDesc.getPartSpec()));\n         }\n       } else {\n-        tgtPath = new Path(tblDesc.getLocation());\n+        tgtPath = new Path(tblDesc.getLocation(),\n+            Warehouse.makePartPath(addPartitionDesc.getPartSpec()));\n       }\n       checkTargetLocationEmpty(fs, tgtPath);\n       addPartitionDesc.setLocation(tgtPath.toString());\n", "projectName": "apache.hive", "bugLineNum": 312, "bugNodeStartChar": 13972, "bugNodeLength": 31, "fixLineNum": 312, "fixNodeStartChar": 13972, "fixNodeLength": 99, "sourceBeforeFix": "new Path(tblDesc.getLocation())", "sourceAfterFix": "new Path(tblDesc.getLocation(),Warehouse.makePartPath(addPartitionDesc.getPartSpec()))"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "37662e89fb569a997ce97d40b1e7d13cf2acdc9c", "fixCommitParentSHA1": "32b046bf93e3d041b2bb07a1c9b4e1ef5d977ddf", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java\nindex b83c343..e791d49 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/merge/BlockMergeTask.java\n@@ -212,7 +212,7 @@\n           }\n           HadoopJobExecHelper.runningJobKillURIs.remove(rj.getJobID());\n         }\n-        RCFileMergeMapper.jobClose(outputPath, noName, job, console);\n+        RCFileMergeMapper.jobClose(outputPath, success, job, console);\n       } catch (Exception e) {\n       }\n     }\n", "projectName": "apache.hive", "bugLineNum": 215, "bugNodeStartChar": 7340, "bugNodeLength": 60, "fixLineNum": 215, "fixNodeStartChar": 7340, "fixNodeLength": 61, "sourceBeforeFix": "RCFileMergeMapper.jobClose(outputPath,noName,job,console)", "sourceAfterFix": "RCFileMergeMapper.jobClose(outputPath,success,job,console)"}, {"bugType": "CHANGE_CALLER_IN_FUNCTION_CALL", "fixCommitSHA1": "56627bebd6356884336d99009b80f2573101a0c6", "fixCommitParentSHA1": "407812c326776d1e1bfa4c71d8faca3fcbba0525", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java\nindex c08c253..da3b1fe 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java\n@@ -202,7 +202,8 @@\n         }\n \n         if (prunerExpr == null) {\n-          // add all partitions corresponding to the table\n+          // This can happen when hive.mapred.mode=nonstrict and there is no predicates at all\n+          // Add all partitions to the unknown_parts so that a MR job is generated.\n           true_parts.addAll(Hive.get().getPartitions(tab));\n         } else {\n           // remove non-partition columns\n@@ -213,7 +214,9 @@\n               \"; filter w/o compacting: \" +\n               ((prunerExpr != null) ? prunerExpr.getExprString(): \"null\"));\n           if (compactExpr == null) {\n-            true_parts.addAll(Hive.get().getPartitions(tab));\n+            // This could happen when hive.mapred.mode=nonstrict and all the predicates\n+            // are on non-partition columns.\n+            unkn_parts.addAll(Hive.get().getPartitions(tab));\n           } else if (Utilities.checkJDOPushDown(tab, compactExpr)) {\n             String filter = compactExpr.getExprString();\n             String oldFilter = prunerExpr.getExprString();\n", "projectName": "apache.hive", "bugLineNum": 216, "bugNodeStartChar": 8263, "bugNodeLength": 48, "fixLineNum": 218, "fixNodeStartChar": 8396, "fixNodeLength": 48, "sourceBeforeFix": "true_parts.addAll(Hive.get().getPartitions(tab))", "sourceAfterFix": "unkn_parts.addAll(Hive.get().getPartitions(tab))"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "56627bebd6356884336d99009b80f2573101a0c6", "fixCommitParentSHA1": "407812c326776d1e1bfa4c71d8faca3fcbba0525", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java\nindex c08c253..da3b1fe 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ppr/PartitionPruner.java\n@@ -202,7 +202,8 @@\n         }\n \n         if (prunerExpr == null) {\n-          // add all partitions corresponding to the table\n+          // This can happen when hive.mapred.mode=nonstrict and there is no predicates at all\n+          // Add all partitions to the unknown_parts so that a MR job is generated.\n           true_parts.addAll(Hive.get().getPartitions(tab));\n         } else {\n           // remove non-partition columns\n@@ -213,7 +214,9 @@\n               \"; filter w/o compacting: \" +\n               ((prunerExpr != null) ? prunerExpr.getExprString(): \"null\"));\n           if (compactExpr == null) {\n-            true_parts.addAll(Hive.get().getPartitions(tab));\n+            // This could happen when hive.mapred.mode=nonstrict and all the predicates\n+            // are on non-partition columns.\n+            unkn_parts.addAll(Hive.get().getPartitions(tab));\n           } else if (Utilities.checkJDOPushDown(tab, compactExpr)) {\n             String filter = compactExpr.getExprString();\n             String oldFilter = prunerExpr.getExprString();\n", "projectName": "apache.hive", "bugLineNum": 216, "bugNodeStartChar": 8263, "bugNodeLength": 48, "fixLineNum": 218, "fixNodeStartChar": 8396, "fixNodeLength": 48, "sourceBeforeFix": "true_parts.addAll(Hive.get().getPartitions(tab))", "sourceAfterFix": "unkn_parts.addAll(Hive.get().getPartitions(tab))"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "4148680194f015c7b4639d7db892b166f86a7fee", "fixCommitParentSHA1": "b9c76cf167432f705cb8700726bc9f481c110c44", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java\nindex 29fa753..df00530 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java\n@@ -186,7 +186,7 @@\n         TableDesc tt_desc;\n         Operator<? extends Serializable> rootOp;\n \n-        if (mjCtx.getOldMapJoin() == null) {\n+        if (mjCtx.getOldMapJoin() == null || setReducer) {\n           taskTmpDir = mjCtx.getTaskTmpDir();\n           tt_desc = mjCtx.getTTDesc();\n           rootOp = mjCtx.getRootMapJoinOp();\n", "projectName": "apache.hive", "bugLineNum": 189, "bugNodeStartChar": 7817, "bugNodeLength": 29, "fixLineNum": 189, "fixNodeStartChar": 7817, "fixNodeLength": 43, "sourceBeforeFix": "mjCtx.getOldMapJoin() == null", "sourceAfterFix": "mjCtx.getOldMapJoin() == null || setReducer"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "4148680194f015c7b4639d7db892b166f86a7fee", "fixCommitParentSHA1": "b9c76cf167432f705cb8700726bc9f481c110c44", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java\nindex 29fa753..df00530 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java\n@@ -186,7 +186,7 @@\n         TableDesc tt_desc;\n         Operator<? extends Serializable> rootOp;\n \n-        if (mjCtx.getOldMapJoin() == null) {\n+        if (mjCtx.getOldMapJoin() == null || setReducer) {\n           taskTmpDir = mjCtx.getTaskTmpDir();\n           tt_desc = mjCtx.getTTDesc();\n           rootOp = mjCtx.getRootMapJoinOp();\n", "projectName": "apache.hive", "bugLineNum": 189, "bugNodeStartChar": 7817, "bugNodeLength": 29, "fixLineNum": 189, "fixNodeStartChar": 7817, "fixNodeLength": 43, "sourceBeforeFix": "mjCtx.getOldMapJoin() == null", "sourceAfterFix": "mjCtx.getOldMapJoin() == null || setReducer"}, {"bugType": "CHANGE_UNARY_OPERATOR", "fixCommitSHA1": "ce4cb40fd95ffc635374e074be1f9f8650dd57d4", "fixCommitParentSHA1": "35969c8182ddb0b2c0b1270e02f3bc989a4d5137", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java b/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java\nindex 7a1d1e1..e9483eb 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/CombineHiveInputFormat.java\n@@ -252,7 +252,7 @@\n \n       // Since there is no easy way of knowing whether MAPREDUCE-1597 is present in the tree or not,\n       // we use a configuration variable for the same\n-      if (this.mrwork != null && this.mrwork.getHadoopSupportsSplittable()) {\n+      if (this.mrwork != null && !this.mrwork.getHadoopSupportsSplittable()) {\n         // The following code should be removed, once\n         // https://issues.apache.org/jira/browse/MAPREDUCE-1597 is fixed.\n         // Hadoop does not handle non-splittable files correctly for CombineFileInputFormat,\n", "projectName": "apache.hive", "bugLineNum": 255, "bugNodeStartChar": 8739, "bugNodeLength": 41, "fixLineNum": 255, "fixNodeStartChar": 8739, "fixNodeLength": 42, "sourceBeforeFix": "this.mrwork.getHadoopSupportsSplittable()", "sourceAfterFix": "!this.mrwork.getHadoopSupportsSplittable()"}, {"bugType": "MORE_SPECIFIC_IF", "fixCommitSHA1": "0357cee3c69cd265d295a0db716ae57fb1107ea4", "fixCommitParentSHA1": "4ed527fd142d53875101afbf5d4df7f161ec6178", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\nindex a32bfd5..6bb2c31 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n@@ -1528,7 +1528,9 @@\n       boolean hasAsClause = (!isInTransform) && (child.getChildCount() == 2);\n \n       // EXPR AS (ALIAS,...) parses, but is only allowed for UDTF's\n-      if (!isUDTF && child.getChildCount() > 2) {\n+      // This check is not needed and invalid when there is a transform b/c the \n+      // AST's are slightly different.\n+      if (!isInTransform && !isUDTF && child.getChildCount() > 2) {\n         throw new SemanticException(ErrorMsg.INVALID_AS.getMsg());\n       }\n \n", "projectName": "apache.hive", "bugLineNum": 1531, "bugNodeStartChar": 60417, "bugNodeLength": 36, "fixLineNum": 1533, "fixNodeStartChar": 60537, "fixNodeLength": 54, "sourceBeforeFix": "!isUDTF && child.getChildCount() > 2", "sourceAfterFix": "!isInTransform && !isUDTF && child.getChildCount() > 2"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "1c7978dbaffd87bc6dec9d0c902a69a5002c9791", "fixCommitParentSHA1": "311557f57188a8cc712f00f76eb26d0742fc37d2", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/Context.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Context.java b/ql/src/java/org/apache/hadoop/hive/ql/Context.java\nindex febdb93..1103b2a 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/Context.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/Context.java\n@@ -206,14 +206,14 @@\n     if (explain) {\n       try {\n         if (localScratchDir != null)\n-          FileSystem.getLocal(conf).delete(localScratchDir);\n+          FileSystem.getLocal(conf).delete(localScratchDir, true);\n       } catch (Exception e) {\n         LOG.warn(\"Error Removing Scratch: \" + StringUtils.stringifyException(e));\n       }\n     } else {\n       for (Path p: allScratchDirs) {\n         try {\n-          p.getFileSystem(conf).delete(p);\n+          p.getFileSystem(conf).delete(p, true);\n         } catch (Exception e) {\n           LOG.warn(\"Error Removing Scratch: \" + StringUtils.stringifyException(e));\n         }\n", "projectName": "apache.hive", "bugLineNum": 209, "bugNodeStartChar": 6286, "bugNodeLength": 49, "fixLineNum": 209, "fixNodeStartChar": 6286, "fixNodeLength": 55, "sourceBeforeFix": "FileSystem.getLocal(conf).delete(localScratchDir)", "sourceAfterFix": "FileSystem.getLocal(conf).delete(localScratchDir,true)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "1c7978dbaffd87bc6dec9d0c902a69a5002c9791", "fixCommitParentSHA1": "311557f57188a8cc712f00f76eb26d0742fc37d2", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/Context.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Context.java b/ql/src/java/org/apache/hadoop/hive/ql/Context.java\nindex febdb93..1103b2a 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/Context.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/Context.java\n@@ -206,14 +206,14 @@\n     if (explain) {\n       try {\n         if (localScratchDir != null)\n-          FileSystem.getLocal(conf).delete(localScratchDir);\n+          FileSystem.getLocal(conf).delete(localScratchDir, true);\n       } catch (Exception e) {\n         LOG.warn(\"Error Removing Scratch: \" + StringUtils.stringifyException(e));\n       }\n     } else {\n       for (Path p: allScratchDirs) {\n         try {\n-          p.getFileSystem(conf).delete(p);\n+          p.getFileSystem(conf).delete(p, true);\n         } catch (Exception e) {\n           LOG.warn(\"Error Removing Scratch: \" + StringUtils.stringifyException(e));\n         }\n", "projectName": "apache.hive", "bugLineNum": 216, "bugNodeStartChar": 6531, "bugNodeLength": 31, "fixLineNum": 216, "fixNodeStartChar": 6531, "fixNodeLength": 37, "sourceBeforeFix": "p.getFileSystem(conf).delete(p)", "sourceAfterFix": "p.getFileSystem(conf).delete(p,true)"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "ddfab2213db771b738d1f971e36e664f5a02f4ab", "fixCommitParentSHA1": "db527813d12741b68e8f7c311e7c7f411c3ab1c9", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java\nindex 80a57fa..6443b2a 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java\n@@ -351,7 +351,7 @@\n     ExecDriverTaskHandle th = (ExecDriverTaskHandle)t;\n     RunningJob rj = th.getRunningJob();\n     this.mapProgress = Math.round(rj.mapProgress() * 100);\n-    this.reduceProgress = Math.round(rj.mapProgress() * 100);\n+    this.reduceProgress = Math.round(rj.reduceProgress() * 100);\n     taskCounters.put(\"CNTR_NAME_\" + getId() + \"_MAP_PROGRESS\", Long.valueOf(this.mapProgress));\n     taskCounters.put(\"CNTR_NAME_\" + getId() + \"_REDUCE_PROGRESS\", Long.valueOf(this.reduceProgress));\n     Counters ctrs = th.getCounters();\n", "projectName": "apache.hive", "bugLineNum": 354, "bugNodeStartChar": 13234, "bugNodeLength": 16, "fixLineNum": 354, "fixNodeStartChar": 13234, "fixNodeLength": 19, "sourceBeforeFix": "rj.mapProgress()", "sourceAfterFix": "rj.reduceProgress()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "ddfab2213db771b738d1f971e36e664f5a02f4ab", "fixCommitParentSHA1": "db527813d12741b68e8f7c311e7c7f411c3ab1c9", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java\nindex 80a57fa..6443b2a 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java\n@@ -351,7 +351,7 @@\n     ExecDriverTaskHandle th = (ExecDriverTaskHandle)t;\n     RunningJob rj = th.getRunningJob();\n     this.mapProgress = Math.round(rj.mapProgress() * 100);\n-    this.reduceProgress = Math.round(rj.mapProgress() * 100);\n+    this.reduceProgress = Math.round(rj.reduceProgress() * 100);\n     taskCounters.put(\"CNTR_NAME_\" + getId() + \"_MAP_PROGRESS\", Long.valueOf(this.mapProgress));\n     taskCounters.put(\"CNTR_NAME_\" + getId() + \"_REDUCE_PROGRESS\", Long.valueOf(this.reduceProgress));\n     Counters ctrs = th.getCounters();\n", "projectName": "apache.hive", "bugLineNum": 354, "bugNodeStartChar": 13234, "bugNodeLength": 16, "fixLineNum": 354, "fixNodeStartChar": 13234, "fixNodeLength": 19, "sourceBeforeFix": "rj.mapProgress()", "sourceAfterFix": "rj.reduceProgress()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "d1cd1f5dd0dd38585b9fb2fe3af369127870583d", "fixCommitParentSHA1": "6d803f5f1136227adb340b75ddfadca2152185f9", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLower.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLower.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLower.java\nindex 36b0889..7443cc8 100755\n--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLower.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLower.java\n@@ -34,7 +34,7 @@\n     if (s == null) {\n       return null;\n     }\n-    t.set(s.toString().toUpperCase());\n+    t.set(s.toString().toLowerCase());\n     return t;\n   }\n \n", "projectName": "apache.hive", "bugLineNum": 37, "bugNodeStartChar": 1167, "bugNodeLength": 26, "fixLineNum": 37, "fixNodeStartChar": 1167, "fixNodeLength": 26, "sourceBeforeFix": "s.toString().toUpperCase()", "sourceAfterFix": "s.toString().toLowerCase()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "d1cd1f5dd0dd38585b9fb2fe3af369127870583d", "fixCommitParentSHA1": "6d803f5f1136227adb340b75ddfadca2152185f9", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLower.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLower.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLower.java\nindex 36b0889..7443cc8 100755\n--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLower.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFLower.java\n@@ -34,7 +34,7 @@\n     if (s == null) {\n       return null;\n     }\n-    t.set(s.toString().toUpperCase());\n+    t.set(s.toString().toLowerCase());\n     return t;\n   }\n \n", "projectName": "apache.hive", "bugLineNum": 37, "bugNodeStartChar": 1167, "bugNodeLength": 26, "fixLineNum": 37, "fixNodeStartChar": 1167, "fixNodeLength": 26, "sourceBeforeFix": "s.toString().toUpperCase()", "sourceAfterFix": "s.toString().toLowerCase()"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "5bcda0dcda632007620256bcc654628a81ab50f0", "fixCommitParentSHA1": "eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExp.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExp.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExp.java\nindex 38360f5..516e477 100755\n--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExp.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExp.java\n@@ -38,7 +38,7 @@\n     if (s == null || regex == null) {\n       return null;\n     }\n-    if (!regex.equals(lastRegex)) {\n+    if (!regex.equals(lastRegex) || p == null) {\n       lastRegex.set(regex);\n       p = Pattern.compile(regex.toString());\n     }\n", "projectName": "apache.hive", "bugLineNum": 41, "bugNodeStartChar": 1347, "bugNodeLength": 24, "fixLineNum": 41, "fixNodeStartChar": 1347, "fixNodeLength": 37, "sourceBeforeFix": "!regex.equals(lastRegex)", "sourceAfterFix": "!regex.equals(lastRegex) || p == null"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "5bcda0dcda632007620256bcc654628a81ab50f0", "fixCommitParentSHA1": "eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpExtract.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpExtract.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpExtract.java\nindex 8bdf8be..cec996d 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpExtract.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpExtract.java\n@@ -43,7 +43,7 @@\n     if (s == null || regex == null) {\n       return null;\n     }\n-    if (!regex.equals(lastRegex)) {\n+    if (!regex.equals(lastRegex) || p == null) {\n       lastRegex = regex;\n       p = Pattern.compile(regex);\n     }\n", "projectName": "apache.hive", "bugLineNum": 46, "bugNodeStartChar": 1672, "bugNodeLength": 24, "fixLineNum": 46, "fixNodeStartChar": 1672, "fixNodeLength": 37, "sourceBeforeFix": "!regex.equals(lastRegex)", "sourceAfterFix": "!regex.equals(lastRegex) || p == null"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "5bcda0dcda632007620256bcc654628a81ab50f0", "fixCommitParentSHA1": "eb3e40c95cba207661acd7f8c5d79cc40625ec9c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpReplace.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpReplace.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpReplace.java\nindex e4ddab1..361f1cb 100755\n--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpReplace.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFRegExpReplace.java\n@@ -30,7 +30,7 @@\n   private Pattern p = null;\n   \n   private Text lastReplacement = new Text();\n-  private String replacementString = null; \n+  private String replacementString = \"\"; \n \n   Text result = new Text();\n   public UDFRegExpReplace() {\n@@ -41,7 +41,7 @@\n       return null;\n     }\n     // If the regex is changed, make sure we compile the regex again.\n-    if (!regex.equals(lastRegex)) {\n+    if (!regex.equals(lastRegex) || p == null) {\n       lastRegex.set(regex);\n       p = Pattern.compile(regex.toString());\n     }\n", "projectName": "apache.hive", "bugLineNum": 44, "bugNodeStartChar": 1486, "bugNodeLength": 24, "fixLineNum": 44, "fixNodeStartChar": 1486, "fixNodeLength": 37, "sourceBeforeFix": "!regex.equals(lastRegex)", "sourceAfterFix": "!regex.equals(lastRegex) || p == null"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "ec5961a27122b228cb8adb634ee183eebcefa98c", "fixCommitParentSHA1": "5fd1cf27b028500451c9b53ce62e24fd7358f7e3", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java\nindex e350fe7..f86941b 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java\n@@ -324,7 +324,7 @@\n     mapredWork work = new mapredWork();\n     work.setPathToAliases(new LinkedHashMap<String, ArrayList<String>>());\n     work.setPathToPartitionInfo(new LinkedHashMap<String, partitionDesc>());\n-    work.setAliasToWork(new HashMap<String, Operator<? extends Serializable>>());\n+    work.setAliasToWork(new LinkedHashMap<String, Operator<? extends Serializable>>());\n     work.setTagToValueDesc(new ArrayList<tableDesc>());\n     work.setReducer(null);\n     return work;\n", "projectName": "apache.hive", "bugLineNum": 327, "bugNodeStartChar": 12895, "bugNodeLength": 49, "fixLineNum": 327, "fixNodeStartChar": 12895, "fixNodeLength": 55, "sourceBeforeFix": "HashMap<String,Operator<? extends Serializable>>", "sourceAfterFix": "LinkedHashMap<String,Operator<? extends Serializable>>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "ec5961a27122b228cb8adb634ee183eebcefa98c", "fixCommitParentSHA1": "5fd1cf27b028500451c9b53ce62e24fd7358f7e3", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java\nindex 7fd7bcd..03bfaaa 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java\n@@ -49,7 +49,7 @@\n     return new mapredWork(\"\", \n                           new LinkedHashMap<String, ArrayList<String>> (),\n                           new LinkedHashMap<String, partitionDesc> (),\n-                          new HashMap<String, Operator<? extends Serializable>> (),\n+                          new LinkedHashMap<String, Operator<? extends Serializable>> (),\n                           new tableDesc(),\n                           new ArrayList<tableDesc> (),\n                           null,\n", "projectName": "apache.hive", "bugLineNum": 52, "bugNodeStartChar": 2203, "bugNodeLength": 49, "fixLineNum": 52, "fixNodeStartChar": 2203, "fixNodeLength": 55, "sourceBeforeFix": "HashMap<String,Operator<? extends Serializable>>", "sourceAfterFix": "LinkedHashMap<String,Operator<? extends Serializable>>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "ec5961a27122b228cb8adb634ee183eebcefa98c", "fixCommitParentSHA1": "5fd1cf27b028500451c9b53ce62e24fd7358f7e3", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java\nindex cd2d8da..2d6a881 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java\n@@ -35,7 +35,7 @@\n   \n   private LinkedHashMap<String,partitionDesc> pathToPartitionInfo;\n   \n-  private HashMap<String,Operator<? extends Serializable>> aliasToWork;\n+  private LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork;\n \n   // map<->reduce interface\n   // schema of the map-reduce 'key' object - this is homogeneous\n@@ -55,7 +55,7 @@\n     final String command,\n     final LinkedHashMap<String,ArrayList<String>> pathToAliases,\n     final LinkedHashMap<String,partitionDesc> pathToPartitionInfo,\n-    final HashMap<String,Operator<? extends Serializable>> aliasToWork,\n+    final LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork,\n     final tableDesc keyDesc,\n     List<tableDesc> tagToValueDesc,\n     final Operator<?> reducer,\n@@ -93,10 +93,10 @@\n   }\n   \n   @explain(displayName=\"Alias -> Map Operator Tree\")\n-  public HashMap<String, Operator<? extends Serializable>> getAliasToWork() {\n+  public LinkedHashMap<String, Operator<? extends Serializable>> getAliasToWork() {\n     return this.aliasToWork;\n   }\n-  public void setAliasToWork(final HashMap<String,Operator<? extends Serializable>> aliasToWork) {\n+  public void setAliasToWork(final LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork) {\n     this.aliasToWork=aliasToWork;\n   }\n   public tableDesc getKeyDesc() {\n", "projectName": "apache.hive", "bugLineNum": 38, "bugNodeStartChar": 1414, "bugNodeLength": 48, "fixLineNum": 38, "fixNodeStartChar": 1414, "fixNodeLength": 54, "sourceBeforeFix": "HashMap<String,Operator<? extends Serializable>>", "sourceAfterFix": "LinkedHashMap<String,Operator<? extends Serializable>>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "ec5961a27122b228cb8adb634ee183eebcefa98c", "fixCommitParentSHA1": "5fd1cf27b028500451c9b53ce62e24fd7358f7e3", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java\nindex cd2d8da..2d6a881 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java\n@@ -35,7 +35,7 @@\n   \n   private LinkedHashMap<String,partitionDesc> pathToPartitionInfo;\n   \n-  private HashMap<String,Operator<? extends Serializable>> aliasToWork;\n+  private LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork;\n \n   // map<->reduce interface\n   // schema of the map-reduce 'key' object - this is homogeneous\n@@ -55,7 +55,7 @@\n     final String command,\n     final LinkedHashMap<String,ArrayList<String>> pathToAliases,\n     final LinkedHashMap<String,partitionDesc> pathToPartitionInfo,\n-    final HashMap<String,Operator<? extends Serializable>> aliasToWork,\n+    final LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork,\n     final tableDesc keyDesc,\n     List<tableDesc> tagToValueDesc,\n     final Operator<?> reducer,\n@@ -93,10 +93,10 @@\n   }\n   \n   @explain(displayName=\"Alias -> Map Operator Tree\")\n-  public HashMap<String, Operator<? extends Serializable>> getAliasToWork() {\n+  public LinkedHashMap<String, Operator<? extends Serializable>> getAliasToWork() {\n     return this.aliasToWork;\n   }\n-  public void setAliasToWork(final HashMap<String,Operator<? extends Serializable>> aliasToWork) {\n+  public void setAliasToWork(final LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork) {\n     this.aliasToWork=aliasToWork;\n   }\n   public tableDesc getKeyDesc() {\n", "projectName": "apache.hive", "bugLineNum": 58, "bugNodeStartChar": 2029, "bugNodeLength": 48, "fixLineNum": 58, "fixNodeStartChar": 2029, "fixNodeLength": 54, "sourceBeforeFix": "HashMap<String,Operator<? extends Serializable>>", "sourceAfterFix": "LinkedHashMap<String,Operator<? extends Serializable>>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "ec5961a27122b228cb8adb634ee183eebcefa98c", "fixCommitParentSHA1": "5fd1cf27b028500451c9b53ce62e24fd7358f7e3", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java\nindex cd2d8da..2d6a881 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java\n@@ -35,7 +35,7 @@\n   \n   private LinkedHashMap<String,partitionDesc> pathToPartitionInfo;\n   \n-  private HashMap<String,Operator<? extends Serializable>> aliasToWork;\n+  private LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork;\n \n   // map<->reduce interface\n   // schema of the map-reduce 'key' object - this is homogeneous\n@@ -55,7 +55,7 @@\n     final String command,\n     final LinkedHashMap<String,ArrayList<String>> pathToAliases,\n     final LinkedHashMap<String,partitionDesc> pathToPartitionInfo,\n-    final HashMap<String,Operator<? extends Serializable>> aliasToWork,\n+    final LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork,\n     final tableDesc keyDesc,\n     List<tableDesc> tagToValueDesc,\n     final Operator<?> reducer,\n@@ -93,10 +93,10 @@\n   }\n   \n   @explain(displayName=\"Alias -> Map Operator Tree\")\n-  public HashMap<String, Operator<? extends Serializable>> getAliasToWork() {\n+  public LinkedHashMap<String, Operator<? extends Serializable>> getAliasToWork() {\n     return this.aliasToWork;\n   }\n-  public void setAliasToWork(final HashMap<String,Operator<? extends Serializable>> aliasToWork) {\n+  public void setAliasToWork(final LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork) {\n     this.aliasToWork=aliasToWork;\n   }\n   public tableDesc getKeyDesc() {\n", "projectName": "apache.hive", "bugLineNum": 96, "bugNodeStartChar": 3371, "bugNodeLength": 49, "fixLineNum": 96, "fixNodeStartChar": 3371, "fixNodeLength": 55, "sourceBeforeFix": "HashMap<String,Operator<? extends Serializable>>", "sourceAfterFix": "LinkedHashMap<String,Operator<? extends Serializable>>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "ec5961a27122b228cb8adb634ee183eebcefa98c", "fixCommitParentSHA1": "5fd1cf27b028500451c9b53ce62e24fd7358f7e3", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java\nindex cd2d8da..2d6a881 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/mapredWork.java\n@@ -35,7 +35,7 @@\n   \n   private LinkedHashMap<String,partitionDesc> pathToPartitionInfo;\n   \n-  private HashMap<String,Operator<? extends Serializable>> aliasToWork;\n+  private LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork;\n \n   // map<->reduce interface\n   // schema of the map-reduce 'key' object - this is homogeneous\n@@ -55,7 +55,7 @@\n     final String command,\n     final LinkedHashMap<String,ArrayList<String>> pathToAliases,\n     final LinkedHashMap<String,partitionDesc> pathToPartitionInfo,\n-    final HashMap<String,Operator<? extends Serializable>> aliasToWork,\n+    final LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork,\n     final tableDesc keyDesc,\n     List<tableDesc> tagToValueDesc,\n     final Operator<?> reducer,\n@@ -93,10 +93,10 @@\n   }\n   \n   @explain(displayName=\"Alias -> Map Operator Tree\")\n-  public HashMap<String, Operator<? extends Serializable>> getAliasToWork() {\n+  public LinkedHashMap<String, Operator<? extends Serializable>> getAliasToWork() {\n     return this.aliasToWork;\n   }\n-  public void setAliasToWork(final HashMap<String,Operator<? extends Serializable>> aliasToWork) {\n+  public void setAliasToWork(final LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork) {\n     this.aliasToWork=aliasToWork;\n   }\n   public tableDesc getKeyDesc() {\n", "projectName": "apache.hive", "bugLineNum": 99, "bugNodeStartChar": 3508, "bugNodeLength": 48, "fixLineNum": 99, "fixNodeStartChar": 3508, "fixNodeLength": 54, "sourceBeforeFix": "HashMap<String,Operator<? extends Serializable>>", "sourceAfterFix": "LinkedHashMap<String,Operator<? extends Serializable>>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "ec5961a27122b228cb8adb634ee183eebcefa98c", "fixCommitParentSHA1": "5fd1cf27b028500451c9b53ce62e24fd7358f7e3", "bugFilePath": "ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java", "fixPatch": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java\nindex 79385dc..44398a5 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java\n@@ -262,7 +262,7 @@\n       cdop1.setConf(cd);\n       CollectOperator cdop2 = (CollectOperator) OperatorFactory.get(collectDesc.class);\n       cdop2.setConf(cd);\n-      HashMap<String,Operator<? extends Serializable>> aliasToWork = new HashMap<String,Operator<? extends Serializable>> ();\n+      LinkedHashMap<String,Operator<? extends Serializable>> aliasToWork = new LinkedHashMap<String,Operator<? extends Serializable>> ();\n       aliasToWork.put(\"a\", cdop1);\n       aliasToWork.put(\"b\", cdop2);\n \n", "projectName": "apache.hive", "bugLineNum": 265, "bugNodeStartChar": 10823, "bugNodeLength": 48, "fixLineNum": 265, "fixNodeStartChar": 10823, "fixNodeLength": 54, "sourceBeforeFix": "HashMap<String,Operator<? extends Serializable>>", "sourceAfterFix": "LinkedHashMap<String,Operator<? extends Serializable>>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "ec5961a27122b228cb8adb634ee183eebcefa98c", "fixCommitParentSHA1": "5fd1cf27b028500451c9b53ce62e24fd7358f7e3", "bugFilePath": "ql/src/test/org/apache/hadoop/hive/ql/exec/TestPlan.java", "fixPatch": "diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestPlan.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestPlan.java\nindex e9f4fde..680e674 100644\n--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestPlan.java\n+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestPlan.java\n@@ -61,7 +61,7 @@\n       LinkedHashMap<String, partitionDesc> pt = new LinkedHashMap<String, partitionDesc> ();\n       pt.put(\"/tmp/testfolder\", partDesc);\n \n-      HashMap<String, Operator<? extends Serializable>> ao = new HashMap<String, Operator<? extends Serializable>> ();\n+      LinkedHashMap<String, Operator<? extends Serializable>> ao = new LinkedHashMap<String, Operator<? extends Serializable>> ();\n       ao.put(\"a\", op);\n \n       mapredWork mrwork = new mapredWork();\n", "projectName": "apache.hive", "bugLineNum": 64, "bugNodeStartChar": 2589, "bugNodeLength": 49, "fixLineNum": 64, "fixNodeStartChar": 2589, "fixNodeLength": 55, "sourceBeforeFix": "HashMap<String,Operator<? extends Serializable>>", "sourceAfterFix": "LinkedHashMap<String,Operator<? extends Serializable>>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "370ead7395fc9194a3dc2d05afc4b3409f6b27da", "fixCommitParentSHA1": "c4dd016566067eb16b97bfaa22f1a8a466afce6c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java\nindex 15c3467..af62c87 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java\n@@ -58,7 +58,7 @@\n   private int outerQueryLimit;\n \n   // used by GroupBy\n-  private HashMap<String, HashMap<String, ASTNode> > destToAggregationExprs;\n+  private LinkedHashMap<String, LinkedHashMap<String, ASTNode> > destToAggregationExprs;\n   private HashMap<String, ASTNode> destToDistinctFuncExpr;\n \n   @SuppressWarnings(\"unused\")\n@@ -76,7 +76,7 @@\n     this.destToSortby = new HashMap<String, ASTNode>();\n     this.destToLimit = new HashMap<String, Integer>();\n     \n-    this.destToAggregationExprs = new HashMap<String, HashMap<String, ASTNode> >();\n+    this.destToAggregationExprs = new LinkedHashMap<String, LinkedHashMap<String, ASTNode> >();\n     this.destToDistinctFuncExpr = new HashMap<String, ASTNode>();\n     \n     this.alias = alias;\n@@ -84,7 +84,7 @@\n     this.outerQueryLimit = -1;\n   }\n \n-  public void setAggregationExprsForClause(String clause, HashMap<String, ASTNode> aggregationTrees) {\n+  public void setAggregationExprsForClause(String clause, LinkedHashMap<String, ASTNode> aggregationTrees) {\n     this.destToAggregationExprs.put(clause, aggregationTrees);\n   }\n \n@@ -273,7 +273,7 @@\n        (!destToClusterby.isEmpty()))\n       return false;\n     \n-    Iterator<Map.Entry<String, HashMap<String, ASTNode>>> aggrIter = destToAggregationExprs.entrySet().iterator();\n+    Iterator<Map.Entry<String, LinkedHashMap<String, ASTNode>>> aggrIter = destToAggregationExprs.entrySet().iterator();\n     while (aggrIter.hasNext()) {\n       HashMap<String, ASTNode> h = aggrIter.next().getValue();\n       if ((h != null) && (!h.isEmpty()))\n", "projectName": "apache.hive", "bugLineNum": 61, "bugNodeStartChar": 2063, "bugNodeLength": 42, "fixLineNum": 61, "fixNodeStartChar": 2063, "fixNodeLength": 54, "sourceBeforeFix": "HashMap<String,HashMap<String,ASTNode>>", "sourceAfterFix": "LinkedHashMap<String,LinkedHashMap<String,ASTNode>>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "370ead7395fc9194a3dc2d05afc4b3409f6b27da", "fixCommitParentSHA1": "c4dd016566067eb16b97bfaa22f1a8a466afce6c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java\nindex 15c3467..af62c87 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java\n@@ -58,7 +58,7 @@\n   private int outerQueryLimit;\n \n   // used by GroupBy\n-  private HashMap<String, HashMap<String, ASTNode> > destToAggregationExprs;\n+  private LinkedHashMap<String, LinkedHashMap<String, ASTNode> > destToAggregationExprs;\n   private HashMap<String, ASTNode> destToDistinctFuncExpr;\n \n   @SuppressWarnings(\"unused\")\n@@ -76,7 +76,7 @@\n     this.destToSortby = new HashMap<String, ASTNode>();\n     this.destToLimit = new HashMap<String, Integer>();\n     \n-    this.destToAggregationExprs = new HashMap<String, HashMap<String, ASTNode> >();\n+    this.destToAggregationExprs = new LinkedHashMap<String, LinkedHashMap<String, ASTNode> >();\n     this.destToDistinctFuncExpr = new HashMap<String, ASTNode>();\n     \n     this.alias = alias;\n@@ -84,7 +84,7 @@\n     this.outerQueryLimit = -1;\n   }\n \n-  public void setAggregationExprsForClause(String clause, HashMap<String, ASTNode> aggregationTrees) {\n+  public void setAggregationExprsForClause(String clause, LinkedHashMap<String, ASTNode> aggregationTrees) {\n     this.destToAggregationExprs.put(clause, aggregationTrees);\n   }\n \n@@ -273,7 +273,7 @@\n        (!destToClusterby.isEmpty()))\n       return false;\n     \n-    Iterator<Map.Entry<String, HashMap<String, ASTNode>>> aggrIter = destToAggregationExprs.entrySet().iterator();\n+    Iterator<Map.Entry<String, LinkedHashMap<String, ASTNode>>> aggrIter = destToAggregationExprs.entrySet().iterator();\n     while (aggrIter.hasNext()) {\n       HashMap<String, ASTNode> h = aggrIter.next().getValue();\n       if ((h != null) && (!h.isEmpty()))\n", "projectName": "apache.hive", "bugLineNum": 79, "bugNodeStartChar": 2973, "bugNodeLength": 42, "fixLineNum": 79, "fixNodeStartChar": 2973, "fixNodeLength": 54, "sourceBeforeFix": "HashMap<String,HashMap<String,ASTNode>>", "sourceAfterFix": "LinkedHashMap<String,LinkedHashMap<String,ASTNode>>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "370ead7395fc9194a3dc2d05afc4b3409f6b27da", "fixCommitParentSHA1": "c4dd016566067eb16b97bfaa22f1a8a466afce6c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java\nindex 15c3467..af62c87 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java\n@@ -58,7 +58,7 @@\n   private int outerQueryLimit;\n \n   // used by GroupBy\n-  private HashMap<String, HashMap<String, ASTNode> > destToAggregationExprs;\n+  private LinkedHashMap<String, LinkedHashMap<String, ASTNode> > destToAggregationExprs;\n   private HashMap<String, ASTNode> destToDistinctFuncExpr;\n \n   @SuppressWarnings(\"unused\")\n@@ -76,7 +76,7 @@\n     this.destToSortby = new HashMap<String, ASTNode>();\n     this.destToLimit = new HashMap<String, Integer>();\n     \n-    this.destToAggregationExprs = new HashMap<String, HashMap<String, ASTNode> >();\n+    this.destToAggregationExprs = new LinkedHashMap<String, LinkedHashMap<String, ASTNode> >();\n     this.destToDistinctFuncExpr = new HashMap<String, ASTNode>();\n     \n     this.alias = alias;\n@@ -84,7 +84,7 @@\n     this.outerQueryLimit = -1;\n   }\n \n-  public void setAggregationExprsForClause(String clause, HashMap<String, ASTNode> aggregationTrees) {\n+  public void setAggregationExprsForClause(String clause, LinkedHashMap<String, ASTNode> aggregationTrees) {\n     this.destToAggregationExprs.put(clause, aggregationTrees);\n   }\n \n@@ -273,7 +273,7 @@\n        (!destToClusterby.isEmpty()))\n       return false;\n     \n-    Iterator<Map.Entry<String, HashMap<String, ASTNode>>> aggrIter = destToAggregationExprs.entrySet().iterator();\n+    Iterator<Map.Entry<String, LinkedHashMap<String, ASTNode>>> aggrIter = destToAggregationExprs.entrySet().iterator();\n     while (aggrIter.hasNext()) {\n       HashMap<String, ASTNode> h = aggrIter.next().getValue();\n       if ((h != null) && (!h.isEmpty()))\n", "projectName": "apache.hive", "bugLineNum": 87, "bugNodeStartChar": 3234, "bugNodeLength": 24, "fixLineNum": 87, "fixNodeStartChar": 3234, "fixNodeLength": 30, "sourceBeforeFix": "HashMap<String,ASTNode>", "sourceAfterFix": "LinkedHashMap<String,ASTNode>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "370ead7395fc9194a3dc2d05afc4b3409f6b27da", "fixCommitParentSHA1": "c4dd016566067eb16b97bfaa22f1a8a466afce6c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java\nindex 15c3467..af62c87 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/QBParseInfo.java\n@@ -58,7 +58,7 @@\n   private int outerQueryLimit;\n \n   // used by GroupBy\n-  private HashMap<String, HashMap<String, ASTNode> > destToAggregationExprs;\n+  private LinkedHashMap<String, LinkedHashMap<String, ASTNode> > destToAggregationExprs;\n   private HashMap<String, ASTNode> destToDistinctFuncExpr;\n \n   @SuppressWarnings(\"unused\")\n@@ -76,7 +76,7 @@\n     this.destToSortby = new HashMap<String, ASTNode>();\n     this.destToLimit = new HashMap<String, Integer>();\n     \n-    this.destToAggregationExprs = new HashMap<String, HashMap<String, ASTNode> >();\n+    this.destToAggregationExprs = new LinkedHashMap<String, LinkedHashMap<String, ASTNode> >();\n     this.destToDistinctFuncExpr = new HashMap<String, ASTNode>();\n     \n     this.alias = alias;\n@@ -84,7 +84,7 @@\n     this.outerQueryLimit = -1;\n   }\n \n-  public void setAggregationExprsForClause(String clause, HashMap<String, ASTNode> aggregationTrees) {\n+  public void setAggregationExprsForClause(String clause, LinkedHashMap<String, ASTNode> aggregationTrees) {\n     this.destToAggregationExprs.put(clause, aggregationTrees);\n   }\n \n@@ -273,7 +273,7 @@\n        (!destToClusterby.isEmpty()))\n       return false;\n     \n-    Iterator<Map.Entry<String, HashMap<String, ASTNode>>> aggrIter = destToAggregationExprs.entrySet().iterator();\n+    Iterator<Map.Entry<String, LinkedHashMap<String, ASTNode>>> aggrIter = destToAggregationExprs.entrySet().iterator();\n     while (aggrIter.hasNext()) {\n       HashMap<String, ASTNode> h = aggrIter.next().getValue();\n       if ((h != null) && (!h.isEmpty()))\n", "projectName": "apache.hive", "bugLineNum": 276, "bugNodeStartChar": 8069, "bugNodeLength": 24, "fixLineNum": 276, "fixNodeStartChar": 8069, "fixNodeLength": 30, "sourceBeforeFix": "HashMap<String,ASTNode>", "sourceAfterFix": "LinkedHashMap<String,ASTNode>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "370ead7395fc9194a3dc2d05afc4b3409f6b27da", "fixCommitParentSHA1": "c4dd016566067eb16b97bfaa22f1a8a466afce6c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\nindex c170491..0e80506 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n@@ -170,11 +170,11 @@\n     }\n   }\n \n-  private HashMap<String, ASTNode> doPhase1GetAggregationsFromSelect(\n+  private LinkedHashMap<String, ASTNode> doPhase1GetAggregationsFromSelect(\n       ASTNode selExpr) {\n     // Iterate over the selects search for aggregation Trees.\n     // Use String as keys to eliminate duplicate trees.\n-    HashMap<String, ASTNode> aggregationTrees = new HashMap<String, ASTNode>();\n+    LinkedHashMap<String, ASTNode> aggregationTrees = new LinkedHashMap<String, ASTNode>();\n     for (int i = 0; i < selExpr.getChildCount(); ++i) {\n       ASTNode sel = (ASTNode) selExpr.getChild(i).getChild(0);\n       doPhase1GetAllAggregations(sel, aggregationTrees);\n@@ -348,7 +348,7 @@\n       case HiveParser.TOK_SELECT:\n         qb.countSel();\n         qbp.setSelExprForClause(ctx_1.dest, ast);\n-        HashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast);\n+        LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast);\n         qbp.setAggregationExprsForClause(ctx_1.dest, aggregations);\n         qbp.setDistinctFuncExprForClause(ctx_1.dest,\n             doPhase1GetDistinctFuncExpr(aggregations));\n", "projectName": "apache.hive", "bugLineNum": 173, "bugNodeStartChar": 6628, "bugNodeLength": 24, "fixLineNum": 173, "fixNodeStartChar": 6628, "fixNodeLength": 30, "sourceBeforeFix": "HashMap<String,ASTNode>", "sourceAfterFix": "LinkedHashMap<String,ASTNode>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "370ead7395fc9194a3dc2d05afc4b3409f6b27da", "fixCommitParentSHA1": "c4dd016566067eb16b97bfaa22f1a8a466afce6c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\nindex c170491..0e80506 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n@@ -170,11 +170,11 @@\n     }\n   }\n \n-  private HashMap<String, ASTNode> doPhase1GetAggregationsFromSelect(\n+  private LinkedHashMap<String, ASTNode> doPhase1GetAggregationsFromSelect(\n       ASTNode selExpr) {\n     // Iterate over the selects search for aggregation Trees.\n     // Use String as keys to eliminate duplicate trees.\n-    HashMap<String, ASTNode> aggregationTrees = new HashMap<String, ASTNode>();\n+    LinkedHashMap<String, ASTNode> aggregationTrees = new LinkedHashMap<String, ASTNode>();\n     for (int i = 0; i < selExpr.getChildCount(); ++i) {\n       ASTNode sel = (ASTNode) selExpr.getChild(i).getChild(0);\n       doPhase1GetAllAggregations(sel, aggregationTrees);\n@@ -348,7 +348,7 @@\n       case HiveParser.TOK_SELECT:\n         qb.countSel();\n         qbp.setSelExprForClause(ctx_1.dest, ast);\n-        HashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast);\n+        LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast);\n         qbp.setAggregationExprsForClause(ctx_1.dest, aggregations);\n         qbp.setDistinctFuncExprForClause(ctx_1.dest,\n             doPhase1GetDistinctFuncExpr(aggregations));\n", "projectName": "apache.hive", "bugLineNum": 177, "bugNodeStartChar": 6835, "bugNodeLength": 24, "fixLineNum": 177, "fixNodeStartChar": 6835, "fixNodeLength": 30, "sourceBeforeFix": "HashMap<String,ASTNode>", "sourceAfterFix": "LinkedHashMap<String,ASTNode>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "370ead7395fc9194a3dc2d05afc4b3409f6b27da", "fixCommitParentSHA1": "c4dd016566067eb16b97bfaa22f1a8a466afce6c", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\nindex c170491..0e80506 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java\n@@ -170,11 +170,11 @@\n     }\n   }\n \n-  private HashMap<String, ASTNode> doPhase1GetAggregationsFromSelect(\n+  private LinkedHashMap<String, ASTNode> doPhase1GetAggregationsFromSelect(\n       ASTNode selExpr) {\n     // Iterate over the selects search for aggregation Trees.\n     // Use String as keys to eliminate duplicate trees.\n-    HashMap<String, ASTNode> aggregationTrees = new HashMap<String, ASTNode>();\n+    LinkedHashMap<String, ASTNode> aggregationTrees = new LinkedHashMap<String, ASTNode>();\n     for (int i = 0; i < selExpr.getChildCount(); ++i) {\n       ASTNode sel = (ASTNode) selExpr.getChild(i).getChild(0);\n       doPhase1GetAllAggregations(sel, aggregationTrees);\n@@ -348,7 +348,7 @@\n       case HiveParser.TOK_SELECT:\n         qb.countSel();\n         qbp.setSelExprForClause(ctx_1.dest, ast);\n-        HashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast);\n+        LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast);\n         qbp.setAggregationExprsForClause(ctx_1.dest, aggregations);\n         qbp.setDistinctFuncExprForClause(ctx_1.dest,\n             doPhase1GetDistinctFuncExpr(aggregations));\n", "projectName": "apache.hive", "bugLineNum": 351, "bugNodeStartChar": 13252, "bugNodeLength": 24, "fixLineNum": 351, "fixNodeStartChar": 13252, "fixNodeLength": 30, "sourceBeforeFix": "HashMap<String,ASTNode>", "sourceAfterFix": "LinkedHashMap<String,ASTNode>"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "782a7444a6cf0d212cac614358d830cd66b8ae10", "fixCommitParentSHA1": "f4a5e678af30db49529cc48d834e520db72a9e83", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java\nindex 45f0d38..aeb9cc3 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java\n@@ -213,7 +213,7 @@\n         assert(expr.getChildCount() == 2);\n         String tabAlias = BaseSemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getText());\n         String colName = BaseSemanticAnalyzer.unescapeIdentifier(expr.getChild(1).getText());\n-        if (tabAlias.equals(tableAlias) && tab.isPartitionKey(colName)) {\n+        if (tabAlias.equalsIgnoreCase(tableAlias) && tab.isPartitionKey(colName)) {\n           hasPPred = true;\n         }\n         break;\n", "projectName": "apache.hive", "bugLineNum": 216, "bugNodeStartChar": 8958, "bugNodeLength": 27, "fixLineNum": 216, "fixNodeStartChar": 8958, "fixNodeLength": 37, "sourceBeforeFix": "tabAlias.equals(tableAlias)", "sourceAfterFix": "tabAlias.equalsIgnoreCase(tableAlias)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "782a7444a6cf0d212cac614358d830cd66b8ae10", "fixCommitParentSHA1": "f4a5e678af30db49529cc48d834e520db72a9e83", "bugFilePath": "ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java", "fixPatch": "diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java\nindex 45f0d38..aeb9cc3 100644\n--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java\n+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/PartitionPruner.java\n@@ -213,7 +213,7 @@\n         assert(expr.getChildCount() == 2);\n         String tabAlias = BaseSemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getText());\n         String colName = BaseSemanticAnalyzer.unescapeIdentifier(expr.getChild(1).getText());\n-        if (tabAlias.equals(tableAlias) && tab.isPartitionKey(colName)) {\n+        if (tabAlias.equalsIgnoreCase(tableAlias) && tab.isPartitionKey(colName)) {\n           hasPPred = true;\n         }\n         break;\n", "projectName": "apache.hive", "bugLineNum": 216, "bugNodeStartChar": 8958, "bugNodeLength": 27, "fixLineNum": 216, "fixNodeStartChar": 8958, "fixNodeLength": 37, "sourceBeforeFix": "tabAlias.equals(tableAlias)", "sourceAfterFix": "tabAlias.equalsIgnoreCase(tableAlias)"}]