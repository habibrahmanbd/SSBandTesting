[
  {
    "bugType": "SWAP_ARGUMENTS",
    "fixCommitSHA1": "9bda229dd362091c9889c026fb77928451faa7c7",
    "fixCommitParentSHA1": "2c6441bbbe67f063b8d5c202b56e78083cb40eee",
    "bugFilePath": "flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogManager.java",
    "fixPatch": "diff --git a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogManager.java b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogManager.java\nindex 839d5a9..5647709 100644\n--- a/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogManager.java\n+++ b/flink-table/flink-table-api-java/src/main/java/org/apache/flink/table/catalog/CatalogManager.java\n@@ -182,8 +182,8 @@\n \n \t\t\tLOG.info(\n \t\t\t\t\"Set the current default database as [{}] in the current default catalog [{}].\",\n-\t\t\t\tcurrentCatalogName,\n-\t\t\t\tcurrentDatabaseName);\n+\t\t\t\tcurrentDatabaseName,\n+\t\t\t\tcurrentCatalogName);\n \t\t}\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 183,
    "bugNodeStartChar": 6443,
    "bugNodeLength": 143,
    "fixLineNum": 183,
    "fixNodeStartChar": 6443,
    "fixNodeLength": 143,
    "sourceBeforeFix": "LOG.info(\"Set the current default database as [{}] in the current default catalog [{}].\",currentCatalogName,currentDatabaseName)",
    "sourceAfterFix": "LOG.info(\"Set the current default database as [{}] in the current default catalog [{}].\",currentDatabaseName,currentCatalogName)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "6618cef776cf1c9ce67ff8ee461412110ca5ada7",
    "fixCommitParentSHA1": "948ab73523bed79d1672884207fd3a06f33d9572",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\nindex b9250d6..f157006 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\n@@ -139,7 +139,7 @@\n \t\t\t\tqueryServiceAddressesFuture.whenCompleteAsync(\n \t\t\t\t\t(Collection<String> queryServiceAddresses, Throwable throwable) -> {\n \t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\tLOG.warn(\"Requesting paths for query services failed.\", throwable);\n+\t\t\t\t\t\t\tLOG.debug(\"Requesting paths for query services failed.\", throwable);\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tfor (String queryServiceAddress : queryServiceAddresses) {\n \t\t\t\t\t\t\t\tretrieveAndQueryMetrics(queryServiceAddress);\n@@ -157,7 +157,7 @@\n \t\t\t\ttaskManagerQueryServiceGatewaysFuture.whenCompleteAsync(\n \t\t\t\t\t(Collection<Tuple2<ResourceID, String>> queryServiceGateways, Throwable throwable) -> {\n \t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\tLOG.warn(\"Requesting TaskManager's path for query services failed.\", throwable);\n+\t\t\t\t\t\t\tLOG.debug(\"Requesting TaskManager's path for query services failed.\", throwable);\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tList<String> taskManagersToRetain = queryServiceGateways\n \t\t\t\t\t\t\t\t.stream()\n@@ -175,7 +175,7 @@\n \t\t\t\t\texecutor);\n \t\t\t}\n \t\t} catch (Exception e) {\n-\t\t\tLOG.warn(\"Exception while fetching metrics.\", e);\n+\t\t\tLOG.debug(\"Exception while fetching metrics.\", e);\n \t\t}\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 142,
    "bugNodeStartChar": 5332,
    "bugNodeLength": 66,
    "fixLineNum": 142,
    "fixNodeStartChar": 5332,
    "fixNodeLength": 67,
    "sourceBeforeFix": "LOG.warn(\"Requesting paths for query services failed.\",throwable)",
    "sourceAfterFix": "LOG.debug(\"Requesting paths for query services failed.\",throwable)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "6618cef776cf1c9ce67ff8ee461412110ca5ada7",
    "fixCommitParentSHA1": "948ab73523bed79d1672884207fd3a06f33d9572",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\nindex b9250d6..f157006 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\n@@ -139,7 +139,7 @@\n \t\t\t\tqueryServiceAddressesFuture.whenCompleteAsync(\n \t\t\t\t\t(Collection<String> queryServiceAddresses, Throwable throwable) -> {\n \t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\tLOG.warn(\"Requesting paths for query services failed.\", throwable);\n+\t\t\t\t\t\t\tLOG.debug(\"Requesting paths for query services failed.\", throwable);\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tfor (String queryServiceAddress : queryServiceAddresses) {\n \t\t\t\t\t\t\t\tretrieveAndQueryMetrics(queryServiceAddress);\n@@ -157,7 +157,7 @@\n \t\t\t\ttaskManagerQueryServiceGatewaysFuture.whenCompleteAsync(\n \t\t\t\t\t(Collection<Tuple2<ResourceID, String>> queryServiceGateways, Throwable throwable) -> {\n \t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\tLOG.warn(\"Requesting TaskManager's path for query services failed.\", throwable);\n+\t\t\t\t\t\t\tLOG.debug(\"Requesting TaskManager's path for query services failed.\", throwable);\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tList<String> taskManagersToRetain = queryServiceGateways\n \t\t\t\t\t\t\t\t.stream()\n@@ -175,7 +175,7 @@\n \t\t\t\t\texecutor);\n \t\t\t}\n \t\t} catch (Exception e) {\n-\t\t\tLOG.warn(\"Exception while fetching metrics.\", e);\n+\t\t\tLOG.debug(\"Exception while fetching metrics.\", e);\n \t\t}\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 142,
    "bugNodeStartChar": 5332,
    "bugNodeLength": 66,
    "fixLineNum": 142,
    "fixNodeStartChar": 5332,
    "fixNodeLength": 67,
    "sourceBeforeFix": "LOG.warn(\"Requesting paths for query services failed.\",throwable)",
    "sourceAfterFix": "LOG.debug(\"Requesting paths for query services failed.\",throwable)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "6618cef776cf1c9ce67ff8ee461412110ca5ada7",
    "fixCommitParentSHA1": "948ab73523bed79d1672884207fd3a06f33d9572",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\nindex b9250d6..f157006 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\n@@ -139,7 +139,7 @@\n \t\t\t\tqueryServiceAddressesFuture.whenCompleteAsync(\n \t\t\t\t\t(Collection<String> queryServiceAddresses, Throwable throwable) -> {\n \t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\tLOG.warn(\"Requesting paths for query services failed.\", throwable);\n+\t\t\t\t\t\t\tLOG.debug(\"Requesting paths for query services failed.\", throwable);\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tfor (String queryServiceAddress : queryServiceAddresses) {\n \t\t\t\t\t\t\t\tretrieveAndQueryMetrics(queryServiceAddress);\n@@ -157,7 +157,7 @@\n \t\t\t\ttaskManagerQueryServiceGatewaysFuture.whenCompleteAsync(\n \t\t\t\t\t(Collection<Tuple2<ResourceID, String>> queryServiceGateways, Throwable throwable) -> {\n \t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\tLOG.warn(\"Requesting TaskManager's path for query services failed.\", throwable);\n+\t\t\t\t\t\t\tLOG.debug(\"Requesting TaskManager's path for query services failed.\", throwable);\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tList<String> taskManagersToRetain = queryServiceGateways\n \t\t\t\t\t\t\t\t.stream()\n@@ -175,7 +175,7 @@\n \t\t\t\t\texecutor);\n \t\t\t}\n \t\t} catch (Exception e) {\n-\t\t\tLOG.warn(\"Exception while fetching metrics.\", e);\n+\t\t\tLOG.debug(\"Exception while fetching metrics.\", e);\n \t\t}\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 160,
    "bugNodeStartChar": 6286,
    "bugNodeLength": 79,
    "fixLineNum": 160,
    "fixNodeStartChar": 6286,
    "fixNodeLength": 80,
    "sourceBeforeFix": "LOG.warn(\"Requesting TaskManager's path for query services failed.\",throwable)",
    "sourceAfterFix": "LOG.debug(\"Requesting TaskManager's path for query services failed.\",throwable)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "6618cef776cf1c9ce67ff8ee461412110ca5ada7",
    "fixCommitParentSHA1": "948ab73523bed79d1672884207fd3a06f33d9572",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\nindex b9250d6..f157006 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\n@@ -139,7 +139,7 @@\n \t\t\t\tqueryServiceAddressesFuture.whenCompleteAsync(\n \t\t\t\t\t(Collection<String> queryServiceAddresses, Throwable throwable) -> {\n \t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\tLOG.warn(\"Requesting paths for query services failed.\", throwable);\n+\t\t\t\t\t\t\tLOG.debug(\"Requesting paths for query services failed.\", throwable);\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tfor (String queryServiceAddress : queryServiceAddresses) {\n \t\t\t\t\t\t\t\tretrieveAndQueryMetrics(queryServiceAddress);\n@@ -157,7 +157,7 @@\n \t\t\t\ttaskManagerQueryServiceGatewaysFuture.whenCompleteAsync(\n \t\t\t\t\t(Collection<Tuple2<ResourceID, String>> queryServiceGateways, Throwable throwable) -> {\n \t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\tLOG.warn(\"Requesting TaskManager's path for query services failed.\", throwable);\n+\t\t\t\t\t\t\tLOG.debug(\"Requesting TaskManager's path for query services failed.\", throwable);\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tList<String> taskManagersToRetain = queryServiceGateways\n \t\t\t\t\t\t\t\t.stream()\n@@ -175,7 +175,7 @@\n \t\t\t\t\texecutor);\n \t\t\t}\n \t\t} catch (Exception e) {\n-\t\t\tLOG.warn(\"Exception while fetching metrics.\", e);\n+\t\t\tLOG.debug(\"Exception while fetching metrics.\", e);\n \t\t}\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 160,
    "bugNodeStartChar": 6286,
    "bugNodeLength": 79,
    "fixLineNum": 160,
    "fixNodeStartChar": 6286,
    "fixNodeLength": 80,
    "sourceBeforeFix": "LOG.warn(\"Requesting TaskManager's path for query services failed.\",throwable)",
    "sourceAfterFix": "LOG.debug(\"Requesting TaskManager's path for query services failed.\",throwable)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "6618cef776cf1c9ce67ff8ee461412110ca5ada7",
    "fixCommitParentSHA1": "948ab73523bed79d1672884207fd3a06f33d9572",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\nindex b9250d6..f157006 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\n@@ -139,7 +139,7 @@\n \t\t\t\tqueryServiceAddressesFuture.whenCompleteAsync(\n \t\t\t\t\t(Collection<String> queryServiceAddresses, Throwable throwable) -> {\n \t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\tLOG.warn(\"Requesting paths for query services failed.\", throwable);\n+\t\t\t\t\t\t\tLOG.debug(\"Requesting paths for query services failed.\", throwable);\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tfor (String queryServiceAddress : queryServiceAddresses) {\n \t\t\t\t\t\t\t\tretrieveAndQueryMetrics(queryServiceAddress);\n@@ -157,7 +157,7 @@\n \t\t\t\ttaskManagerQueryServiceGatewaysFuture.whenCompleteAsync(\n \t\t\t\t\t(Collection<Tuple2<ResourceID, String>> queryServiceGateways, Throwable throwable) -> {\n \t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\tLOG.warn(\"Requesting TaskManager's path for query services failed.\", throwable);\n+\t\t\t\t\t\t\tLOG.debug(\"Requesting TaskManager's path for query services failed.\", throwable);\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tList<String> taskManagersToRetain = queryServiceGateways\n \t\t\t\t\t\t\t\t.stream()\n@@ -175,7 +175,7 @@\n \t\t\t\t\texecutor);\n \t\t\t}\n \t\t} catch (Exception e) {\n-\t\t\tLOG.warn(\"Exception while fetching metrics.\", e);\n+\t\t\tLOG.debug(\"Exception while fetching metrics.\", e);\n \t\t}\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 178,
    "bugNodeStartChar": 6868,
    "bugNodeLength": 48,
    "fixLineNum": 178,
    "fixNodeStartChar": 6868,
    "fixNodeLength": 49,
    "sourceBeforeFix": "LOG.warn(\"Exception while fetching metrics.\",e)",
    "sourceAfterFix": "LOG.debug(\"Exception while fetching metrics.\",e)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "6618cef776cf1c9ce67ff8ee461412110ca5ada7",
    "fixCommitParentSHA1": "948ab73523bed79d1672884207fd3a06f33d9572",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\nindex b9250d6..f157006 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricFetcherImpl.java\n@@ -139,7 +139,7 @@\n \t\t\t\tqueryServiceAddressesFuture.whenCompleteAsync(\n \t\t\t\t\t(Collection<String> queryServiceAddresses, Throwable throwable) -> {\n \t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\tLOG.warn(\"Requesting paths for query services failed.\", throwable);\n+\t\t\t\t\t\t\tLOG.debug(\"Requesting paths for query services failed.\", throwable);\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tfor (String queryServiceAddress : queryServiceAddresses) {\n \t\t\t\t\t\t\t\tretrieveAndQueryMetrics(queryServiceAddress);\n@@ -157,7 +157,7 @@\n \t\t\t\ttaskManagerQueryServiceGatewaysFuture.whenCompleteAsync(\n \t\t\t\t\t(Collection<Tuple2<ResourceID, String>> queryServiceGateways, Throwable throwable) -> {\n \t\t\t\t\t\tif (throwable != null) {\n-\t\t\t\t\t\t\tLOG.warn(\"Requesting TaskManager's path for query services failed.\", throwable);\n+\t\t\t\t\t\t\tLOG.debug(\"Requesting TaskManager's path for query services failed.\", throwable);\n \t\t\t\t\t\t} else {\n \t\t\t\t\t\t\tList<String> taskManagersToRetain = queryServiceGateways\n \t\t\t\t\t\t\t\t.stream()\n@@ -175,7 +175,7 @@\n \t\t\t\t\texecutor);\n \t\t\t}\n \t\t} catch (Exception e) {\n-\t\t\tLOG.warn(\"Exception while fetching metrics.\", e);\n+\t\t\tLOG.debug(\"Exception while fetching metrics.\", e);\n \t\t}\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 178,
    "bugNodeStartChar": 6868,
    "bugNodeLength": 48,
    "fixLineNum": 178,
    "fixNodeStartChar": 6868,
    "fixNodeLength": 49,
    "sourceBeforeFix": "LOG.warn(\"Exception while fetching metrics.\",e)",
    "sourceAfterFix": "LOG.debug(\"Exception while fetching metrics.\",e)"
  },
  {
    "bugType": "OVERLOAD_METHOD_DELETED_ARGS",
    "fixCommitSHA1": "bde51f9e46600154bd60321d3d345bb539b30ed1",
    "fixCommitParentSHA1": "8dbc64105c13dfae467f2cfb75ec67d0d2d56c84",
    "bugFilePath": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java",
    "fixPatch": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java\nindex f78bd1c..b619eb5 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java\n@@ -632,8 +632,7 @@\n \t\tOperandTypes.or(\n \t\t\tOperandTypes.NILADIC,\n \t\t\tOperandTypes.family(SqlTypeFamily.STRING),\n-\t\t\tOperandTypes.family(SqlTypeFamily.STRING, SqlTypeFamily.STRING),\n-\t\t\tOperandTypes.family(SqlTypeFamily.TIMESTAMP)),\n+\t\t\tOperandTypes.family(SqlTypeFamily.STRING, SqlTypeFamily.STRING)),\n \t\tSqlFunctionCategory.TIMEDATE) {\n \n \t\t@Override\n",
    "projectName": "apache.flink",
    "bugLineNum": 632,
    "bugNodeStartChar": 19388,
    "bugNodeLength": 204,
    "fixLineNum": 632,
    "fixNodeStartChar": 19388,
    "fixNodeLength": 155,
    "sourceBeforeFix": "OperandTypes.or(OperandTypes.NILADIC,OperandTypes.family(SqlTypeFamily.STRING),OperandTypes.family(SqlTypeFamily.STRING,SqlTypeFamily.STRING),OperandTypes.family(SqlTypeFamily.TIMESTAMP))",
    "sourceAfterFix": "OperandTypes.or(OperandTypes.NILADIC,OperandTypes.family(SqlTypeFamily.STRING),OperandTypes.family(SqlTypeFamily.STRING,SqlTypeFamily.STRING))"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "ca414aeb4e7495bbfa2751d241aafc85bc3fa9b6",
    "fixCommitParentSHA1": "78d1e68ab120e2fecb931e6d9ea1ee8bef247cde",
    "bugFilePath": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java",
    "fixPatch": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java\nindex 50ff193..dbabb69 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/planner/functions/sql/FlinkSqlOperatorTable.java\n@@ -946,9 +946,9 @@\n \tpublic static final SqlFirstLastValueAggFunction LAST_VALUE = new SqlFirstLastValueAggFunction(SqlKind.LAST_VALUE);\n \n \t/**\n-\t * <code>CONCAT_AGG</code> aggregate function.\n+\t * <code>LISTAGG</code> aggregate function.\n \t */\n-\tpublic static final SqlConcatAggFunction CONCAT_AGG = new SqlConcatAggFunction();\n+\tpublic static final SqlListAggFunction LISTAGG = new SqlListAggFunction();\n \n \t/**\n \t * <code>INCR_SUM</code> aggregate function.\n",
    "projectName": "apache.flink",
    "bugLineNum": 948,
    "bugNodeStartChar": 30252,
    "bugNodeLength": 139,
    "fixLineNum": 948,
    "fixNodeStartChar": 30252,
    "fixNodeLength": 132,
    "sourceBeforeFix": "/**   * <code>CONCAT_AGG</code> aggregate function.  */ public static final SqlConcatAggFunction CONCAT_AGG=new SqlConcatAggFunction(); ",
    "sourceAfterFix": "/**   * <code>CONCAT_AGG</code> aggregate function.  */ public static final SqlListAggFunction LISTAGG=new SqlListAggFunction(); "
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "e0617be01454b663de5e21ae73f0f1d61b504da0",
    "fixCommitParentSHA1": "e408636a831b63b414c9705b9c3a006fc0572cdc",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionBuilder.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionBuilder.java b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionBuilder.java\nindex 27eaab8..4bb1a85 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionBuilder.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/io/network/partition/ResultPartitionBuilder.java\n@@ -113,7 +113,7 @@\n \t\treturn this;\n \t}\n \n-\tpublic ResultPartitionBuilder setNetworkBufferSize(int networkBufferSize) {\n+\tResultPartitionBuilder setNetworkBufferSize(int networkBufferSize) {\n \t\tthis.networkBufferSize = networkBufferSize;\n \t\treturn this;\n \t}\n@@ -129,7 +129,8 @@\n \t\treturn this;\n \t}\n \n-\tpublic ResultPartitionBuilder setBoundedBlockingSubpartitionType(BoundedBlockingSubpartitionType blockingSubpartitionType) {\n+\tResultPartitionBuilder setBoundedBlockingSubpartitionType(\n+\t\t\t@SuppressWarnings(\"SameParameterValue\") BoundedBlockingSubpartitionType blockingSubpartitionType) {\n \t\tthis.blockingSubpartitionType = blockingSubpartitionType;\n \t\treturn this;\n \t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 116,
    "bugNodeStartChar": 4310,
    "bugNodeLength": 139,
    "fixLineNum": 116,
    "fixNodeStartChar": 4310,
    "fixNodeLength": 132,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "0"
  },
  {
    "bugType": "SWAP_BOOLEAN_LITERAL",
    "fixCommitSHA1": "763dad49dcfe96fa56fb88da0a4d7d53e148f337",
    "fixCommitParentSHA1": "32ad3fa960681ffc8c21179b6592f6e0a6875c11",
    "bugFilePath": "flink-table/flink-table-planner/src/main/java/org/apache/calcite/avatica/util/DateTimeUtils.java",
    "fixPatch": "diff --git a/flink-table/flink-table-planner/src/main/java/org/apache/calcite/avatica/util/DateTimeUtils.java b/flink-table/flink-table-planner/src/main/java/org/apache/calcite/avatica/util/DateTimeUtils.java\nindex fe09d18..2bb8515 100644\n--- a/flink-table/flink-table-planner/src/main/java/org/apache/calcite/avatica/util/DateTimeUtils.java\n+++ b/flink-table/flink-table-planner/src/main/java/org/apache/calcite/avatica/util/DateTimeUtils.java\n@@ -26,7 +26,8 @@\n import java.util.TimeZone;\n \n /*\n- * THIS FILE HAS BEEN COPIED FROM THE APACHE CALCITE PROJECT UNTIL CALCITE-1884 IS FIXED.\n+ * THIS FILE HAS BEEN COPIED FROM THE APACHE CALCITE PROJECT UNTIL CALCITE-1884, CALCITE-3199 IS FIXED.\n+ * (Modified line: 838)\n  */\n \n /**\n@@ -835,7 +836,7 @@\n \t}\n \n \tpublic static long unixDateCeil(TimeUnitRange range, long date) {\n-\t\treturn julianDateFloor(range, (int) date + EPOCH_JULIAN, true);\n+\t\treturn julianDateFloor(range, (int) date + EPOCH_JULIAN, false);\n \t}\n \n \tprivate static int julianDateFloor(TimeUnitRange range, int julian,\n",
    "projectName": "apache.flink",
    "bugLineNum": 838,
    "bugNodeStartChar": 23299,
    "bugNodeLength": 55,
    "fixLineNum": 838,
    "fixNodeStartChar": 23299,
    "fixNodeLength": 56,
    "sourceBeforeFix": "julianDateFloor(range,(int)date + EPOCH_JULIAN,true)",
    "sourceAfterFix": "julianDateFloor(range,(int)date + EPOCH_JULIAN,false)"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "3f62d5d78a3f792e3fb984ff88d38190dabad586",
    "fixCommitParentSHA1": "de1a8a0444c231df6c57a70cefb689aa7126a502",
    "bugFilePath": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java",
    "fixPatch": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java\nindex b791a70..6e2f9c7 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java\n@@ -33,15 +33,15 @@\n  * <p>While {@link ObjectPath} is used within the same catalog, instances of this class can be used\n  * across catalogs.\n  *\n- * <p>Two objects are considered equal if they share the same type identifier in a stable session context.\n+ * <p>Two objects are considered equal if they share the same object identifier in a stable session context.\n  */\n public final class ObjectIdentifier implements Serializable {\n \n-\tprivate String catalogName;\n+\tprivate final String catalogName;\n \n-\tprivate String databaseName;\n+\tprivate final String databaseName;\n \n-\tprivate String objectName;\n+\tprivate final String objectName;\n \n \tpublic static ObjectIdentifier of(String catalogName, String databaseName, String objectName) {\n \t\treturn new ObjectIdentifier(catalogName, databaseName, objectName);\n",
    "projectName": "apache.flink",
    "bugLineNum": 40,
    "bugNodeStartChar": 1586,
    "bugNodeLength": 27,
    "fixLineNum": 40,
    "fixNodeStartChar": 1586,
    "fixNodeLength": 33,
    "sourceBeforeFix": "2",
    "sourceAfterFix": "18"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "3f62d5d78a3f792e3fb984ff88d38190dabad586",
    "fixCommitParentSHA1": "de1a8a0444c231df6c57a70cefb689aa7126a502",
    "bugFilePath": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java",
    "fixPatch": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java\nindex b791a70..6e2f9c7 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java\n@@ -33,15 +33,15 @@\n  * <p>While {@link ObjectPath} is used within the same catalog, instances of this class can be used\n  * across catalogs.\n  *\n- * <p>Two objects are considered equal if they share the same type identifier in a stable session context.\n+ * <p>Two objects are considered equal if they share the same object identifier in a stable session context.\n  */\n public final class ObjectIdentifier implements Serializable {\n \n-\tprivate String catalogName;\n+\tprivate final String catalogName;\n \n-\tprivate String databaseName;\n+\tprivate final String databaseName;\n \n-\tprivate String objectName;\n+\tprivate final String objectName;\n \n \tpublic static ObjectIdentifier of(String catalogName, String databaseName, String objectName) {\n \t\treturn new ObjectIdentifier(catalogName, databaseName, objectName);\n",
    "projectName": "apache.flink",
    "bugLineNum": 42,
    "bugNodeStartChar": 1616,
    "bugNodeLength": 28,
    "fixLineNum": 42,
    "fixNodeStartChar": 1616,
    "fixNodeLength": 34,
    "sourceBeforeFix": "2",
    "sourceAfterFix": "18"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "3f62d5d78a3f792e3fb984ff88d38190dabad586",
    "fixCommitParentSHA1": "de1a8a0444c231df6c57a70cefb689aa7126a502",
    "bugFilePath": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java",
    "fixPatch": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java\nindex b791a70..6e2f9c7 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/catalog/ObjectIdentifier.java\n@@ -33,15 +33,15 @@\n  * <p>While {@link ObjectPath} is used within the same catalog, instances of this class can be used\n  * across catalogs.\n  *\n- * <p>Two objects are considered equal if they share the same type identifier in a stable session context.\n+ * <p>Two objects are considered equal if they share the same object identifier in a stable session context.\n  */\n public final class ObjectIdentifier implements Serializable {\n \n-\tprivate String catalogName;\n+\tprivate final String catalogName;\n \n-\tprivate String databaseName;\n+\tprivate final String databaseName;\n \n-\tprivate String objectName;\n+\tprivate final String objectName;\n \n \tpublic static ObjectIdentifier of(String catalogName, String databaseName, String objectName) {\n \t\treturn new ObjectIdentifier(catalogName, databaseName, objectName);\n",
    "projectName": "apache.flink",
    "bugLineNum": 44,
    "bugNodeStartChar": 1647,
    "bugNodeLength": 26,
    "fixLineNum": 44,
    "fixNodeStartChar": 1647,
    "fixNodeLength": 32,
    "sourceBeforeFix": "2",
    "sourceAfterFix": "18"
  },
  {
    "bugType": "SWAP_ARGUMENTS",
    "fixCommitSHA1": "72638160b5b9eac20db8abf0fffc1b2c206ea051",
    "fixCommitParentSHA1": "4f1837297e8f4ffae09b04ad68bd936db1d1231d",
    "bugFilePath": "flink-core/src/main/java/org/apache/flink/util/IOUtils.java",
    "fixPatch": "diff --git a/flink-core/src/main/java/org/apache/flink/util/IOUtils.java b/flink-core/src/main/java/org/apache/flink/util/IOUtils.java\nindex 498c899..02b11e6 100644\n--- a/flink-core/src/main/java/org/apache/flink/util/IOUtils.java\n+++ b/flink-core/src/main/java/org/apache/flink/util/IOUtils.java\n@@ -231,7 +231,7 @@\n \t\t\t\t\t\tcloseable.close();\n \t\t\t\t\t}\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tcollectedExceptions = ExceptionUtils.firstOrSuppressed(collectedExceptions, e);\n+\t\t\t\t\tcollectedExceptions = ExceptionUtils.firstOrSuppressed(e, collectedExceptions);\n \t\t\t\t}\n \t\t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 234,
    "bugNodeStartChar": 6884,
    "bugNodeLength": 56,
    "fixLineNum": 234,
    "fixNodeStartChar": 6884,
    "fixNodeLength": 56,
    "sourceBeforeFix": "ExceptionUtils.firstOrSuppressed(collectedExceptions,e)",
    "sourceAfterFix": "ExceptionUtils.firstOrSuppressed(e,collectedExceptions)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "44ff1443fc37c91a60b53687df167406859610aa",
    "fixCommitParentSHA1": "92fe83ffa88659d310a18666a57d1562fcba8fcb",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerServices.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerServices.java b/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerServices.java\nindex e211fc0..d4e27e0 100755\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerServices.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerServices.java\n@@ -527,14 +527,14 @@\n \n \t@VisibleForTesting\n \tpublic static ResourceProfile computeSlotResourceProfile(int numOfSlots, long managedMemorySize) {\n-\t\tint managedMemoryPerSlot = (int) (managedMemorySize / numOfSlots);\n+\t\tint managedMemoryPerSlotMB = (int) bytesToMegabytes(managedMemorySize / numOfSlots);\n \t\treturn new ResourceProfile(\n \t\t\tDouble.MAX_VALUE,\n \t\t\tInteger.MAX_VALUE,\n \t\t\tInteger.MAX_VALUE,\n \t\t\tInteger.MAX_VALUE,\n \t\t\tInteger.MAX_VALUE,\n-\t\t\tmanagedMemoryPerSlot,\n+\t\t\tmanagedMemoryPerSlotMB,\n \t\t\tCollections.emptyMap());\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 531,
    "bugNodeStartChar": 21069,
    "bugNodeLength": 181,
    "fixLineNum": 531,
    "fixNodeStartChar": 21069,
    "fixNodeLength": 183,
    "sourceBeforeFix": "new ResourceProfile(Double.MAX_VALUE,Integer.MAX_VALUE,Integer.MAX_VALUE,Integer.MAX_VALUE,Integer.MAX_VALUE,managedMemoryPerSlot,Collections.emptyMap())",
    "sourceAfterFix": "new ResourceProfile(Double.MAX_VALUE,Integer.MAX_VALUE,Integer.MAX_VALUE,Integer.MAX_VALUE,Integer.MAX_VALUE,managedMemoryPerSlotMB,Collections.emptyMap())"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "3fa91f277480e4089c47dba6af63c5f38a7ca5dd",
    "fixCommitParentSHA1": "de6706ae2bd0019011516aa9851bc3284350853d",
    "bugFilePath": "flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/executor/ExecutorBase.java",
    "fixPatch": "diff --git a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/executor/ExecutorBase.java b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/executor/ExecutorBase.java\nindex 4d28407..fb6bbf1 100644\n--- a/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/executor/ExecutorBase.java\n+++ b/flink-table/flink-table-planner-blink/src/main/java/org/apache/flink/table/executor/ExecutorBase.java\n@@ -32,7 +32,9 @@\n  */\n @Internal\n public abstract class ExecutorBase implements Executor {\n-\tprivate final String DEFAULT_JOB_NAME = \"Flink Exec Table Job\";\n+\n+\tprivate static final String DEFAULT_JOB_NAME = \"Flink Exec Table Job\";\n+\n \tprivate final StreamExecutionEnvironment executionEnvironment;\n \tprotected List<Transformation<?>> transformations = new ArrayList<>();\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 35,
    "bugNodeStartChar": 1339,
    "bugNodeLength": 63,
    "fixLineNum": 36,
    "fixNodeStartChar": 1340,
    "fixNodeLength": 70,
    "sourceBeforeFix": "18",
    "sourceAfterFix": "26"
  },
  {
    "bugType": "CHANGE_UNARY_OPERATOR",
    "fixCommitSHA1": "4e280c8467faf7c255557dce9e0e87eebf5c15e0",
    "fixCommitParentSHA1": "37331a01b257693f0b2c746219b9277535044ef4",
    "bugFilePath": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/TypeInferenceUtil.java",
    "fixPatch": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/TypeInferenceUtil.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/TypeInferenceUtil.java\nindex 045ec0e..04ab899 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/TypeInferenceUtil.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/inference/TypeInferenceUtil.java\n@@ -180,7 +180,7 @@\n \t\t\t}\n \t\t});\n \n-\t\tif (argumentCount.isValidCount(actualCount)) {\n+\t\tif (!argumentCount.isValidCount(actualCount)) {\n \t\t\tthrow new ValidationException(\n \t\t\t\tString.format(\n \t\t\t\t\t\"Invalid number of arguments. %d arguments passed.\",\n",
    "projectName": "apache.flink",
    "bugLineNum": 183,
    "bugNodeStartChar": 5954,
    "bugNodeLength": 39,
    "fixLineNum": 183,
    "fixNodeStartChar": 5954,
    "fixNodeLength": 40,
    "sourceBeforeFix": "argumentCount.isValidCount(actualCount)",
    "sourceAfterFix": "!argumentCount.isValidCount(actualCount)"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "e3c68859dd31ee2d651cbba5f31474652c9553dd",
    "fixCommitParentSHA1": "66b38acd3b82967c44450d9eec58266e1d2fb124",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/util/clock/ManualClock.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/util/clock/ManualClock.java b/flink-runtime/src/test/java/org/apache/flink/runtime/util/clock/ManualClock.java\nindex 69ae7be..a3a15e5 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/util/clock/ManualClock.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/util/clock/ManualClock.java\n@@ -30,12 +30,12 @@\n \n \t@Override\n \tpublic long absoluteTimeMillis() {\n-\t\treturn currentTime.get() / 1_000L;\n+\t\treturn currentTime.get() / 1_000_000L;\n \t}\n \n \t@Override\n \tpublic long relativeTimeMillis() {\n-\t\treturn currentTime.get() / 1_000L;\n+\t\treturn currentTime.get() / 1_000_000L;\n \t}\n \n \t@Override\n",
    "projectName": "apache.flink",
    "bugLineNum": 33,
    "bugNodeStartChar": 1171,
    "bugNodeLength": 26,
    "fixLineNum": 33,
    "fixNodeStartChar": 1171,
    "fixNodeLength": 30,
    "sourceBeforeFix": "currentTime.get() / 1_000L",
    "sourceAfterFix": "currentTime.get() / 1_000_000L"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "e3c68859dd31ee2d651cbba5f31474652c9553dd",
    "fixCommitParentSHA1": "66b38acd3b82967c44450d9eec58266e1d2fb124",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/util/clock/ManualClock.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/util/clock/ManualClock.java b/flink-runtime/src/test/java/org/apache/flink/runtime/util/clock/ManualClock.java\nindex 69ae7be..a3a15e5 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/util/clock/ManualClock.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/util/clock/ManualClock.java\n@@ -30,12 +30,12 @@\n \n \t@Override\n \tpublic long absoluteTimeMillis() {\n-\t\treturn currentTime.get() / 1_000L;\n+\t\treturn currentTime.get() / 1_000_000L;\n \t}\n \n \t@Override\n \tpublic long relativeTimeMillis() {\n-\t\treturn currentTime.get() / 1_000L;\n+\t\treturn currentTime.get() / 1_000_000L;\n \t}\n \n \t@Override\n",
    "projectName": "apache.flink",
    "bugLineNum": 38,
    "bugNodeStartChar": 1259,
    "bugNodeLength": 26,
    "fixLineNum": 38,
    "fixNodeStartChar": 1259,
    "fixNodeLength": 30,
    "sourceBeforeFix": "currentTime.get() / 1_000L",
    "sourceAfterFix": "currentTime.get() / 1_000_000L"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "5f5f02b1272ceba5e72ac8bb29e3d260d66bd493",
    "fixCommitParentSHA1": "d11958281ea785b6b5fcc32a169ed4b96dcecd87",
    "bugFilePath": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/logical/AnyType.java",
    "fixPatch": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/logical/AnyType.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/logical/AnyType.java\nindex 4f17539..ba849ca 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/logical/AnyType.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/logical/AnyType.java\n@@ -141,7 +141,7 @@\n \t\tif (serializerString == null) {\n \t\t\tfinal DataOutputSerializer outputSerializer = new DataOutputSerializer(128);\n \t\t\ttry {\n-\t\t\t\tserializer.snapshotConfiguration().writeSnapshot(outputSerializer);\n+\t\t\t\tTypeSerializerSnapshot.writeVersionedSnapshot(outputSerializer, serializer.snapshotConfiguration());\n \t\t\t\tserializerString = EncodingUtils.encodeBytesToBase64(outputSerializer.getCopyOfBuffer());\n \t\t\t\treturn serializerString;\n \t\t\t} catch (Exception e) {\n@@ -149,7 +149,7 @@\n \t\t\t\t\t\"Unable to generate a string representation of the serializer snapshot of '%s' \" +\n \t\t\t\t\t\t\"describing the class '%s' for the ANY type.\",\n \t\t\t\t\tserializer.getClass().getName(),\n-\t\t\t\t\tclazz.toString()));\n+\t\t\t\t\tclazz.toString()), e);\n \t\t\t}\n \t\t}\n \t\treturn serializerString;\n",
    "projectName": "apache.flink",
    "bugLineNum": 148,
    "bugNodeStartChar": 4632,
    "bugNodeLength": 236,
    "fixLineNum": 148,
    "fixNodeStartChar": 4632,
    "fixNodeLength": 239,
    "sourceBeforeFix": "new TableException(String.format(\"Unable to generate a string representation of the serializer snapshot of '%s' \" + \"describing the class '%s' for the ANY type.\",serializer.getClass().getName(),clazz.toString()))",
    "sourceAfterFix": "new TableException(String.format(\"Unable to generate a string representation of the serializer snapshot of '%s' \" + \"describing the class '%s' for the ANY type.\",serializer.getClass().getName(),clazz.toString()),e)"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "3b13b60dc127d075b378d2f2e0a02b1b738c1245",
    "fixCommitParentSHA1": "d7d24425e47408eb094b7f5aa6ecfcffd7f89db7",
    "bugFilePath": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/ClassDataTypeConverter.java",
    "fixPatch": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/ClassDataTypeConverter.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/ClassDataTypeConverter.java\nindex 599c8b2..a71c682 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/ClassDataTypeConverter.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/types/utils/ClassDataTypeConverter.java\n@@ -56,7 +56,7 @@\n \t\taddDefaultDataType(double.class, DataTypes.DOUBLE());\n \t\taddDefaultDataType(java.sql.Date.class, DataTypes.DATE());\n \t\taddDefaultDataType(java.time.LocalDate.class, DataTypes.DATE());\n-\t\taddDefaultDataType(java.sql.Time.class, DataTypes.TIME(9));\n+\t\taddDefaultDataType(java.sql.Time.class, DataTypes.TIME(3));\n \t\taddDefaultDataType(java.time.LocalTime.class, DataTypes.TIME(9));\n \t\taddDefaultDataType(java.sql.Timestamp.class, DataTypes.TIMESTAMP(9));\n \t\taddDefaultDataType(java.time.LocalDateTime.class, DataTypes.TIMESTAMP(9));\n",
    "projectName": "apache.flink",
    "bugLineNum": 59,
    "bugNodeStartChar": 2572,
    "bugNodeLength": 17,
    "fixLineNum": 59,
    "fixNodeStartChar": 2572,
    "fixNodeLength": 17,
    "sourceBeforeFix": "DataTypes.TIME(9)",
    "sourceAfterFix": "DataTypes.TIME(3)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "98239a3d325ccdbbc600a623c40892722188a3ee",
    "fixCommitParentSHA1": "12b8f56545fc0a3c69def2fc262d56c5fc80ca2a",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapPriorityQueueSet.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapPriorityQueueSet.java b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapPriorityQueueSet.java\nindex c0215ac..a51a0c1 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapPriorityQueueSet.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapPriorityQueueSet.java\n@@ -155,7 +155,7 @@\n \t}\n \n \tprivate int globalKeyGroupToLocalIndex(int keyGroup) {\n-\t\tcheckArgument(keyGroupRange.contains(keyGroup));\n+\t\tcheckArgument(keyGroupRange.contains(keyGroup), \"%s does not contain key group %s\", keyGroupRange, keyGroup);\n \t\treturn keyGroup - keyGroupRange.getStartKeyGroup();\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 158,
    "bugNodeStartChar": 5983,
    "bugNodeLength": 47,
    "fixLineNum": 158,
    "fixNodeStartChar": 5983,
    "fixNodeLength": 108,
    "sourceBeforeFix": "checkArgument(keyGroupRange.contains(keyGroup))",
    "sourceAfterFix": "checkArgument(keyGroupRange.contains(keyGroup),\"%s does not contain key group %s\",keyGroupRange,keyGroup)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "8a91b07d5ab85b6bf100a36c5e26e6297a7f3a2a",
    "fixCommitParentSHA1": "3422308f542222d4d4ce64231ba75ff39764bc8e",
    "bugFilePath": "flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBListState.java",
    "fixPatch": "diff --git a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBListState.java b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBListState.java\nindex 00123f5..c18adb1 100644\n--- a/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBListState.java\n+++ b/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/RocksDBListState.java\n@@ -148,7 +148,7 @@\n \t\t\t\treturn element;\n \t\t\t}\n \t\t} catch (IOException e) {\n-\t\t\tthrow new FlinkRuntimeException(\"Unexpected list element deserialization failure\");\n+\t\t\tthrow new FlinkRuntimeException(\"Unexpected list element deserialization failure\", e);\n \t\t}\n \t\treturn null;\n \t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 151,
    "bugNodeStartChar": 4966,
    "bugNodeLength": 76,
    "fixLineNum": 151,
    "fixNodeStartChar": 4966,
    "fixNodeLength": 79,
    "sourceBeforeFix": "new FlinkRuntimeException(\"Unexpected list element deserialization failure\")",
    "sourceAfterFix": "new FlinkRuntimeException(\"Unexpected list element deserialization failure\",e)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "6b7a9eb44a285097eee57a08732ce247ca8e3bc3",
    "fixCommitParentSHA1": "92f2d0d2a65fdbf3c2cb3abefb0a6276822271aa",
    "bugFilePath": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/expressions/TypeLiteralExpression.java",
    "fixPatch": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/expressions/TypeLiteralExpression.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/expressions/TypeLiteralExpression.java\nindex 45ff9b3..e7ff10c 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/expressions/TypeLiteralExpression.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/expressions/TypeLiteralExpression.java\n@@ -50,7 +50,7 @@\n \n \t@Override\n \tpublic <R> R accept(ExpressionVisitor<R> visitor) {\n-\t\treturn visitor.visit(this);\n+\t\treturn visitor.visitTypeLiteral(this);\n \t}\n \n \t@Override\n",
    "projectName": "apache.flink",
    "bugLineNum": 53,
    "bugNodeStartChar": 1666,
    "bugNodeLength": 19,
    "fixLineNum": 53,
    "fixNodeStartChar": 1666,
    "fixNodeLength": 30,
    "sourceBeforeFix": "visitor.visit(this)",
    "sourceAfterFix": "visitor.visitTypeLiteral(this)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "6b7a9eb44a285097eee57a08732ce247ca8e3bc3",
    "fixCommitParentSHA1": "92f2d0d2a65fdbf3c2cb3abefb0a6276822271aa",
    "bugFilePath": "flink-table/flink-table-common/src/main/java/org/apache/flink/table/expressions/TypeLiteralExpression.java",
    "fixPatch": "diff --git a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/expressions/TypeLiteralExpression.java b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/expressions/TypeLiteralExpression.java\nindex 45ff9b3..e7ff10c 100644\n--- a/flink-table/flink-table-common/src/main/java/org/apache/flink/table/expressions/TypeLiteralExpression.java\n+++ b/flink-table/flink-table-common/src/main/java/org/apache/flink/table/expressions/TypeLiteralExpression.java\n@@ -50,7 +50,7 @@\n \n \t@Override\n \tpublic <R> R accept(ExpressionVisitor<R> visitor) {\n-\t\treturn visitor.visit(this);\n+\t\treturn visitor.visitTypeLiteral(this);\n \t}\n \n \t@Override\n",
    "projectName": "apache.flink",
    "bugLineNum": 53,
    "bugNodeStartChar": 1666,
    "bugNodeLength": 19,
    "fixLineNum": 53,
    "fixNodeStartChar": 1666,
    "fixNodeLength": 30,
    "sourceBeforeFix": "visitor.visit(this)",
    "sourceAfterFix": "visitor.visitTypeLiteral(this)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "06f8fe87421493280f6602aeb73153e677844542",
    "fixCommitParentSHA1": "2d045731da193d28bb3e6b15fbee514ec9a0c69f",
    "bugFilePath": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\nindex 55360a4..f2c1f8b 100644\n--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n@@ -115,7 +115,7 @@\n \t\tAssert.assertEquals(1L, statuses.length);\n \t\tAssert.assertTrue(\n \t\t\t\tstatuses[0].getPath().getPath().startsWith(\n-\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).toString())\n+\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).getPath())\n \t\t);\n \t}\n \n@@ -184,7 +184,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n@@ -195,7 +195,7 @@\n \n \t\t// verify that the in-progress file is still there\n \t\tAssert.assertTrue(paths.iterator().next().startsWith(\n-\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").toString())));\n+\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").getPath())));\n \t}\n \n \t@Test\n@@ -258,7 +258,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 118,
    "bugNodeStartChar": 4579,
    "bugNodeLength": 65,
    "fixLineNum": 118,
    "fixNodeStartChar": 4579,
    "fixNodeLength": 64,
    "sourceBeforeFix": "(new Path(testBucket.getParent(),\".test.inprogress\")).toString()",
    "sourceAfterFix": "(new Path(testBucket.getParent(),\".test.inprogress\")).getPath()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "06f8fe87421493280f6602aeb73153e677844542",
    "fixCommitParentSHA1": "2d045731da193d28bb3e6b15fbee514ec9a0c69f",
    "bugFilePath": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\nindex 55360a4..f2c1f8b 100644\n--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n@@ -115,7 +115,7 @@\n \t\tAssert.assertEquals(1L, statuses.length);\n \t\tAssert.assertTrue(\n \t\t\t\tstatuses[0].getPath().getPath().startsWith(\n-\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).toString())\n+\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).getPath())\n \t\t);\n \t}\n \n@@ -184,7 +184,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n@@ -195,7 +195,7 @@\n \n \t\t// verify that the in-progress file is still there\n \t\tAssert.assertTrue(paths.iterator().next().startsWith(\n-\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").toString())));\n+\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").getPath())));\n \t}\n \n \t@Test\n@@ -258,7 +258,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 118,
    "bugNodeStartChar": 4579,
    "bugNodeLength": 65,
    "fixLineNum": 118,
    "fixNodeStartChar": 4579,
    "fixNodeLength": 64,
    "sourceBeforeFix": "(new Path(testBucket.getParent(),\".test.inprogress\")).toString()",
    "sourceAfterFix": "(new Path(testBucket.getParent(),\".test.inprogress\")).getPath()"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "06f8fe87421493280f6602aeb73153e677844542",
    "fixCommitParentSHA1": "2d045731da193d28bb3e6b15fbee514ec9a0c69f",
    "bugFilePath": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\nindex 55360a4..f2c1f8b 100644\n--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n@@ -115,7 +115,7 @@\n \t\tAssert.assertEquals(1L, statuses.length);\n \t\tAssert.assertTrue(\n \t\t\t\tstatuses[0].getPath().getPath().startsWith(\n-\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).toString())\n+\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).getPath())\n \t\t);\n \t}\n \n@@ -184,7 +184,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n@@ -195,7 +195,7 @@\n \n \t\t// verify that the in-progress file is still there\n \t\tAssert.assertTrue(paths.iterator().next().startsWith(\n-\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").toString())));\n+\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").getPath())));\n \t}\n \n \t@Test\n@@ -258,7 +258,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 187,
    "bugNodeStartChar": 7377,
    "bugNodeLength": 54,
    "fixLineNum": 187,
    "fixNodeStartChar": 7377,
    "fixNodeLength": 53,
    "sourceBeforeFix": "new Path(bucketPath,\"part-\" + i + '-'+ j).toString()",
    "sourceAfterFix": "new Path(bucketPath,\"part-\" + i + '-'+ j).getPath()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "06f8fe87421493280f6602aeb73153e677844542",
    "fixCommitParentSHA1": "2d045731da193d28bb3e6b15fbee514ec9a0c69f",
    "bugFilePath": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\nindex 55360a4..f2c1f8b 100644\n--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n@@ -115,7 +115,7 @@\n \t\tAssert.assertEquals(1L, statuses.length);\n \t\tAssert.assertTrue(\n \t\t\t\tstatuses[0].getPath().getPath().startsWith(\n-\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).toString())\n+\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).getPath())\n \t\t);\n \t}\n \n@@ -184,7 +184,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n@@ -195,7 +195,7 @@\n \n \t\t// verify that the in-progress file is still there\n \t\tAssert.assertTrue(paths.iterator().next().startsWith(\n-\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").toString())));\n+\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").getPath())));\n \t}\n \n \t@Test\n@@ -258,7 +258,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 187,
    "bugNodeStartChar": 7377,
    "bugNodeLength": 54,
    "fixLineNum": 187,
    "fixNodeStartChar": 7377,
    "fixNodeLength": 53,
    "sourceBeforeFix": "new Path(bucketPath,\"part-\" + i + '-'+ j).toString()",
    "sourceAfterFix": "new Path(bucketPath,\"part-\" + i + '-'+ j).getPath()"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "06f8fe87421493280f6602aeb73153e677844542",
    "fixCommitParentSHA1": "2d045731da193d28bb3e6b15fbee514ec9a0c69f",
    "bugFilePath": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\nindex 55360a4..f2c1f8b 100644\n--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n@@ -115,7 +115,7 @@\n \t\tAssert.assertEquals(1L, statuses.length);\n \t\tAssert.assertTrue(\n \t\t\t\tstatuses[0].getPath().getPath().startsWith(\n-\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).toString())\n+\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).getPath())\n \t\t);\n \t}\n \n@@ -184,7 +184,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n@@ -195,7 +195,7 @@\n \n \t\t// verify that the in-progress file is still there\n \t\tAssert.assertTrue(paths.iterator().next().startsWith(\n-\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").toString())));\n+\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").getPath())));\n \t}\n \n \t@Test\n@@ -258,7 +258,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 198,
    "bugNodeStartChar": 7707,
    "bugNodeLength": 65,
    "fixLineNum": 198,
    "fixNodeStartChar": 7707,
    "fixNodeLength": 64,
    "sourceBeforeFix": "new Path(testBucket.getParent(),\".test-2.inprogress\").toString()",
    "sourceAfterFix": "new Path(testBucket.getParent(),\".test-2.inprogress\").getPath()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "06f8fe87421493280f6602aeb73153e677844542",
    "fixCommitParentSHA1": "2d045731da193d28bb3e6b15fbee514ec9a0c69f",
    "bugFilePath": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\nindex 55360a4..f2c1f8b 100644\n--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n@@ -115,7 +115,7 @@\n \t\tAssert.assertEquals(1L, statuses.length);\n \t\tAssert.assertTrue(\n \t\t\t\tstatuses[0].getPath().getPath().startsWith(\n-\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).toString())\n+\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).getPath())\n \t\t);\n \t}\n \n@@ -184,7 +184,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n@@ -195,7 +195,7 @@\n \n \t\t// verify that the in-progress file is still there\n \t\tAssert.assertTrue(paths.iterator().next().startsWith(\n-\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").toString())));\n+\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").getPath())));\n \t}\n \n \t@Test\n@@ -258,7 +258,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 198,
    "bugNodeStartChar": 7707,
    "bugNodeLength": 65,
    "fixLineNum": 198,
    "fixNodeStartChar": 7707,
    "fixNodeLength": 64,
    "sourceBeforeFix": "new Path(testBucket.getParent(),\".test-2.inprogress\").toString()",
    "sourceAfterFix": "new Path(testBucket.getParent(),\".test-2.inprogress\").getPath()"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "06f8fe87421493280f6602aeb73153e677844542",
    "fixCommitParentSHA1": "2d045731da193d28bb3e6b15fbee514ec9a0c69f",
    "bugFilePath": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\nindex 55360a4..f2c1f8b 100644\n--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n@@ -115,7 +115,7 @@\n \t\tAssert.assertEquals(1L, statuses.length);\n \t\tAssert.assertTrue(\n \t\t\t\tstatuses[0].getPath().getPath().startsWith(\n-\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).toString())\n+\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).getPath())\n \t\t);\n \t}\n \n@@ -184,7 +184,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n@@ -195,7 +195,7 @@\n \n \t\t// verify that the in-progress file is still there\n \t\tAssert.assertTrue(paths.iterator().next().startsWith(\n-\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").toString())));\n+\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").getPath())));\n \t}\n \n \t@Test\n@@ -258,7 +258,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 261,
    "bugNodeStartChar": 10312,
    "bugNodeLength": 54,
    "fixLineNum": 261,
    "fixNodeStartChar": 10312,
    "fixNodeLength": 53,
    "sourceBeforeFix": "new Path(bucketPath,\"test-\" + i + '-'+ j).toString()",
    "sourceAfterFix": "new Path(bucketPath,\"test-\" + i + '-'+ j).getPath()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "06f8fe87421493280f6602aeb73153e677844542",
    "fixCommitParentSHA1": "2d045731da193d28bb3e6b15fbee514ec9a0c69f",
    "bugFilePath": "flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\nindex 55360a4..f2c1f8b 100644\n--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/api/functions/sink/filesystem/BucketStateSerializerTest.java\n@@ -115,7 +115,7 @@\n \t\tAssert.assertEquals(1L, statuses.length);\n \t\tAssert.assertTrue(\n \t\t\t\tstatuses[0].getPath().getPath().startsWith(\n-\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).toString())\n+\t\t\t\t\t\t(new Path(testBucket.getParent(), \".test.inprogress\")).getPath())\n \t\t);\n \t}\n \n@@ -184,7 +184,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"part-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n@@ -195,7 +195,7 @@\n \n \t\t// verify that the in-progress file is still there\n \t\tAssert.assertTrue(paths.iterator().next().startsWith(\n-\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").toString())));\n+\t\t\t\t(new Path(testBucket.getParent(), \".test-2.inprogress\").getPath())));\n \t}\n \n \t@Test\n@@ -258,7 +258,7 @@\n \n \t\tfor (int i = 0; i < noOfTasks; i++) {\n \t\t\tfor (int j = 0; j < 2 + i; j++) {\n-\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).toString();\n+\t\t\t\tfinal String part = new Path(bucketPath, \"test-\" + i + '-' + j).getPath();\n \t\t\t\tAssert.assertTrue(paths.contains(part));\n \t\t\t\tpaths.remove(part);\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 261,
    "bugNodeStartChar": 10312,
    "bugNodeLength": 54,
    "fixLineNum": 261,
    "fixNodeStartChar": 10312,
    "fixNodeLength": 53,
    "sourceBeforeFix": "new Path(bucketPath,\"test-\" + i + '-'+ j).toString()",
    "sourceAfterFix": "new Path(bucketPath,\"test-\" + i + '-'+ j).getPath()"
  },
  {
    "bugType": "CHANGE_OPERATOR",
    "fixCommitSHA1": "aee1f182e8fbd311713d1661ab61755b069b1a6c",
    "fixCommitParentSHA1": "21cff1d00d4866614c005e005000b4e8ad783142",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/state/ttl/TtlStateTestBase.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/state/ttl/TtlStateTestBase.java b/flink-runtime/src/test/java/org/apache/flink/runtime/state/ttl/TtlStateTestBase.java\nindex e6d5ba3..5c92e67 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/state/ttl/TtlStateTestBase.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/state/ttl/TtlStateTestBase.java\n@@ -515,7 +515,7 @@\n \t\t// trigger more cleanup by doing something out side of INC_CLEANUP_ALL_KEYS\n \t\tfor (int i = INC_CLEANUP_ALL_KEYS; i < INC_CLEANUP_ALL_KEYS * 2; i++) {\n \t\t\tsbetc.setCurrentKey(Integer.toString(i));\n-\t\t\tif (i / 2 == 0) {\n+\t\t\tif (i % 2 == 0) {\n \t\t\t\tctx().get();\n \t\t\t} else {\n \t\t\t\tctx().update(ctx().updateEmpty);\n",
    "projectName": "apache.flink",
    "bugLineNum": 518,
    "bugNodeStartChar": 17407,
    "bugNodeLength": 5,
    "fixLineNum": 518,
    "fixNodeStartChar": 17407,
    "fixNodeLength": 5,
    "sourceBeforeFix": "i / 2",
    "sourceAfterFix": "i % 2"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "ca94fbcd444c6d36e7c779def6f364c69f1cf40d",
    "fixCommitParentSHA1": "02ff4bfe90d8e8b896c9f1a1bdbe8d43a48f5de7",
    "bugFilePath": "flink-core/src/test/java/org/apache/flink/api/common/typeutils/TypeSerializerSnapshotMigrationTestBase.java",
    "fixPatch": "diff --git a/flink-core/src/test/java/org/apache/flink/api/common/typeutils/TypeSerializerSnapshotMigrationTestBase.java b/flink-core/src/test/java/org/apache/flink/api/common/typeutils/TypeSerializerSnapshotMigrationTestBase.java\nindex 2e363fb..0548d2f 100644\n--- a/flink-core/src/test/java/org/apache/flink/api/common/typeutils/TypeSerializerSnapshotMigrationTestBase.java\n+++ b/flink-core/src/test/java/org/apache/flink/api/common/typeutils/TypeSerializerSnapshotMigrationTestBase.java\n@@ -197,8 +197,7 @@\n \t/**\n \t * Test Specification.\n \t */\n-\t@SuppressWarnings(\"WeakerAccess\")\n-\tprotected static final class TestSpecification<T> {\n+\tpublic static final class TestSpecification<T> {\n \t\tprivate final Class<? extends TypeSerializer<T>> serializerType;\n \t\tprivate final Class<? extends TypeSerializerSnapshot<T>> snapshotClass;\n \t\tprivate final String name;\n@@ -298,7 +297,7 @@\n \t * multiple test migration versions. For each test specification added,\n \t * an entry will be added for each specified migration version.\n \t */\n-\tprotected static final class TestSpecifications {\n+\tpublic static final class TestSpecifications {\n \n \t\tprivate static final int DEFAULT_TEST_DATA_COUNT = 10;\n \t\tprivate static final String DEFAULT_SNAPSHOT_FILENAME_FORMAT = \"flink-%s-%s-snapshot\";\n",
    "projectName": "apache.flink",
    "bugLineNum": 197,
    "bugNodeStartChar": 7470,
    "bugNodeLength": 3355,
    "fixLineNum": 197,
    "fixNodeStartChar": 7470,
    "fixNodeLength": 3317,
    "sourceBeforeFix": "28",
    "sourceAfterFix": "25"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "642ff291920b7eec294a37785f2fd0437c8c56a1",
    "fixCommitParentSHA1": "79905ea14af301ea511f0b7f6f11a8080000ca14",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/testutils/CommonTestUtils.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/CommonTestUtils.java b/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/CommonTestUtils.java\nindex 18c4228..da07dfd 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/CommonTestUtils.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/testutils/CommonTestUtils.java\n@@ -160,7 +160,7 @@\n \n \tpublic static void waitUntilCondition(SupplierWithException<Boolean, Exception> condition, Deadline timeout, long retryIntervalMillis) throws Exception {\n \t\twhile (timeout.hasTimeLeft() && !condition.get()) {\n-\t\t\tThread.sleep(Math.min(RETRY_INTERVAL, timeout.timeLeft().toMillis()));\n+\t\t\tThread.sleep(Math.min(retryIntervalMillis, timeout.timeLeft().toMillis()));\n \t\t}\n \n \t\tif (!timeout.hasTimeLeft()) {\n",
    "projectName": "apache.flink",
    "bugLineNum": 163,
    "bugNodeStartChar": 5121,
    "bugNodeLength": 55,
    "fixLineNum": 163,
    "fixNodeStartChar": 5121,
    "fixNodeLength": 60,
    "sourceBeforeFix": "Math.min(RETRY_INTERVAL,timeout.timeLeft().toMillis())",
    "sourceAfterFix": "Math.min(retryIntervalMillis,timeout.timeLeft().toMillis())"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "c7d4e85765529189d759c7bed44507ba1ccacdee",
    "fixCommitParentSHA1": "9839126b7d05eb4b6966e77e0893fb09b445803f",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/MainThreadValidationTest.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/MainThreadValidationTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/MainThreadValidationTest.java\nindex 507c1c3..c13abdd 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/MainThreadValidationTest.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/MainThreadValidationTest.java\n@@ -30,6 +30,10 @@\n \n import static org.junit.Assert.assertTrue;\n \n+/**\n+ * Tests that the {@link AkkaRpcService} runs all RPCs in the {@link AkkaRpcActor}'s\n+ * main thread.\n+ */\n public class MainThreadValidationTest extends TestLogger {\n \n \t@Test\n@@ -80,10 +84,9 @@\n \t\tvoid someConcurrencyCriticalFunction();\n \t}\n \n-\t@SuppressWarnings(\"unused\")\n-\tpublic static class TestEndpoint extends RpcEndpoint implements TestGateway {\n+\tprivate static class TestEndpoint extends RpcEndpoint implements TestGateway {\n \n-\t\tpublic TestEndpoint(RpcService rpcService) {\n+\t\tprivate TestEndpoint(RpcService rpcService) {\n \t\t\tsuper(rpcService);\n \t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 83,
    "bugNodeStartChar": 2606,
    "bugNodeLength": 395,
    "fixLineNum": 83,
    "fixNodeStartChar": 2606,
    "fixNodeLength": 367,
    "sourceBeforeFix": "9",
    "sourceAfterFix": "10"
  },
  {
    "bugType": "CHANGE_OPERATOR",
    "fixCommitSHA1": "1c3c663ba7d55fcd33af1502ca426274d51d2ba1",
    "fixCommitParentSHA1": "d3f0075b041c8fd681a776b3278d12c4483a48ac",
    "bugFilePath": "flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/descriptors/DescriptorProperties.java",
    "fixPatch": "diff --git a/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/descriptors/DescriptorProperties.java b/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/descriptors/DescriptorProperties.java\nindex c476981..7628912 100644\n--- a/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/descriptors/DescriptorProperties.java\n+++ b/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/descriptors/DescriptorProperties.java\n@@ -938,7 +938,7 @@\n \t\t}\n \n \t\t// validate\n-\t\tfor (int i = 0; i < maxIndex; i++) {\n+\t\tfor (int i = 0; i <= maxIndex; i++) {\n \t\t\tfor (Map.Entry<String, Consumer<String>> subKey : subKeyValidation.entrySet()) {\n \t\t\t\tfinal String fullKey = key + '.' + i + '.' + subKey.getKey();\n \t\t\t\tif (properties.containsKey(fullKey)) {\n@@ -1134,7 +1134,7 @@\n \t\t}\n \n \t\t// validate array elements\n-\t\tfor (int i = 0; i < maxIndex; i++) {\n+\t\tfor (int i = 0; i <= maxIndex; i++) {\n \t\t\tfinal String fullKey = key + '.' + i;\n \t\t\tif (properties.containsKey(fullKey)) {\n \t\t\t\t// run validation logic\n",
    "projectName": "apache.flink",
    "bugLineNum": 941,
    "bugNodeStartChar": 27252,
    "bugNodeLength": 12,
    "fixLineNum": 941,
    "fixNodeStartChar": 27252,
    "fixNodeLength": 13,
    "sourceBeforeFix": "i < maxIndex",
    "sourceAfterFix": "i <= maxIndex"
  },
  {
    "bugType": "CHANGE_OPERATOR",
    "fixCommitSHA1": "1c3c663ba7d55fcd33af1502ca426274d51d2ba1",
    "fixCommitParentSHA1": "d3f0075b041c8fd681a776b3278d12c4483a48ac",
    "bugFilePath": "flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/descriptors/DescriptorProperties.java",
    "fixPatch": "diff --git a/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/descriptors/DescriptorProperties.java b/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/descriptors/DescriptorProperties.java\nindex c476981..7628912 100644\n--- a/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/descriptors/DescriptorProperties.java\n+++ b/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/descriptors/DescriptorProperties.java\n@@ -938,7 +938,7 @@\n \t\t}\n \n \t\t// validate\n-\t\tfor (int i = 0; i < maxIndex; i++) {\n+\t\tfor (int i = 0; i <= maxIndex; i++) {\n \t\t\tfor (Map.Entry<String, Consumer<String>> subKey : subKeyValidation.entrySet()) {\n \t\t\t\tfinal String fullKey = key + '.' + i + '.' + subKey.getKey();\n \t\t\t\tif (properties.containsKey(fullKey)) {\n@@ -1134,7 +1134,7 @@\n \t\t}\n \n \t\t// validate array elements\n-\t\tfor (int i = 0; i < maxIndex; i++) {\n+\t\tfor (int i = 0; i <= maxIndex; i++) {\n \t\t\tfinal String fullKey = key + '.' + i;\n \t\t\tif (properties.containsKey(fullKey)) {\n \t\t\t\t// run validation logic\n",
    "projectName": "apache.flink",
    "bugLineNum": 1137,
    "bugNodeStartChar": 32971,
    "bugNodeLength": 12,
    "fixLineNum": 1137,
    "fixNodeStartChar": 32971,
    "fixNodeLength": 13,
    "sourceBeforeFix": "i < maxIndex",
    "sourceAfterFix": "i <= maxIndex"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "1ccc740dcb588e324c33571df8d7475c09bf962a",
    "fixCommitParentSHA1": "33f6d067225967c8335c0d61920c19a00dd62786",
    "bugFilePath": "flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java",
    "fixPatch": "diff --git a/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java b/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java\nindex 6ffd5d2..2391d54 100644\n--- a/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java\n+++ b/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java\n@@ -378,9 +378,9 @@\n \t\ttimeoutPattern4.put(\"start\", Collections.singletonList(new Event(2, \"start\", 1.0)));\n \n \t\texpectedTimeoutPatterns.add(Tuple2.of(timeoutPattern1, 11L));\n-\t\texpectedTimeoutPatterns.add(Tuple2.of(timeoutPattern2, 13L));\n+\t\texpectedTimeoutPatterns.add(Tuple2.of(timeoutPattern2, 12L));\n \t\texpectedTimeoutPatterns.add(Tuple2.of(timeoutPattern3, 11L));\n-\t\texpectedTimeoutPatterns.add(Tuple2.of(timeoutPattern4, 13L));\n+\t\texpectedTimeoutPatterns.add(Tuple2.of(timeoutPattern4, 12L));\n \n \t\tPattern<Event, ?> pattern = Pattern.<Event>begin(\"start\").where(new SimpleCondition<Event>() {\n \t\t\tprivate static final long serialVersionUID = 7907391379273505897L;\n",
    "projectName": "apache.flink",
    "bugLineNum": 381,
    "bugNodeStartChar": 13503,
    "bugNodeLength": 31,
    "fixLineNum": 381,
    "fixNodeStartChar": 13503,
    "fixNodeLength": 31,
    "sourceBeforeFix": "Tuple2.of(timeoutPattern2,13L)",
    "sourceAfterFix": "Tuple2.of(timeoutPattern2,12L)"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "1ccc740dcb588e324c33571df8d7475c09bf962a",
    "fixCommitParentSHA1": "33f6d067225967c8335c0d61920c19a00dd62786",
    "bugFilePath": "flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java",
    "fixPatch": "diff --git a/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java b/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java\nindex 6ffd5d2..2391d54 100644\n--- a/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java\n+++ b/flink-libraries/flink-cep/src/test/java/org/apache/flink/cep/nfa/NFAITCase.java\n@@ -378,9 +378,9 @@\n \t\ttimeoutPattern4.put(\"start\", Collections.singletonList(new Event(2, \"start\", 1.0)));\n \n \t\texpectedTimeoutPatterns.add(Tuple2.of(timeoutPattern1, 11L));\n-\t\texpectedTimeoutPatterns.add(Tuple2.of(timeoutPattern2, 13L));\n+\t\texpectedTimeoutPatterns.add(Tuple2.of(timeoutPattern2, 12L));\n \t\texpectedTimeoutPatterns.add(Tuple2.of(timeoutPattern3, 11L));\n-\t\texpectedTimeoutPatterns.add(Tuple2.of(timeoutPattern4, 13L));\n+\t\texpectedTimeoutPatterns.add(Tuple2.of(timeoutPattern4, 12L));\n \n \t\tPattern<Event, ?> pattern = Pattern.<Event>begin(\"start\").where(new SimpleCondition<Event>() {\n \t\t\tprivate static final long serialVersionUID = 7907391379273505897L;\n",
    "projectName": "apache.flink",
    "bugLineNum": 383,
    "bugNodeStartChar": 13631,
    "bugNodeLength": 31,
    "fixLineNum": 383,
    "fixNodeStartChar": 13631,
    "fixNodeLength": 31,
    "sourceBeforeFix": "Tuple2.of(timeoutPattern4,13L)",
    "sourceAfterFix": "Tuple2.of(timeoutPattern4,12L)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "7e86a80b714dac6dc0ae8fda0155d655f623d31e",
    "fixCommitParentSHA1": "b480971a1b5d70b9de692f38b69b5578b8ad2355",
    "bugFilePath": "flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/utils/EncodingUtils.java",
    "fixPatch": "diff --git a/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/utils/EncodingUtils.java b/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/utils/EncodingUtils.java\nindex 47aac25..5531082 100644\n--- a/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/utils/EncodingUtils.java\n+++ b/flink-libraries/flink-table-common/src/main/java/org/apache/flink/table/utils/EncodingUtils.java\n@@ -76,7 +76,7 @@\n \t\t\treturn instance;\n \t\t} catch (Exception e) {\n \t\t\tthrow new ValidationException(\n-\t\t\t\t\"Unable to deserialize string '\" + base64String + \"' of base class '\" + baseClass.getName() + \"'.\");\n+\t\t\t\t\"Unable to deserialize string '\" + base64String + \"' of base class '\" + baseClass.getName() + \"'.\", e);\n \t\t}\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 78,
    "bugNodeStartChar": 3011,
    "bugNodeLength": 128,
    "fixLineNum": 78,
    "fixNodeStartChar": 3011,
    "fixNodeLength": 131,
    "sourceBeforeFix": "new ValidationException(\"Unable to deserialize string '\" + base64String + \"' of base class '\"+ baseClass.getName()+ \"'.\")",
    "sourceAfterFix": "new ValidationException(\"Unable to deserialize string '\" + base64String + \"' of base class '\"+ baseClass.getName()+ \"'.\",e)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "a13119af042b2db29aae5f04382cc62102b21899",
    "fixCommitParentSHA1": "f1eaf060d77d7942a5594a433bceb4fc740e7f97",
    "bugFilePath": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamConfig.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamConfig.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamConfig.java\nindex 85c676c..457eee2 100644\n--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamConfig.java\n+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamConfig.java\n@@ -233,7 +233,7 @@\n \t\t\t\t\t\t\t\"\\nClass was actually found in classloader - deserialization issue.\" :\n \t\t\t\t\t\t\t\"\\nClass not resolvable through given classloader.\");\n \n-\t\t\tthrow new StreamTaskException(exceptionMessage);\n+\t\t\tthrow new StreamTaskException(exceptionMessage, e);\n \t\t}\n \t\tcatch (Exception e) {\n \t\t\tthrow new StreamTaskException(\"Cannot instantiate user function.\", e);\n",
    "projectName": "apache.flink",
    "bugLineNum": 236,
    "bugNodeStartChar": 9184,
    "bugNodeLength": 41,
    "fixLineNum": 236,
    "fixNodeStartChar": 9184,
    "fixNodeLength": 44,
    "sourceBeforeFix": "new StreamTaskException(exceptionMessage)",
    "sourceAfterFix": "new StreamTaskException(exceptionMessage,e)"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "ce4fe6e385fca71f097d6ed5dd694251403e15c5",
    "fixCommitParentSHA1": "2fe70a5c1256eed9781c74e48574da54d976b8cc",
    "bugFilePath": "flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/relational/TPCHQuery3.java",
    "fixPatch": "diff --git a/flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/relational/TPCHQuery3.java b/flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/relational/TPCHQuery3.java\nindex 66213e3..cf9f078 100644\n--- a/flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/relational/TPCHQuery3.java\n+++ b/flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/relational/TPCHQuery3.java\n@@ -238,7 +238,10 @@\n \t\t}\n \t}\n \n-\tprivate static class ShippingPriorityItem extends Tuple4<Long, Double, String, Long> {\n+\t/**\n+\t * ShippingPriorityItem.\n+\t */\n+\tpublic static class ShippingPriorityItem extends Tuple4<Long, Double, String, Long> {\n \n \t\tpublic ShippingPriorityItem() {}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 241,
    "bugNodeStartChar": 7714,
    "bugNodeLength": 695,
    "fixLineNum": 241,
    "fixNodeStartChar": 7714,
    "fixNodeLength": 730,
    "sourceBeforeFix": "10",
    "sourceAfterFix": "9"
  },
  {
    "bugType": "SWAP_BOOLEAN_LITERAL",
    "fixCommitSHA1": "2f9d955099f62c12ced65ab9f9cc6ca39a62f469",
    "fixCommitParentSHA1": "ac74a0ce6297ce3754744ceb07107f6024c1a163",
    "bugFilePath": "flink-core/src/main/java/org/apache/flink/api/common/typeutils/TypeSerializerSnapshotSerializationUtil.java",
    "fixPatch": "diff --git a/flink-core/src/main/java/org/apache/flink/api/common/typeutils/TypeSerializerSnapshotSerializationUtil.java b/flink-core/src/main/java/org/apache/flink/api/common/typeutils/TypeSerializerSnapshotSerializationUtil.java\nindex 110b2ab..0bcff93 100644\n--- a/flink-core/src/main/java/org/apache/flink/api/common/typeutils/TypeSerializerSnapshotSerializationUtil.java\n+++ b/flink-core/src/main/java/org/apache/flink/api/common/typeutils/TypeSerializerSnapshotSerializationUtil.java\n@@ -149,7 +149,7 @@\n \t\t\tClass<? extends TypeSerializerSnapshot> serializerConfigSnapshotClass;\n \t\t\ttry {\n \t\t\t\tserializerConfigSnapshotClass = (Class<? extends TypeSerializerSnapshot>)\n-\t\t\t\t\tClass.forName(serializerConfigClassname, true, userCodeClassLoader);\n+\t\t\t\t\tClass.forName(serializerConfigClassname, false, userCodeClassLoader);\n \t\t\t} catch (ClassNotFoundException e) {\n \t\t\t\tthrow new IOException(\n \t\t\t\t\t\"Could not find requested TypeSerializerConfigSnapshot class \"\n",
    "projectName": "apache.flink",
    "bugLineNum": 152,
    "bugNodeStartChar": 6032,
    "bugNodeLength": 67,
    "fixLineNum": 152,
    "fixNodeStartChar": 6032,
    "fixNodeLength": 68,
    "sourceBeforeFix": "Class.forName(serializerConfigClassname,true,userCodeClassLoader)",
    "sourceAfterFix": "Class.forName(serializerConfigClassname,false,userCodeClassLoader)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "bb562c242d8b01af3b54696ee9bb81d03fdeb66a",
    "fixCommitParentSHA1": "d393a713adc87f73572a23389eef9cf25e8d644c",
    "bugFilePath": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamContextEnvironment.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamContextEnvironment.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamContextEnvironment.java\nindex 010628f..c0216e5 100644\n--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamContextEnvironment.java\n+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamContextEnvironment.java\n@@ -48,7 +48,7 @@\n \n \t@Override\n \tpublic JobExecutionResult execute(String jobName) throws Exception {\n-\t\tPreconditions.checkNotNull(\"Streaming Job name should not be null.\");\n+\t\tPreconditions.checkNotNull(jobName, \"Streaming Job name should not be null.\");\n \n \t\tStreamGraph streamGraph = this.getStreamGraph();\n \t\tstreamGraph.setJobName(jobName);\n",
    "projectName": "apache.flink",
    "bugLineNum": 51,
    "bugNodeStartChar": 1981,
    "bugNodeLength": 68,
    "fixLineNum": 51,
    "fixNodeStartChar": 1981,
    "fixNodeLength": 77,
    "sourceBeforeFix": "Preconditions.checkNotNull(\"Streaming Job name should not be null.\")",
    "sourceAfterFix": "Preconditions.checkNotNull(jobName,\"Streaming Job name should not be null.\")"
  },
  {
    "bugType": "CHANGE_OPERATOR",
    "fixCommitSHA1": "7d4fcae2f04b29fb14e54346d614ebd1021ae0f1",
    "fixCommitParentSHA1": "020f3299ebccc36297a0d3db425cc5f44c70a8b7",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/HeartbeatManagerImpl.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/HeartbeatManagerImpl.java b/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/HeartbeatManagerImpl.java\nindex 42268fc..242fbaa 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/HeartbeatManagerImpl.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/heartbeat/HeartbeatManagerImpl.java\n@@ -277,7 +277,7 @@\n \t\t\tthis.scheduledExecutor = Preconditions.checkNotNull(scheduledExecutor);\n \t\t\tthis.heartbeatListener = Preconditions.checkNotNull(heartbeatListener);\n \n-\t\t\tPreconditions.checkArgument(heartbeatTimeoutIntervalMs >= 0L, \"The heartbeat timeout interval has to be larger than 0.\");\n+\t\t\tPreconditions.checkArgument(heartbeatTimeoutIntervalMs > 0L, \"The heartbeat timeout interval has to be larger than 0.\");\n \t\t\tthis.heartbeatTimeoutIntervalMs = heartbeatTimeoutIntervalMs;\n \n \t\t\tlastHeartbeat = 0L;\n",
    "projectName": "apache.flink",
    "bugLineNum": 280,
    "bugNodeStartChar": 9654,
    "bugNodeLength": 32,
    "fixLineNum": 280,
    "fixNodeStartChar": 9654,
    "fixNodeLength": 31,
    "sourceBeforeFix": "heartbeatTimeoutIntervalMs >= 0L",
    "sourceAfterFix": "heartbeatTimeoutIntervalMs > 0L"
  },
  {
    "bugType": "SWAP_ARGUMENTS",
    "fixCommitSHA1": "056486a1b81e9648a6d3dc795e7e2c6976f8388c",
    "fixCommitParentSHA1": "690bc370e19d8003add4e41c05acfb4dccc662b4",
    "bugFilePath": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java\nindex b6fff03..0406afc 100644\n--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java\n+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/sink/filesystem/StreamingFileSink.java\n@@ -332,7 +332,7 @@\n \t\tfinal int subtaskIndex = getRuntimeContext().getIndexOfThisSubtask();\n \n \t\t// setting the values in the bucketer context\n-\t\tbucketerContext.update(context.timestamp(), currentProcessingTime, context.currentWatermark());\n+\t\tbucketerContext.update(context.timestamp(), context.currentWatermark(), currentProcessingTime);\n \n \t\tfinal String bucketId = bucketer.getBucketId(value, bucketerContext);\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 335,
    "bugNodeStartChar": 13583,
    "bugNodeLength": 94,
    "fixLineNum": 335,
    "fixNodeStartChar": 13583,
    "fixNodeLength": 94,
    "sourceBeforeFix": "bucketerContext.update(context.timestamp(),currentProcessingTime,context.currentWatermark())",
    "sourceAfterFix": "bucketerContext.update(context.timestamp(),context.currentWatermark(),currentProcessingTime)"
  },
  {
    "bugType": "CHANGE_CALLER_IN_FUNCTION_CALL",
    "fixCommitSHA1": "cad6e4d396e7b901b8c83257312860021f01c060",
    "fixCommitParentSHA1": "5ee5dbf3dd9e5240ed13c8c3eaff6cca158010b3",
    "bugFilePath": "flink-queryable-state/flink-queryable-state-client-java/src/main/java/org/apache/flink/queryablestate/network/AbstractServerHandler.java",
    "fixPatch": "diff --git a/flink-queryable-state/flink-queryable-state-client-java/src/main/java/org/apache/flink/queryablestate/network/AbstractServerHandler.java b/flink-queryable-state/flink-queryable-state-client-java/src/main/java/org/apache/flink/queryablestate/network/AbstractServerHandler.java\nindex b2f7a47..fb835e3 100644\n--- a/flink-queryable-state/flink-queryable-state-client-java/src/main/java/org/apache/flink/queryablestate/network/AbstractServerHandler.java\n+++ b/flink-queryable-state/flink-queryable-state-client-java/src/main/java/org/apache/flink/queryablestate/network/AbstractServerHandler.java\n@@ -165,7 +165,7 @@\n \t@Override\n \tpublic void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {\n \t\tfinal String msg = \"Exception in server pipeline. Caused by: \" + ExceptionUtils.stringifyException(cause);\n-\t\tfinal ByteBuf err = serializer.serializeServerFailure(ctx.alloc(), new RuntimeException(msg));\n+\t\tfinal ByteBuf err = MessageSerializer.serializeServerFailure(ctx.alloc(), new RuntimeException(msg));\n \n \t\tLOG.debug(msg);\n \t\tctx.writeAndFlush(err).addListener(ChannelFutureListener.CLOSE);\n",
    "projectName": "apache.flink",
    "bugLineNum": 168,
    "bugNodeStartChar": 6248,
    "bugNodeLength": 73,
    "fixLineNum": 168,
    "fixNodeStartChar": 6248,
    "fixNodeLength": 80,
    "sourceBeforeFix": "serializer.serializeServerFailure(ctx.alloc(),new RuntimeException(msg))",
    "sourceAfterFix": "MessageSerializer.serializeServerFailure(ctx.alloc(),new RuntimeException(msg))"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "cad6e4d396e7b901b8c83257312860021f01c060",
    "fixCommitParentSHA1": "5ee5dbf3dd9e5240ed13c8c3eaff6cca158010b3",
    "bugFilePath": "flink-queryable-state/flink-queryable-state-client-java/src/main/java/org/apache/flink/queryablestate/network/AbstractServerHandler.java",
    "fixPatch": "diff --git a/flink-queryable-state/flink-queryable-state-client-java/src/main/java/org/apache/flink/queryablestate/network/AbstractServerHandler.java b/flink-queryable-state/flink-queryable-state-client-java/src/main/java/org/apache/flink/queryablestate/network/AbstractServerHandler.java\nindex b2f7a47..fb835e3 100644\n--- a/flink-queryable-state/flink-queryable-state-client-java/src/main/java/org/apache/flink/queryablestate/network/AbstractServerHandler.java\n+++ b/flink-queryable-state/flink-queryable-state-client-java/src/main/java/org/apache/flink/queryablestate/network/AbstractServerHandler.java\n@@ -165,7 +165,7 @@\n \t@Override\n \tpublic void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception {\n \t\tfinal String msg = \"Exception in server pipeline. Caused by: \" + ExceptionUtils.stringifyException(cause);\n-\t\tfinal ByteBuf err = serializer.serializeServerFailure(ctx.alloc(), new RuntimeException(msg));\n+\t\tfinal ByteBuf err = MessageSerializer.serializeServerFailure(ctx.alloc(), new RuntimeException(msg));\n \n \t\tLOG.debug(msg);\n \t\tctx.writeAndFlush(err).addListener(ChannelFutureListener.CLOSE);\n",
    "projectName": "apache.flink",
    "bugLineNum": 168,
    "bugNodeStartChar": 6248,
    "bugNodeLength": 73,
    "fixLineNum": 168,
    "fixNodeStartChar": 6248,
    "fixNodeLength": 80,
    "sourceBeforeFix": "serializer.serializeServerFailure(ctx.alloc(),new RuntimeException(msg))",
    "sourceAfterFix": "MessageSerializer.serializeServerFailure(ctx.alloc(),new RuntimeException(msg))"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "62bf8fd4813271c64afbb1509b31811c79246bce",
    "fixCommitParentSHA1": "6d002941f2b8a5501ed637dd12f6a8c367d737e2",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/taskmanager/AbstractTaskManagerFileHandler.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/taskmanager/AbstractTaskManagerFileHandler.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/taskmanager/AbstractTaskManagerFileHandler.java\nindex 5b5d97d..83fab69 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/taskmanager/AbstractTaskManagerFileHandler.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/taskmanager/AbstractTaskManagerFileHandler.java\n@@ -155,7 +155,7 @@\n \t\tresultFuture.whenComplete(\n \t\t\t(Void ignored, Throwable throwable) -> {\n \t\t\t\tif (throwable != null) {\n-\t\t\t\t\tlog.debug(\"Failed to transfer file from TaskExecutor {}.\", taskManagerId, throwable);\n+\t\t\t\t\tlog.error(\"Failed to transfer file from TaskExecutor {}.\", taskManagerId, throwable);\n \t\t\t\t\tfileBlobKeys.invalidate(taskManagerId);\n \n \t\t\t\t\tfinal Throwable strippedThrowable = ExceptionUtils.stripCompletionException(throwable);\n",
    "projectName": "apache.flink",
    "bugLineNum": 158,
    "bugNodeStartChar": 7733,
    "bugNodeLength": 84,
    "fixLineNum": 158,
    "fixNodeStartChar": 7733,
    "fixNodeLength": 84,
    "sourceBeforeFix": "log.debug(\"Failed to transfer file from TaskExecutor {}.\",taskManagerId,throwable)",
    "sourceAfterFix": "log.error(\"Failed to transfer file from TaskExecutor {}.\",taskManagerId,throwable)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "62bf8fd4813271c64afbb1509b31811c79246bce",
    "fixCommitParentSHA1": "6d002941f2b8a5501ed637dd12f6a8c367d737e2",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/taskmanager/AbstractTaskManagerFileHandler.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/taskmanager/AbstractTaskManagerFileHandler.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/taskmanager/AbstractTaskManagerFileHandler.java\nindex 5b5d97d..83fab69 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/taskmanager/AbstractTaskManagerFileHandler.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/taskmanager/AbstractTaskManagerFileHandler.java\n@@ -155,7 +155,7 @@\n \t\tresultFuture.whenComplete(\n \t\t\t(Void ignored, Throwable throwable) -> {\n \t\t\t\tif (throwable != null) {\n-\t\t\t\t\tlog.debug(\"Failed to transfer file from TaskExecutor {}.\", taskManagerId, throwable);\n+\t\t\t\t\tlog.error(\"Failed to transfer file from TaskExecutor {}.\", taskManagerId, throwable);\n \t\t\t\t\tfileBlobKeys.invalidate(taskManagerId);\n \n \t\t\t\t\tfinal Throwable strippedThrowable = ExceptionUtils.stripCompletionException(throwable);\n",
    "projectName": "apache.flink",
    "bugLineNum": 158,
    "bugNodeStartChar": 7733,
    "bugNodeLength": 84,
    "fixLineNum": 158,
    "fixNodeStartChar": 7733,
    "fixNodeLength": 84,
    "sourceBeforeFix": "log.debug(\"Failed to transfer file from TaskExecutor {}.\",taskManagerId,throwable)",
    "sourceAfterFix": "log.error(\"Failed to transfer file from TaskExecutor {}.\",taskManagerId,throwable)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "c80e76bd7ccfe864566bdbffd6dcb6549709ea73",
    "fixCommitParentSHA1": "1c5a929c7ad6e40939e1170cdaf19c6ffd64f583",
    "bugFilePath": "flink-java/src/main/java/org/apache/flink/api/java/ExecutionEnvironment.java",
    "fixPatch": "diff --git a/flink-java/src/main/java/org/apache/flink/api/java/ExecutionEnvironment.java b/flink-java/src/main/java/org/apache/flink/api/java/ExecutionEnvironment.java\nindex 5a5bddb..8206220 100644\n--- a/flink-java/src/main/java/org/apache/flink/api/java/ExecutionEnvironment.java\n+++ b/flink-java/src/main/java/org/apache/flink/api/java/ExecutionEnvironment.java\n@@ -703,7 +703,7 @@\n \t\tcatch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + data[0].getClass().getName()\n \t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n-\t\t\t\t\t+ \"ExecutionEnvironment#fromElements(Collection, TypeInformation)\");\n+\t\t\t\t\t+ \"ExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n \t\t}\n \n \t\treturn fromCollection(Arrays.asList(data), typeInfo, Utils.getCallLocationName());\n@@ -736,7 +736,7 @@\n \t\tcatch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + type.getName()\n \t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n-\t\t\t\t\t+ \"ExecutionEnvironment#fromElements(Collection, TypeInformation)\");\n+\t\t\t\t\t+ \"ExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n \t\t}\n \n \t\treturn fromCollection(Arrays.asList(data), typeInfo, Utils.getCallLocationName());\n",
    "projectName": "apache.flink",
    "bugLineNum": 704,
    "bugNodeStartChar": 30709,
    "bugNodeLength": 229,
    "fixLineNum": 704,
    "fixNodeStartChar": 30709,
    "fixNodeLength": 232,
    "sourceBeforeFix": "new RuntimeException(\"Could not create TypeInformation for type \" + data[0].getClass().getName() + \"; please specify the TypeInformation manually via \"+ \"ExecutionEnvironment#fromElements(Collection, TypeInformation)\")",
    "sourceAfterFix": "new RuntimeException(\"Could not create TypeInformation for type \" + data[0].getClass().getName() + \"; please specify the TypeInformation manually via \"+ \"ExecutionEnvironment#fromElements(Collection, TypeInformation)\",e)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "c80e76bd7ccfe864566bdbffd6dcb6549709ea73",
    "fixCommitParentSHA1": "1c5a929c7ad6e40939e1170cdaf19c6ffd64f583",
    "bugFilePath": "flink-java/src/main/java/org/apache/flink/api/java/ExecutionEnvironment.java",
    "fixPatch": "diff --git a/flink-java/src/main/java/org/apache/flink/api/java/ExecutionEnvironment.java b/flink-java/src/main/java/org/apache/flink/api/java/ExecutionEnvironment.java\nindex 5a5bddb..8206220 100644\n--- a/flink-java/src/main/java/org/apache/flink/api/java/ExecutionEnvironment.java\n+++ b/flink-java/src/main/java/org/apache/flink/api/java/ExecutionEnvironment.java\n@@ -703,7 +703,7 @@\n \t\tcatch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + data[0].getClass().getName()\n \t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n-\t\t\t\t\t+ \"ExecutionEnvironment#fromElements(Collection, TypeInformation)\");\n+\t\t\t\t\t+ \"ExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n \t\t}\n \n \t\treturn fromCollection(Arrays.asList(data), typeInfo, Utils.getCallLocationName());\n@@ -736,7 +736,7 @@\n \t\tcatch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + type.getName()\n \t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n-\t\t\t\t\t+ \"ExecutionEnvironment#fromElements(Collection, TypeInformation)\");\n+\t\t\t\t\t+ \"ExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n \t\t}\n \n \t\treturn fromCollection(Arrays.asList(data), typeInfo, Utils.getCallLocationName());\n",
    "projectName": "apache.flink",
    "bugLineNum": 737,
    "bugNodeStartChar": 32043,
    "bugNodeLength": 215,
    "fixLineNum": 737,
    "fixNodeStartChar": 32043,
    "fixNodeLength": 218,
    "sourceBeforeFix": "new RuntimeException(\"Could not create TypeInformation for type \" + type.getName() + \"; please specify the TypeInformation manually via \"+ \"ExecutionEnvironment#fromElements(Collection, TypeInformation)\")",
    "sourceAfterFix": "new RuntimeException(\"Could not create TypeInformation for type \" + type.getName() + \"; please specify the TypeInformation manually via \"+ \"ExecutionEnvironment#fromElements(Collection, TypeInformation)\",e)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "c80e76bd7ccfe864566bdbffd6dcb6549709ea73",
    "fixCommitParentSHA1": "1c5a929c7ad6e40939e1170cdaf19c6ffd64f583",
    "bugFilePath": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java\nindex 83ca4a6..f35434f 100644\n--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java\n+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java\n@@ -711,7 +711,7 @@\n \t\tcatch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + data[0].getClass().getName()\n \t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n-\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\");\n+\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n \t\t}\n \t\treturn fromCollection(Arrays.asList(data), typeInfo);\n \t}\n@@ -744,7 +744,7 @@\n \t\tcatch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + type.getName()\n \t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n-\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\");\n+\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n \t\t}\n \t\treturn fromCollection(Arrays.asList(data), typeInfo);\n \t}\n@@ -785,7 +785,7 @@\n \t\tcatch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + first.getClass()\n \t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n-\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\");\n+\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n \t\t}\n \t\treturn fromCollection(data, typeInfo);\n \t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 712,
    "bugNodeStartChar": 28359,
    "bugNodeLength": 235,
    "fixLineNum": 712,
    "fixNodeStartChar": 28359,
    "fixNodeLength": 238,
    "sourceBeforeFix": "new RuntimeException(\"Could not create TypeInformation for type \" + data[0].getClass().getName() + \"; please specify the TypeInformation manually via \"+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\")",
    "sourceAfterFix": "new RuntimeException(\"Could not create TypeInformation for type \" + data[0].getClass().getName() + \"; please specify the TypeInformation manually via \"+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\",e)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "c80e76bd7ccfe864566bdbffd6dcb6549709ea73",
    "fixCommitParentSHA1": "1c5a929c7ad6e40939e1170cdaf19c6ffd64f583",
    "bugFilePath": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java\nindex 83ca4a6..f35434f 100644\n--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java\n+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java\n@@ -711,7 +711,7 @@\n \t\tcatch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + data[0].getClass().getName()\n \t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n-\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\");\n+\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n \t\t}\n \t\treturn fromCollection(Arrays.asList(data), typeInfo);\n \t}\n@@ -744,7 +744,7 @@\n \t\tcatch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + type.getName()\n \t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n-\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\");\n+\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n \t\t}\n \t\treturn fromCollection(Arrays.asList(data), typeInfo);\n \t}\n@@ -785,7 +785,7 @@\n \t\tcatch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + first.getClass()\n \t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n-\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\");\n+\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n \t\t}\n \t\treturn fromCollection(data, typeInfo);\n \t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 745,
    "bugNodeStartChar": 29699,
    "bugNodeLength": 221,
    "fixLineNum": 745,
    "fixNodeStartChar": 29699,
    "fixNodeLength": 224,
    "sourceBeforeFix": "new RuntimeException(\"Could not create TypeInformation for type \" + type.getName() + \"; please specify the TypeInformation manually via \"+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\")",
    "sourceAfterFix": "new RuntimeException(\"Could not create TypeInformation for type \" + type.getName() + \"; please specify the TypeInformation manually via \"+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\",e)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "c80e76bd7ccfe864566bdbffd6dcb6549709ea73",
    "fixCommitParentSHA1": "1c5a929c7ad6e40939e1170cdaf19c6ffd64f583",
    "bugFilePath": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java\nindex 83ca4a6..f35434f 100644\n--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java\n+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.java\n@@ -711,7 +711,7 @@\n \t\tcatch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + data[0].getClass().getName()\n \t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n-\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\");\n+\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n \t\t}\n \t\treturn fromCollection(Arrays.asList(data), typeInfo);\n \t}\n@@ -744,7 +744,7 @@\n \t\tcatch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + type.getName()\n \t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n-\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\");\n+\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n \t\t}\n \t\treturn fromCollection(Arrays.asList(data), typeInfo);\n \t}\n@@ -785,7 +785,7 @@\n \t\tcatch (Exception e) {\n \t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + first.getClass()\n \t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n-\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\");\n+\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n \t\t}\n \t\treturn fromCollection(data, typeInfo);\n \t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 786,
    "bugNodeStartChar": 31312,
    "bugNodeLength": 223,
    "fixLineNum": 786,
    "fixNodeStartChar": 31312,
    "fixNodeLength": 226,
    "sourceBeforeFix": "new RuntimeException(\"Could not create TypeInformation for type \" + first.getClass() + \"; please specify the TypeInformation manually via \"+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\")",
    "sourceAfterFix": "new RuntimeException(\"Could not create TypeInformation for type \" + first.getClass() + \"; please specify the TypeInformation manually via \"+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\",e)"
  },
  {
    "bugType": "SWAP_ARGUMENTS",
    "fixCommitSHA1": "15cdc5cc799e2ceb78af974d32feff2096f6571e",
    "fixCommitParentSHA1": "105b30686f76dc2d6affcf6cdf3eb41f322df53a",
    "bugFilePath": "flink-queryable-state/flink-queryable-state-runtime/src/main/java/org/apache/flink/queryablestate/client/proxy/KvStateClientProxyHandler.java",
    "fixPatch": "diff --git a/flink-queryable-state/flink-queryable-state-runtime/src/main/java/org/apache/flink/queryablestate/client/proxy/KvStateClientProxyHandler.java b/flink-queryable-state/flink-queryable-state-runtime/src/main/java/org/apache/flink/queryablestate/client/proxy/KvStateClientProxyHandler.java\nindex 2e24431..8201305 100644\n--- a/flink-queryable-state/flink-queryable-state-runtime/src/main/java/org/apache/flink/queryablestate/client/proxy/KvStateClientProxyHandler.java\n+++ b/flink-queryable-state/flink-queryable-state-runtime/src/main/java/org/apache/flink/queryablestate/client/proxy/KvStateClientProxyHandler.java\n@@ -197,14 +197,14 @@\n \t\tfinal CompletableFuture<KvStateLocation> cachedFuture = lookupCache.get(cacheKey);\n \n \t\tif (!forceUpdate && cachedFuture != null && !cachedFuture.isCompletedExceptionally()) {\n-\t\t\tLOG.debug(\"Retrieving location for state={} of job={} from the cache.\", jobId, queryableStateName);\n+\t\t\tLOG.debug(\"Retrieving location for state={} of job={} from the cache.\", queryableStateName, jobId);\n \t\t\treturn cachedFuture;\n \t\t}\n \n \t\tfinal KvStateLocationOracle kvStateLocationOracle = proxy.getKvStateLocationOracle(jobId);\n \n \t\tif (kvStateLocationOracle != null) {\n-\t\t\tLOG.debug(\"Retrieving location for state={} of job={} from the key-value state location oracle.\", jobId, queryableStateName);\n+\t\t\tLOG.debug(\"Retrieving location for state={} of job={} from the key-value state location oracle.\", queryableStateName, jobId);\n \t\t\tfinal CompletableFuture<KvStateLocation> location = new CompletableFuture<>();\n \t\t\tlookupCache.put(cacheKey, location);\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 200,
    "bugNodeStartChar": 8460,
    "bugNodeLength": 98,
    "fixLineNum": 200,
    "fixNodeStartChar": 8460,
    "fixNodeLength": 98,
    "sourceBeforeFix": "LOG.debug(\"Retrieving location for state={} of job={} from the cache.\",jobId,queryableStateName)",
    "sourceAfterFix": "LOG.debug(\"Retrieving location for state={} of job={} from the cache.\",queryableStateName,jobId)"
  },
  {
    "bugType": "SWAP_ARGUMENTS",
    "fixCommitSHA1": "15cdc5cc799e2ceb78af974d32feff2096f6571e",
    "fixCommitParentSHA1": "105b30686f76dc2d6affcf6cdf3eb41f322df53a",
    "bugFilePath": "flink-queryable-state/flink-queryable-state-runtime/src/main/java/org/apache/flink/queryablestate/client/proxy/KvStateClientProxyHandler.java",
    "fixPatch": "diff --git a/flink-queryable-state/flink-queryable-state-runtime/src/main/java/org/apache/flink/queryablestate/client/proxy/KvStateClientProxyHandler.java b/flink-queryable-state/flink-queryable-state-runtime/src/main/java/org/apache/flink/queryablestate/client/proxy/KvStateClientProxyHandler.java\nindex 2e24431..8201305 100644\n--- a/flink-queryable-state/flink-queryable-state-runtime/src/main/java/org/apache/flink/queryablestate/client/proxy/KvStateClientProxyHandler.java\n+++ b/flink-queryable-state/flink-queryable-state-runtime/src/main/java/org/apache/flink/queryablestate/client/proxy/KvStateClientProxyHandler.java\n@@ -197,14 +197,14 @@\n \t\tfinal CompletableFuture<KvStateLocation> cachedFuture = lookupCache.get(cacheKey);\n \n \t\tif (!forceUpdate && cachedFuture != null && !cachedFuture.isCompletedExceptionally()) {\n-\t\t\tLOG.debug(\"Retrieving location for state={} of job={} from the cache.\", jobId, queryableStateName);\n+\t\t\tLOG.debug(\"Retrieving location for state={} of job={} from the cache.\", queryableStateName, jobId);\n \t\t\treturn cachedFuture;\n \t\t}\n \n \t\tfinal KvStateLocationOracle kvStateLocationOracle = proxy.getKvStateLocationOracle(jobId);\n \n \t\tif (kvStateLocationOracle != null) {\n-\t\t\tLOG.debug(\"Retrieving location for state={} of job={} from the key-value state location oracle.\", jobId, queryableStateName);\n+\t\t\tLOG.debug(\"Retrieving location for state={} of job={} from the key-value state location oracle.\", queryableStateName, jobId);\n \t\t\tfinal CompletableFuture<KvStateLocation> location = new CompletableFuture<>();\n \t\t\tlookupCache.put(cacheKey, location);\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 207,
    "bugNodeStartChar": 8725,
    "bugNodeLength": 124,
    "fixLineNum": 207,
    "fixNodeStartChar": 8725,
    "fixNodeLength": 124,
    "sourceBeforeFix": "LOG.debug(\"Retrieving location for state={} of job={} from the key-value state location oracle.\",jobId,queryableStateName)",
    "sourceAfterFix": "LOG.debug(\"Retrieving location for state={} of job={} from the key-value state location oracle.\",queryableStateName,jobId)"
  },
  {
    "bugType": "DELETE_THROWS_EXCEPTION",
    "fixCommitSHA1": "e04a3bf7703544cf5012eb2f55626af8e0a2cb8c",
    "fixCommitParentSHA1": "6c37b15721af3a84fb3ac0de14486a9e98433215",
    "bugFilePath": "flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java",
    "fixPatch": "diff --git a/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java b/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\nindex 91fb4c1..e9e79ac 100644\n--- a/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\n+++ b/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\n@@ -58,7 +58,7 @@\n  *\n  * <p>The stream is bounded and will complete after about a minute.\n  * The result is always constant.\n- * The job is killed on the first attemped and restarted.\n+ * The job is killed on the first attempt and restarted.\n  *\n  * <p>Parameters:\n  * -outputPath Sets the path to where the result data is written.\n@@ -249,12 +249,12 @@\n \t\t}\n \n \t\t@Override\n-\t\tpublic List<Long> snapshotState(long checkpointId, long timestamp) throws Exception {\n+\t\tpublic List<Long> snapshotState(long checkpointId, long timestamp) {\n \t\t\treturn Collections.singletonList(ms);\n \t\t}\n \n \t\t@Override\n-\t\tpublic void restoreState(List<Long> state) throws Exception {\n+\t\tpublic void restoreState(List<Long> state) {\n \t\t\tfor (Long l : state) {\n \t\t\t\tms += l;\n \t\t\t}\n@@ -272,7 +272,7 @@\n \t\tprivate int lostRecordCnt = 0;\n \n \t\t@Override\n-\t\tpublic Row map(Row value) throws Exception {\n+\t\tpublic Row map(Row value) {\n \n \t\t\t// the both counts are the same only in the first execution attempt\n \t\t\tif (saveRecordCnt == 1 && lostRecordCnt == 1) {\n@@ -294,12 +294,12 @@\n \t\t}\n \n \t\t@Override\n-\t\tpublic List<Integer> snapshotState(long checkpointId, long timestamp) throws Exception {\n+\t\tpublic List<Integer> snapshotState(long checkpointId, long timestamp) {\n \t\t\treturn Collections.singletonList(saveRecordCnt);\n \t\t}\n \n \t\t@Override\n-\t\tpublic void restoreState(List<Integer> state) throws Exception {\n+\t\tpublic void restoreState(List<Integer> state) {\n \t\t\tfor (Integer i : state) {\n \t\t\t\tsaveRecordCnt += i;\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 251,
    "bugNodeStartChar": 8396,
    "bugNodeLength": 142,
    "fixLineNum": 251,
    "fixNodeStartChar": 8396,
    "fixNodeLength": 125,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1"
  },
  {
    "bugType": "DELETE_THROWS_EXCEPTION",
    "fixCommitSHA1": "e04a3bf7703544cf5012eb2f55626af8e0a2cb8c",
    "fixCommitParentSHA1": "6c37b15721af3a84fb3ac0de14486a9e98433215",
    "bugFilePath": "flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java",
    "fixPatch": "diff --git a/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java b/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\nindex 91fb4c1..e9e79ac 100644\n--- a/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\n+++ b/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\n@@ -58,7 +58,7 @@\n  *\n  * <p>The stream is bounded and will complete after about a minute.\n  * The result is always constant.\n- * The job is killed on the first attemped and restarted.\n+ * The job is killed on the first attempt and restarted.\n  *\n  * <p>Parameters:\n  * -outputPath Sets the path to where the result data is written.\n@@ -249,12 +249,12 @@\n \t\t}\n \n \t\t@Override\n-\t\tpublic List<Long> snapshotState(long checkpointId, long timestamp) throws Exception {\n+\t\tpublic List<Long> snapshotState(long checkpointId, long timestamp) {\n \t\t\treturn Collections.singletonList(ms);\n \t\t}\n \n \t\t@Override\n-\t\tpublic void restoreState(List<Long> state) throws Exception {\n+\t\tpublic void restoreState(List<Long> state) {\n \t\t\tfor (Long l : state) {\n \t\t\t\tms += l;\n \t\t\t}\n@@ -272,7 +272,7 @@\n \t\tprivate int lostRecordCnt = 0;\n \n \t\t@Override\n-\t\tpublic Row map(Row value) throws Exception {\n+\t\tpublic Row map(Row value) {\n \n \t\t\t// the both counts are the same only in the first execution attempt\n \t\t\tif (saveRecordCnt == 1 && lostRecordCnt == 1) {\n@@ -294,12 +294,12 @@\n \t\t}\n \n \t\t@Override\n-\t\tpublic List<Integer> snapshotState(long checkpointId, long timestamp) throws Exception {\n+\t\tpublic List<Integer> snapshotState(long checkpointId, long timestamp) {\n \t\t\treturn Collections.singletonList(saveRecordCnt);\n \t\t}\n \n \t\t@Override\n-\t\tpublic void restoreState(List<Integer> state) throws Exception {\n+\t\tpublic void restoreState(List<Integer> state) {\n \t\t\tfor (Integer i : state) {\n \t\t\t\tsaveRecordCnt += i;\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 256,
    "bugNodeStartChar": 8542,
    "bugNodeLength": 121,
    "fixLineNum": 256,
    "fixNodeStartChar": 8542,
    "fixNodeLength": 104,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1"
  },
  {
    "bugType": "DELETE_THROWS_EXCEPTION",
    "fixCommitSHA1": "e04a3bf7703544cf5012eb2f55626af8e0a2cb8c",
    "fixCommitParentSHA1": "6c37b15721af3a84fb3ac0de14486a9e98433215",
    "bugFilePath": "flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java",
    "fixPatch": "diff --git a/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java b/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\nindex 91fb4c1..e9e79ac 100644\n--- a/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\n+++ b/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\n@@ -58,7 +58,7 @@\n  *\n  * <p>The stream is bounded and will complete after about a minute.\n  * The result is always constant.\n- * The job is killed on the first attemped and restarted.\n+ * The job is killed on the first attempt and restarted.\n  *\n  * <p>Parameters:\n  * -outputPath Sets the path to where the result data is written.\n@@ -249,12 +249,12 @@\n \t\t}\n \n \t\t@Override\n-\t\tpublic List<Long> snapshotState(long checkpointId, long timestamp) throws Exception {\n+\t\tpublic List<Long> snapshotState(long checkpointId, long timestamp) {\n \t\t\treturn Collections.singletonList(ms);\n \t\t}\n \n \t\t@Override\n-\t\tpublic void restoreState(List<Long> state) throws Exception {\n+\t\tpublic void restoreState(List<Long> state) {\n \t\t\tfor (Long l : state) {\n \t\t\t\tms += l;\n \t\t\t}\n@@ -272,7 +272,7 @@\n \t\tprivate int lostRecordCnt = 0;\n \n \t\t@Override\n-\t\tpublic Row map(Row value) throws Exception {\n+\t\tpublic Row map(Row value) {\n \n \t\t\t// the both counts are the same only in the first execution attempt\n \t\t\tif (saveRecordCnt == 1 && lostRecordCnt == 1) {\n@@ -294,12 +294,12 @@\n \t\t}\n \n \t\t@Override\n-\t\tpublic List<Integer> snapshotState(long checkpointId, long timestamp) throws Exception {\n+\t\tpublic List<Integer> snapshotState(long checkpointId, long timestamp) {\n \t\t\treturn Collections.singletonList(saveRecordCnt);\n \t\t}\n \n \t\t@Override\n-\t\tpublic void restoreState(List<Integer> state) throws Exception {\n+\t\tpublic void restoreState(List<Integer> state) {\n \t\t\tfor (Integer i : state) {\n \t\t\t\tsaveRecordCnt += i;\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 274,
    "bugNodeStartChar": 9084,
    "bugNodeLength": 390,
    "fixLineNum": 274,
    "fixNodeStartChar": 9084,
    "fixNodeLength": 373,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1"
  },
  {
    "bugType": "DELETE_THROWS_EXCEPTION",
    "fixCommitSHA1": "e04a3bf7703544cf5012eb2f55626af8e0a2cb8c",
    "fixCommitParentSHA1": "6c37b15721af3a84fb3ac0de14486a9e98433215",
    "bugFilePath": "flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java",
    "fixPatch": "diff --git a/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java b/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\nindex 91fb4c1..e9e79ac 100644\n--- a/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\n+++ b/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\n@@ -58,7 +58,7 @@\n  *\n  * <p>The stream is bounded and will complete after about a minute.\n  * The result is always constant.\n- * The job is killed on the first attemped and restarted.\n+ * The job is killed on the first attempt and restarted.\n  *\n  * <p>Parameters:\n  * -outputPath Sets the path to where the result data is written.\n@@ -249,12 +249,12 @@\n \t\t}\n \n \t\t@Override\n-\t\tpublic List<Long> snapshotState(long checkpointId, long timestamp) throws Exception {\n+\t\tpublic List<Long> snapshotState(long checkpointId, long timestamp) {\n \t\t\treturn Collections.singletonList(ms);\n \t\t}\n \n \t\t@Override\n-\t\tpublic void restoreState(List<Long> state) throws Exception {\n+\t\tpublic void restoreState(List<Long> state) {\n \t\t\tfor (Long l : state) {\n \t\t\t\tms += l;\n \t\t\t}\n@@ -272,7 +272,7 @@\n \t\tprivate int lostRecordCnt = 0;\n \n \t\t@Override\n-\t\tpublic Row map(Row value) throws Exception {\n+\t\tpublic Row map(Row value) {\n \n \t\t\t// the both counts are the same only in the first execution attempt\n \t\t\tif (saveRecordCnt == 1 && lostRecordCnt == 1) {\n@@ -294,12 +294,12 @@\n \t\t}\n \n \t\t@Override\n-\t\tpublic List<Integer> snapshotState(long checkpointId, long timestamp) throws Exception {\n+\t\tpublic List<Integer> snapshotState(long checkpointId, long timestamp) {\n \t\t\treturn Collections.singletonList(saveRecordCnt);\n \t\t}\n \n \t\t@Override\n-\t\tpublic void restoreState(List<Integer> state) throws Exception {\n+\t\tpublic void restoreState(List<Integer> state) {\n \t\t\tfor (Integer i : state) {\n \t\t\t\tsaveRecordCnt += i;\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 296,
    "bugNodeStartChar": 9593,
    "bugNodeLength": 156,
    "fixLineNum": 296,
    "fixNodeStartChar": 9593,
    "fixNodeLength": 139,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1"
  },
  {
    "bugType": "DELETE_THROWS_EXCEPTION",
    "fixCommitSHA1": "e04a3bf7703544cf5012eb2f55626af8e0a2cb8c",
    "fixCommitParentSHA1": "6c37b15721af3a84fb3ac0de14486a9e98433215",
    "bugFilePath": "flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java",
    "fixPatch": "diff --git a/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java b/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\nindex 91fb4c1..e9e79ac 100644\n--- a/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\n+++ b/flink-end-to-end-tests/flink-stream-sql-test/src/main/java/org/apache/flink/sql/tests/StreamSQLTestProgram.java\n@@ -58,7 +58,7 @@\n  *\n  * <p>The stream is bounded and will complete after about a minute.\n  * The result is always constant.\n- * The job is killed on the first attemped and restarted.\n+ * The job is killed on the first attempt and restarted.\n  *\n  * <p>Parameters:\n  * -outputPath Sets the path to where the result data is written.\n@@ -249,12 +249,12 @@\n \t\t}\n \n \t\t@Override\n-\t\tpublic List<Long> snapshotState(long checkpointId, long timestamp) throws Exception {\n+\t\tpublic List<Long> snapshotState(long checkpointId, long timestamp) {\n \t\t\treturn Collections.singletonList(ms);\n \t\t}\n \n \t\t@Override\n-\t\tpublic void restoreState(List<Long> state) throws Exception {\n+\t\tpublic void restoreState(List<Long> state) {\n \t\t\tfor (Long l : state) {\n \t\t\t\tms += l;\n \t\t\t}\n@@ -272,7 +272,7 @@\n \t\tprivate int lostRecordCnt = 0;\n \n \t\t@Override\n-\t\tpublic Row map(Row value) throws Exception {\n+\t\tpublic Row map(Row value) {\n \n \t\t\t// the both counts are the same only in the first execution attempt\n \t\t\tif (saveRecordCnt == 1 && lostRecordCnt == 1) {\n@@ -294,12 +294,12 @@\n \t\t}\n \n \t\t@Override\n-\t\tpublic List<Integer> snapshotState(long checkpointId, long timestamp) throws Exception {\n+\t\tpublic List<Integer> snapshotState(long checkpointId, long timestamp) {\n \t\t\treturn Collections.singletonList(saveRecordCnt);\n \t\t}\n \n \t\t@Override\n-\t\tpublic void restoreState(List<Integer> state) throws Exception {\n+\t\tpublic void restoreState(List<Integer> state) {\n \t\t\tfor (Integer i : state) {\n \t\t\t\tsaveRecordCnt += i;\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 301,
    "bugNodeStartChar": 9753,
    "bugNodeLength": 138,
    "fixLineNum": 301,
    "fixNodeStartChar": 9753,
    "fixNodeLength": 121,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "6ad626ae51a157306ddf4165f13ff5eb5b4d5e8b",
    "fixCommitParentSHA1": "45397fe974e1390cd39a34fc2eb216f3771ddf06",
    "bugFilePath": "flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/ClientTest.java",
    "fixPatch": "diff --git a/flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/ClientTest.java b/flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/ClientTest.java\nindex 8638efa..6aa4710 100644\n--- a/flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/ClientTest.java\n+++ b/flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/ClientTest.java\n@@ -111,7 +111,8 @@\n \t@After\n \tpublic void tearDown() throws Exception {\n \t\tif (nioGroup != null) {\n-\t\t\tnioGroup.shutdownGracefully();\n+\t\t\t// note: no \"quiet period\" to not trigger Netty#4357\n+\t\t\tnioGroup.shutdownGracefully(0, 10, TimeUnit.SECONDS);\n \t\t}\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 114,
    "bugNodeStartChar": 4925,
    "bugNodeLength": 29,
    "fixLineNum": 115,
    "fixNodeStartChar": 4981,
    "fixNodeLength": 52,
    "sourceBeforeFix": "nioGroup.shutdownGracefully()",
    "sourceAfterFix": "nioGroup.shutdownGracefully(0,10,TimeUnit.SECONDS)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "6ad626ae51a157306ddf4165f13ff5eb5b4d5e8b",
    "fixCommitParentSHA1": "45397fe974e1390cd39a34fc2eb216f3771ddf06",
    "bugFilePath": "flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/KvStateServerTest.java",
    "fixPatch": "diff --git a/flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/KvStateServerTest.java b/flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/KvStateServerTest.java\nindex 8af9cf5..79c23ad 100644\n--- a/flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/KvStateServerTest.java\n+++ b/flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/KvStateServerTest.java\n@@ -79,7 +79,8 @@\n \t@AfterClass\n \tpublic static void tearDown() throws Exception {\n \t\tif (NIO_GROUP != null) {\n-\t\t\tNIO_GROUP.shutdownGracefully();\n+\t\t\t// note: no \"quiet period\" to not trigger Netty#4357\n+\t\t\tNIO_GROUP.shutdownGracefully(0, 10, TimeUnit.SECONDS);\n \t\t}\n \t}\n \n@@ -191,7 +192,8 @@\n \t\t\tif (bootstrap != null) {\n \t\t\t\tEventLoopGroup group = bootstrap.group();\n \t\t\t\tif (group != null) {\n-\t\t\t\t\tgroup.shutdownGracefully();\n+\t\t\t\t\t// note: no \"quiet period\" to not trigger Netty#4357\n+\t\t\t\t\tgroup.shutdownGracefully(0, 10, TimeUnit.SECONDS);\n \t\t\t\t}\n \t\t\t}\n \t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 82,
    "bugNodeStartChar": 3857,
    "bugNodeLength": 30,
    "fixLineNum": 83,
    "fixNodeStartChar": 3913,
    "fixNodeLength": 53,
    "sourceBeforeFix": "NIO_GROUP.shutdownGracefully()",
    "sourceAfterFix": "NIO_GROUP.shutdownGracefully(0,10,TimeUnit.SECONDS)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "6ad626ae51a157306ddf4165f13ff5eb5b4d5e8b",
    "fixCommitParentSHA1": "45397fe974e1390cd39a34fc2eb216f3771ddf06",
    "bugFilePath": "flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/KvStateServerTest.java",
    "fixPatch": "diff --git a/flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/KvStateServerTest.java b/flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/KvStateServerTest.java\nindex 8af9cf5..79c23ad 100644\n--- a/flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/KvStateServerTest.java\n+++ b/flink-queryable-state/flink-queryable-state-runtime/src/test/java/org/apache/flink/queryablestate/network/KvStateServerTest.java\n@@ -79,7 +79,8 @@\n \t@AfterClass\n \tpublic static void tearDown() throws Exception {\n \t\tif (NIO_GROUP != null) {\n-\t\t\tNIO_GROUP.shutdownGracefully();\n+\t\t\t// note: no \"quiet period\" to not trigger Netty#4357\n+\t\t\tNIO_GROUP.shutdownGracefully(0, 10, TimeUnit.SECONDS);\n \t\t}\n \t}\n \n@@ -191,7 +192,8 @@\n \t\t\tif (bootstrap != null) {\n \t\t\t\tEventLoopGroup group = bootstrap.group();\n \t\t\t\tif (group != null) {\n-\t\t\t\t\tgroup.shutdownGracefully();\n+\t\t\t\t\t// note: no \"quiet period\" to not trigger Netty#4357\n+\t\t\t\t\tgroup.shutdownGracefully(0, 10, TimeUnit.SECONDS);\n \t\t\t\t}\n \t\t\t}\n \t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 194,
    "bugNodeStartChar": 7344,
    "bugNodeLength": 26,
    "fixLineNum": 195,
    "fixNodeStartChar": 7402,
    "fixNodeLength": 49,
    "sourceBeforeFix": "group.shutdownGracefully()",
    "sourceAfterFix": "group.shutdownGracefully(0,10,TimeUnit.SECONDS)"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "6165b3db5587170bed1a40bb1e5f2f3613f24e3f",
    "fixCommitParentSHA1": "6e9e0dd6eb3c4fdb1168cc4e294d9fa52641ddb0",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannel.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannel.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannel.java\nindex 990166f..0f70d44 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannel.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/partition/consumer/RemoteInputChannel.java\n@@ -338,7 +338,7 @@\n \t}\n \n \t@VisibleForTesting\n-\tpublic boolean isWaitingForFloatingBuffers() {\n+\tboolean isWaitingForFloatingBuffers() {\n \t\treturn isWaitingForFloatingBuffers;\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 340,
    "bugNodeStartChar": 11767,
    "bugNodeLength": 107,
    "fixLineNum": 340,
    "fixNodeStartChar": 11767,
    "fixNodeLength": 100,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "0"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "0ae7364bdd2a0ad1db1ec02dc6b1b730187f2b78",
    "fixCommitParentSHA1": "2886a41728c0c13b3d01221c502a3e2a7014605d",
    "bugFilePath": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java",
    "fixPatch": "diff --git a/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java b/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java\nindex d126301..df35de6 100644\n--- a/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java\n+++ b/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java\n@@ -601,7 +601,7 @@\n \n \t\t\t\t\t\twhile (running) {\n \t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n-\t\t\t\t\t\t\t\tLOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\");\n+\t\t\t\t\t\t\t\tLOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\", getRuntimeContext().getIndexOfThisSubtask());\n \t\t\t\t\t\t\t}\n \n \t\t\t\t\t\t\ttry {\n",
    "projectName": "apache.flink",
    "bugLineNum": 604,
    "bugNodeStartChar": 28317,
    "bugNodeLength": 73,
    "fixLineNum": 604,
    "fixNodeStartChar": 28317,
    "fixNodeLength": 118,
    "sourceBeforeFix": "LOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\")",
    "sourceAfterFix": "LOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\",getRuntimeContext().getIndexOfThisSubtask())"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/entrypoint/ClusterEntrypoint.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/entrypoint/ClusterEntrypoint.java b/flink-runtime/src/main/java/org/apache/flink/runtime/entrypoint/ClusterEntrypoint.java\nindex f347d05..8e4f936 100755\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/entrypoint/ClusterEntrypoint.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/entrypoint/ClusterEntrypoint.java\n@@ -213,7 +213,7 @@\n \n \t\t\t// TODO: Make shutDownAndTerminate non blocking to not use the global executor\n \t\t\tdispatcher.getTerminationFuture().whenCompleteAsync(\n-\t\t\t\t(Boolean success, Throwable throwable) -> {\n+\t\t\t\t(Void value, Throwable throwable) -> {\n \t\t\t\t\tif (throwable != null) {\n \t\t\t\t\t\tLOG.info(\"Could not properly terminate the Dispatcher.\", throwable);\n \t\t\t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 216,
    "bugNodeStartChar": 7953,
    "bugNodeLength": 15,
    "fixLineNum": 216,
    "fixNodeStartChar": 7953,
    "fixNodeLength": 10,
    "sourceBeforeFix": "Boolean success",
    "sourceAfterFix": "Void value"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobManagerRunner.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobManagerRunner.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobManagerRunner.java\nindex 5740bd7..4269243 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobManagerRunner.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobManagerRunner.java\n@@ -225,10 +225,10 @@\n \n \t\t\t\tjobManager.shutDown();\n \n-\t\t\t\tfinal CompletableFuture<Boolean> jobManagerTerminationFuture = jobManager.getTerminationFuture();\n+\t\t\t\tfinal CompletableFuture<Void> jobManagerTerminationFuture = jobManager.getTerminationFuture();\n \n \t\t\t\tjobManagerTerminationFuture.whenComplete(\n-\t\t\t\t\t(Boolean ignored, Throwable throwable) -> {\n+\t\t\t\t\t(Void ignored, Throwable throwable) -> {\n \t\t\t\t\t\ttry {\n \t\t\t\t\t\t\tleaderElectionService.stop();\n \t\t\t\t\t\t} catch (Throwable t) {\n",
    "projectName": "apache.flink",
    "bugLineNum": 228,
    "bugNodeStartChar": 8529,
    "bugNodeLength": 26,
    "fixLineNum": 228,
    "fixNodeStartChar": 8529,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobManagerRunner.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobManagerRunner.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobManagerRunner.java\nindex 5740bd7..4269243 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobManagerRunner.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobManagerRunner.java\n@@ -225,10 +225,10 @@\n \n \t\t\t\tjobManager.shutDown();\n \n-\t\t\t\tfinal CompletableFuture<Boolean> jobManagerTerminationFuture = jobManager.getTerminationFuture();\n+\t\t\t\tfinal CompletableFuture<Void> jobManagerTerminationFuture = jobManager.getTerminationFuture();\n \n \t\t\t\tjobManagerTerminationFuture.whenComplete(\n-\t\t\t\t\t(Boolean ignored, Throwable throwable) -> {\n+\t\t\t\t\t(Void ignored, Throwable throwable) -> {\n \t\t\t\t\t\ttry {\n \t\t\t\t\t\t\tleaderElectionService.stop();\n \t\t\t\t\t\t} catch (Throwable t) {\n",
    "projectName": "apache.flink",
    "bugLineNum": 231,
    "bugNodeStartChar": 8674,
    "bugNodeLength": 15,
    "fixLineNum": 231,
    "fixNodeStartChar": 8674,
    "fixNodeLength": 12,
    "sourceBeforeFix": "Boolean ignored",
    "sourceAfterFix": "Void ignored"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java\nindex dd2a7ea..015751b 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmaster/JobMaster.java\n@@ -407,7 +407,7 @@\n \n \t\t// shut down will internally release all registered slots\n \t\tslotPool.shutDown();\n-\t\tCompletableFuture<Boolean> terminationFuture = slotPool.getTerminationFuture();\n+\t\tCompletableFuture<Void> terminationFuture = slotPool.getTerminationFuture();\n \n \t\tException exception = null;\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 410,
    "bugNodeStartChar": 16954,
    "bugNodeLength": 26,
    "fixLineNum": 410,
    "fixNodeStartChar": 16954,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rpc/RpcEndpoint.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/RpcEndpoint.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/RpcEndpoint.java\nindex 9c27c95..9c2ed83 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/RpcEndpoint.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/RpcEndpoint.java\n@@ -228,7 +228,7 @@\n \t *\n \t * @return Future which is completed when the rpc endpoint has been terminated.\n \t */\n-\tpublic CompletableFuture<Boolean> getTerminationFuture() {\n+\tpublic CompletableFuture<Void> getTerminationFuture() {\n \t\treturn rpcServer.getTerminationFuture();\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 231,
    "bugNodeStartChar": 7938,
    "bugNodeLength": 26,
    "fixLineNum": 231,
    "fixNodeStartChar": 7938,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rpc/RpcServer.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/RpcServer.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/RpcServer.java\nindex ac2f7eb..14d0cc9 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/RpcServer.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/RpcServer.java\n@@ -30,5 +30,5 @@\n \t *\n \t * @return Future indicating when the rpc endpoint has been terminated\n \t */\n-\tCompletableFuture<Boolean> getTerminationFuture();\n+\tCompletableFuture<Void> getTerminationFuture();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 33,
    "bugNodeStartChar": 1190,
    "bugNodeLength": 26,
    "fixLineNum": 33,
    "fixNodeStartChar": 1190,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java\nindex 863b780..cc54f2e 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java\n@@ -84,7 +84,7 @@\n \n \t// null if gateway; otherwise non-null\n \t@Nullable\n-\tprivate final CompletableFuture<Boolean> terminationFuture;\n+\tprivate final CompletableFuture<Void> terminationFuture;\n \n \tAkkaInvocationHandler(\n \t\t\tString address,\n@@ -92,7 +92,7 @@\n \t\t\tActorRef rpcEndpoint,\n \t\t\tTime timeout,\n \t\t\tlong maximumFramesize,\n-\t\t\t@Nullable CompletableFuture<Boolean> terminationFuture) {\n+\t\t\t@Nullable CompletableFuture<Void> terminationFuture) {\n \n \t\tthis.address = Preconditions.checkNotNull(address);\n \t\tthis.hostname = Preconditions.checkNotNull(hostname);\n@@ -341,7 +341,7 @@\n \t}\n \n \t@Override\n-\tpublic CompletableFuture<Boolean> getTerminationFuture() {\n+\tpublic CompletableFuture<Void> getTerminationFuture() {\n \t\treturn terminationFuture;\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 87,
    "bugNodeStartChar": 3246,
    "bugNodeLength": 26,
    "fixLineNum": 87,
    "fixNodeStartChar": 3246,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java\nindex 863b780..cc54f2e 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java\n@@ -84,7 +84,7 @@\n \n \t// null if gateway; otherwise non-null\n \t@Nullable\n-\tprivate final CompletableFuture<Boolean> terminationFuture;\n+\tprivate final CompletableFuture<Void> terminationFuture;\n \n \tAkkaInvocationHandler(\n \t\t\tString address,\n@@ -92,7 +92,7 @@\n \t\t\tActorRef rpcEndpoint,\n \t\t\tTime timeout,\n \t\t\tlong maximumFramesize,\n-\t\t\t@Nullable CompletableFuture<Boolean> terminationFuture) {\n+\t\t\t@Nullable CompletableFuture<Void> terminationFuture) {\n \n \t\tthis.address = Preconditions.checkNotNull(address);\n \t\tthis.hostname = Preconditions.checkNotNull(hostname);\n@@ -341,7 +341,7 @@\n \t}\n \n \t@Override\n-\tpublic CompletableFuture<Boolean> getTerminationFuture() {\n+\tpublic CompletableFuture<Void> getTerminationFuture() {\n \t\treturn terminationFuture;\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 95,
    "bugNodeStartChar": 3437,
    "bugNodeLength": 26,
    "fixLineNum": 95,
    "fixNodeStartChar": 3437,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java\nindex 863b780..cc54f2e 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaInvocationHandler.java\n@@ -84,7 +84,7 @@\n \n \t// null if gateway; otherwise non-null\n \t@Nullable\n-\tprivate final CompletableFuture<Boolean> terminationFuture;\n+\tprivate final CompletableFuture<Void> terminationFuture;\n \n \tAkkaInvocationHandler(\n \t\t\tString address,\n@@ -92,7 +92,7 @@\n \t\t\tActorRef rpcEndpoint,\n \t\t\tTime timeout,\n \t\t\tlong maximumFramesize,\n-\t\t\t@Nullable CompletableFuture<Boolean> terminationFuture) {\n+\t\t\t@Nullable CompletableFuture<Void> terminationFuture) {\n \n \t\tthis.address = Preconditions.checkNotNull(address);\n \t\tthis.hostname = Preconditions.checkNotNull(hostname);\n@@ -341,7 +341,7 @@\n \t}\n \n \t@Override\n-\tpublic CompletableFuture<Boolean> getTerminationFuture() {\n+\tpublic CompletableFuture<Void> getTerminationFuture() {\n \t\treturn terminationFuture;\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 344,
    "bugNodeStartChar": 11266,
    "bugNodeLength": 26,
    "fixLineNum": 344,
    "fixNodeStartChar": 11266,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaRpcService.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaRpcService.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaRpcService.java\nindex a65fe46..8e96492 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaRpcService.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/AkkaRpcService.java\n@@ -195,7 +195,7 @@\n \tpublic <C extends RpcEndpoint & RpcGateway> RpcServer startServer(C rpcEndpoint) {\n \t\tcheckNotNull(rpcEndpoint, \"rpc endpoint\");\n \n-\t\tCompletableFuture<Boolean> terminationFuture = new CompletableFuture<>();\n+\t\tCompletableFuture<Void> terminationFuture = new CompletableFuture<>();\n \t\tfinal Props akkaRpcActorProps;\n \n \t\tif (rpcEndpoint instanceof FencedRpcEndpoint) {\n",
    "projectName": "apache.flink",
    "bugLineNum": 198,
    "bugNodeStartChar": 6042,
    "bugNodeLength": 26,
    "fixLineNum": 198,
    "fixNodeStartChar": 6042,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/FencedAkkaInvocationHandler.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/FencedAkkaInvocationHandler.java b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/FencedAkkaInvocationHandler.java\nindex 3ca75e2..564b1ef 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/FencedAkkaInvocationHandler.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/rpc/akka/FencedAkkaInvocationHandler.java\n@@ -60,7 +60,7 @@\n \t\t\tActorRef rpcEndpoint,\n \t\t\tTime timeout,\n \t\t\tlong maximumFramesize,\n-\t\t\t@Nullable CompletableFuture<Boolean> terminationFuture,\n+\t\t\t@Nullable CompletableFuture<Void> terminationFuture,\n \t\t\tSupplier<F> fencingTokenSupplier) {\n \t\tsuper(address, hostname, rpcEndpoint, timeout, maximumFramesize, terminationFuture);\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 63,
    "bugNodeStartChar": 2515,
    "bugNodeLength": 26,
    "fixLineNum": 63,
    "fixNodeStartChar": 2515,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerRunner.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerRunner.java b/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerRunner.java\nindex 4620585..2de1be8 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerRunner.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/taskexecutor/TaskManagerRunner.java\n@@ -198,7 +198,7 @@\n \t}\n \n \t// export the termination future for caller to know it is terminated\n-\tpublic CompletableFuture<Boolean> getTerminationFuture() {\n+\tpublic CompletableFuture<Void> getTerminationFuture() {\n \t\treturn taskManager.getTerminationFuture();\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 201,
    "bugNodeStartChar": 7005,
    "bugNodeLength": 26,
    "fixLineNum": 201,
    "fixNodeStartChar": 7005,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/dispatcher/MiniDispatcherTest.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/dispatcher/MiniDispatcherTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/dispatcher/MiniDispatcherTest.java\nindex 4291ef2..2b98939 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/dispatcher/MiniDispatcherTest.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/dispatcher/MiniDispatcherTest.java\n@@ -222,7 +222,7 @@\n \n \t\t\tresultFuture.complete(archivedExecutionGraph);\n \n-\t\t\tfinal CompletableFuture<Boolean> terminationFuture = miniDispatcher.getTerminationFuture();\n+\t\t\tfinal CompletableFuture<Void> terminationFuture = miniDispatcher.getTerminationFuture();\n \n \t\t\tassertThat(terminationFuture.isDone(), is(false));\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 225,
    "bugNodeStartChar": 7806,
    "bugNodeLength": 26,
    "fixLineNum": 225,
    "fixNodeStartChar": 7806,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java\nindex 1b45006..2a65cac 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java\n@@ -185,7 +185,7 @@\n \t\tfinal DummyRpcEndpoint rpcEndpoint = new DummyRpcEndpoint(akkaRpcService);\n \t\trpcEndpoint.start();\n \n-\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n+\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n \n \t\tassertFalse(terminationFuture.isDone());\n \n@@ -246,7 +246,7 @@\n \n \t\trpcEndpoint.shutDown();\n \n-\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n+\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n \n \t\ttry {\n \t\t\tterminationFuture.get();\n@@ -265,7 +265,7 @@\n \n \t\tsimpleRpcEndpoint.shutDown();\n \n-\t\tCompletableFuture<Boolean> terminationFuture = simpleRpcEndpoint.getTerminationFuture();\n+\t\tCompletableFuture<Void> terminationFuture = simpleRpcEndpoint.getTerminationFuture();\n \n \t\t// check that we executed the postStop method in the main thread, otherwise an exception\n \t\t// would be thrown here.\n@@ -285,7 +285,7 @@\n \n \t\t\trpcEndpoint.start();\n \n-\t\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n+\t\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n \n \t\t\trpcService.stopService();\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 188,
    "bugNodeStartChar": 6713,
    "bugNodeLength": 26,
    "fixLineNum": 188,
    "fixNodeStartChar": 6713,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java\nindex 1b45006..2a65cac 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java\n@@ -185,7 +185,7 @@\n \t\tfinal DummyRpcEndpoint rpcEndpoint = new DummyRpcEndpoint(akkaRpcService);\n \t\trpcEndpoint.start();\n \n-\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n+\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n \n \t\tassertFalse(terminationFuture.isDone());\n \n@@ -246,7 +246,7 @@\n \n \t\trpcEndpoint.shutDown();\n \n-\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n+\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n \n \t\ttry {\n \t\t\tterminationFuture.get();\n@@ -265,7 +265,7 @@\n \n \t\tsimpleRpcEndpoint.shutDown();\n \n-\t\tCompletableFuture<Boolean> terminationFuture = simpleRpcEndpoint.getTerminationFuture();\n+\t\tCompletableFuture<Void> terminationFuture = simpleRpcEndpoint.getTerminationFuture();\n \n \t\t// check that we executed the postStop method in the main thread, otherwise an exception\n \t\t// would be thrown here.\n@@ -285,7 +285,7 @@\n \n \t\t\trpcEndpoint.start();\n \n-\t\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n+\t\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n \n \t\t\trpcService.stopService();\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 249,
    "bugNodeStartChar": 8628,
    "bugNodeLength": 26,
    "fixLineNum": 249,
    "fixNodeStartChar": 8628,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java\nindex 1b45006..2a65cac 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java\n@@ -185,7 +185,7 @@\n \t\tfinal DummyRpcEndpoint rpcEndpoint = new DummyRpcEndpoint(akkaRpcService);\n \t\trpcEndpoint.start();\n \n-\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n+\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n \n \t\tassertFalse(terminationFuture.isDone());\n \n@@ -246,7 +246,7 @@\n \n \t\trpcEndpoint.shutDown();\n \n-\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n+\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n \n \t\ttry {\n \t\t\tterminationFuture.get();\n@@ -265,7 +265,7 @@\n \n \t\tsimpleRpcEndpoint.shutDown();\n \n-\t\tCompletableFuture<Boolean> terminationFuture = simpleRpcEndpoint.getTerminationFuture();\n+\t\tCompletableFuture<Void> terminationFuture = simpleRpcEndpoint.getTerminationFuture();\n \n \t\t// check that we executed the postStop method in the main thread, otherwise an exception\n \t\t// would be thrown here.\n@@ -285,7 +285,7 @@\n \n \t\t\trpcEndpoint.start();\n \n-\t\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n+\t\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n \n \t\t\trpcService.stopService();\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 268,
    "bugNodeStartChar": 9196,
    "bugNodeLength": 26,
    "fixLineNum": 268,
    "fixNodeStartChar": 9196,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c131546eaadd07baf950bd6a44d07ee42d109e4c",
    "fixCommitParentSHA1": "c27e2a77005db355da9e72656af8b0df8b1dfe75",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java\nindex 1b45006..2a65cac 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/rpc/akka/AkkaRpcActorTest.java\n@@ -185,7 +185,7 @@\n \t\tfinal DummyRpcEndpoint rpcEndpoint = new DummyRpcEndpoint(akkaRpcService);\n \t\trpcEndpoint.start();\n \n-\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n+\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n \n \t\tassertFalse(terminationFuture.isDone());\n \n@@ -246,7 +246,7 @@\n \n \t\trpcEndpoint.shutDown();\n \n-\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n+\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n \n \t\ttry {\n \t\t\tterminationFuture.get();\n@@ -265,7 +265,7 @@\n \n \t\tsimpleRpcEndpoint.shutDown();\n \n-\t\tCompletableFuture<Boolean> terminationFuture = simpleRpcEndpoint.getTerminationFuture();\n+\t\tCompletableFuture<Void> terminationFuture = simpleRpcEndpoint.getTerminationFuture();\n \n \t\t// check that we executed the postStop method in the main thread, otherwise an exception\n \t\t// would be thrown here.\n@@ -285,7 +285,7 @@\n \n \t\t\trpcEndpoint.start();\n \n-\t\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n+\t\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n \n \t\t\trpcService.stopService();\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 288,
    "bugNodeStartChar": 9911,
    "bugNodeLength": 26,
    "fixLineNum": 288,
    "fixNodeStartChar": 9911,
    "fixNodeLength": 23,
    "sourceBeforeFix": "CompletableFuture<Boolean>",
    "sourceAfterFix": "CompletableFuture<Void>"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "c212701d56cfe9cffd9e5dc1e34c3483a50f8182",
    "fixCommitParentSHA1": "b2fc8b427eb03a40df232f481c908eb7bf48b7b8",
    "bugFilePath": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java",
    "fixPatch": "diff --git a/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java b/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java\nindex deee0a1..5624db9 100644\n--- a/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java\n+++ b/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java\n@@ -1132,7 +1132,7 @@\n \t\t\t\tint length = in.readInt();\n \n \t\t\t\tbyte[] serCondition = new byte[length];\n-\t\t\t\tin.read(serCondition);\n+\t\t\t\tin.readFully(serCondition);\n \n \t\t\t\tByteArrayInputStream bais = new ByteArrayInputStream(serCondition);\n \t\t\t\tObjectInputStream ois = new ObjectInputStream(bais);\n",
    "projectName": "apache.flink",
    "bugLineNum": 1135,
    "bugNodeStartChar": 40671,
    "bugNodeLength": 21,
    "fixLineNum": 1135,
    "fixNodeStartChar": 40671,
    "fixNodeLength": 26,
    "sourceBeforeFix": "in.read(serCondition)",
    "sourceAfterFix": "in.readFully(serCondition)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c212701d56cfe9cffd9e5dc1e34c3483a50f8182",
    "fixCommitParentSHA1": "b2fc8b427eb03a40df232f481c908eb7bf48b7b8",
    "bugFilePath": "flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java",
    "fixPatch": "diff --git a/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java b/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java\nindex deee0a1..5624db9 100644\n--- a/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java\n+++ b/flink-libraries/flink-cep/src/main/java/org/apache/flink/cep/nfa/NFA.java\n@@ -1132,7 +1132,7 @@\n \t\t\t\tint length = in.readInt();\n \n \t\t\t\tbyte[] serCondition = new byte[length];\n-\t\t\t\tin.read(serCondition);\n+\t\t\t\tin.readFully(serCondition);\n \n \t\t\t\tByteArrayInputStream bais = new ByteArrayInputStream(serCondition);\n \t\t\t\tObjectInputStream ois = new ObjectInputStream(bais);\n",
    "projectName": "apache.flink",
    "bugLineNum": 1135,
    "bugNodeStartChar": 40671,
    "bugNodeLength": 21,
    "fixLineNum": 1135,
    "fixNodeStartChar": 40671,
    "fixNodeLength": 26,
    "sourceBeforeFix": "in.read(serCondition)",
    "sourceAfterFix": "in.readFully(serCondition)"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "16107cf67416391ccf1493bda747a108bbbb3a20",
    "fixCommitParentSHA1": "2906698b4a87f21c6fd099cf8a028f68fc311b1f",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java\nindex e73f61d..89fb9e8 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyMessage.java\n@@ -52,8 +52,11 @@\n \n /**\n  * A simple and generic interface to serialize messages to Netty's buffer space.\n+ *\n+ * <p>This class must be public as long as we are using a Netty version prior to 4.0.45. Please check FLINK-7845 for\n+ * more information.\n  */\n-abstract class NettyMessage {\n+public abstract class NettyMessage {\n \n \t// ------------------------------------------------------------------------\n \t// Note: Every NettyMessage subtype needs to have a public 0-argument\n",
    "projectName": "apache.flink",
    "bugLineNum": 53,
    "bugNodeStartChar": 2527,
    "bugNodeLength": 14619,
    "fixLineNum": 53,
    "fixNodeStartChar": 2527,
    "fixNodeLength": 14626,
    "sourceBeforeFix": "1024",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "872c35e52fc3034da081239fca062e5889c14c8a",
    "fixCommitParentSHA1": "1c123e40e564b097fde22da648679963c40bdfe3",
    "bugFilePath": "flink-connectors/flink-connector-kafka-0.11/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer011.java",
    "fixPatch": "diff --git a/flink-connectors/flink-connector-kafka-0.11/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer011.java b/flink-connectors/flink-connector-kafka-0.11/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer011.java\nindex e83966c..6242a20 100644\n--- a/flink-connectors/flink-connector-kafka-0.11/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer011.java\n+++ b/flink-connectors/flink-connector-kafka-0.11/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer011.java\n@@ -136,7 +136,7 @@\n \t\tNONE\n \t}\n \n-\tprivate static final Logger LOG = LoggerFactory.getLogger(FlinkKafkaProducerBase.class);\n+\tprivate static final Logger LOG = LoggerFactory.getLogger(FlinkKafkaProducer011.class);\n \n \tprivate static final long serialVersionUID = 1L;\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 139,
    "bugNodeStartChar": 6372,
    "bugNodeLength": 28,
    "fixLineNum": 139,
    "fixNodeStartChar": 6372,
    "fixNodeLength": 27,
    "sourceBeforeFix": "FlinkKafkaProducerBase.class",
    "sourceAfterFix": "FlinkKafkaProducer011.class"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "dfbf83ff6356fae4948cc44a1d0908defaeca45f",
    "fixCommitParentSHA1": "a9743eb6850809784930c1199741aa83ae54e99a",
    "bugFilePath": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java",
    "fixPatch": "diff --git a/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java b/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java\nindex 3088b15..7cd1ae1 100644\n--- a/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java\n+++ b/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java\n@@ -534,7 +534,7 @@\n \n \t\t\t@Override\n \t\t\tpublic void onException(Throwable cause) {\n-\t\t\t\tLOG.error(\"Async Kafka commit failed.\", cause);\n+\t\t\t\tLOG.warn(\"Async Kafka commit failed.\", cause);\n \t\t\t\tfailedCommits.inc();\n \t\t\t}\n \t\t};\n",
    "projectName": "apache.flink",
    "bugLineNum": 537,
    "bugNodeStartChar": 25436,
    "bugNodeLength": 46,
    "fixLineNum": 537,
    "fixNodeStartChar": 25436,
    "fixNodeLength": 45,
    "sourceBeforeFix": "LOG.error(\"Async Kafka commit failed.\",cause)",
    "sourceAfterFix": "LOG.warn(\"Async Kafka commit failed.\",cause)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "dfbf83ff6356fae4948cc44a1d0908defaeca45f",
    "fixCommitParentSHA1": "a9743eb6850809784930c1199741aa83ae54e99a",
    "bugFilePath": "flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java",
    "fixPatch": "diff --git a/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java b/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java\nindex 3088b15..7cd1ae1 100644\n--- a/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java\n+++ b/flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBase.java\n@@ -534,7 +534,7 @@\n \n \t\t\t@Override\n \t\t\tpublic void onException(Throwable cause) {\n-\t\t\t\tLOG.error(\"Async Kafka commit failed.\", cause);\n+\t\t\t\tLOG.warn(\"Async Kafka commit failed.\", cause);\n \t\t\t\tfailedCommits.inc();\n \t\t\t}\n \t\t};\n",
    "projectName": "apache.flink",
    "bugLineNum": 537,
    "bugNodeStartChar": 25436,
    "bugNodeLength": 46,
    "fixLineNum": 537,
    "fixNodeStartChar": 25436,
    "fixNodeLength": 45,
    "sourceBeforeFix": "LOG.error(\"Async Kafka commit failed.\",cause)",
    "sourceAfterFix": "LOG.warn(\"Async Kafka commit failed.\",cause)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "7d5100742355448169f00a607cc350884bbbd987",
    "fixCommitParentSHA1": "3bbff97b7e86a361fb77e8b243417fa5e1c60b7a",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java\nindex 2946d5e..b52c08c 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java\n@@ -108,7 +108,10 @@\n \t\t\t\theartbeatServices,\n \t\t\t\tExecutors.newScheduledThreadPool(1),\n \t\t\t\tblobServer,\n-\t\t\t\tnew BlobLibraryCacheManager(blobServer, FlinkUserCodeClassLoaders.ResolveOrder.CHILD_FIRST),\n+\t\t\t\tnew BlobLibraryCacheManager(\n+\t\t\t\t\tblobServer,\n+\t\t\t\t\tFlinkUserCodeClassLoaders.ResolveOrder.CHILD_FIRST,\n+\t\t\t\t\tnew String[0]),\n \t\t\t\tmock(RestartStrategyFactory.class),\n \t\t\t\ttestingTimeout,\n \t\t\t\tnull,\n@@ -212,7 +215,10 @@\n \t\t\t\theartbeatServices,\n \t\t\t\tExecutors.newScheduledThreadPool(1),\n \t\t\t\tblobServer,\n-\t\t\t\tnew BlobLibraryCacheManager(blobServer, FlinkUserCodeClassLoaders.ResolveOrder.CHILD_FIRST),\n+\t\t\t\tnew BlobLibraryCacheManager(\n+\t\t\t\t\tblobServer,\n+\t\t\t\t\tFlinkUserCodeClassLoaders.ResolveOrder.CHILD_FIRST,\n+\t\t\t\t\tnew String[0]),\n \t\t\t\tmock(RestartStrategyFactory.class),\n \t\t\t\ttestingTimeout,\n \t\t\t\tnull,\n",
    "projectName": "apache.flink",
    "bugLineNum": 111,
    "bugNodeStartChar": 4993,
    "bugNodeLength": 91,
    "fixLineNum": 111,
    "fixNodeStartChar": 4993,
    "fixNodeLength": 122,
    "sourceBeforeFix": "new BlobLibraryCacheManager(blobServer,FlinkUserCodeClassLoaders.ResolveOrder.CHILD_FIRST)",
    "sourceAfterFix": "new BlobLibraryCacheManager(blobServer,FlinkUserCodeClassLoaders.ResolveOrder.CHILD_FIRST,new String[0])"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "7d5100742355448169f00a607cc350884bbbd987",
    "fixCommitParentSHA1": "3bbff97b7e86a361fb77e8b243417fa5e1c60b7a",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java\nindex 2946d5e..b52c08c 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/jobmaster/JobMasterTest.java\n@@ -108,7 +108,10 @@\n \t\t\t\theartbeatServices,\n \t\t\t\tExecutors.newScheduledThreadPool(1),\n \t\t\t\tblobServer,\n-\t\t\t\tnew BlobLibraryCacheManager(blobServer, FlinkUserCodeClassLoaders.ResolveOrder.CHILD_FIRST),\n+\t\t\t\tnew BlobLibraryCacheManager(\n+\t\t\t\t\tblobServer,\n+\t\t\t\t\tFlinkUserCodeClassLoaders.ResolveOrder.CHILD_FIRST,\n+\t\t\t\t\tnew String[0]),\n \t\t\t\tmock(RestartStrategyFactory.class),\n \t\t\t\ttestingTimeout,\n \t\t\t\tnull,\n@@ -212,7 +215,10 @@\n \t\t\t\theartbeatServices,\n \t\t\t\tExecutors.newScheduledThreadPool(1),\n \t\t\t\tblobServer,\n-\t\t\t\tnew BlobLibraryCacheManager(blobServer, FlinkUserCodeClassLoaders.ResolveOrder.CHILD_FIRST),\n+\t\t\t\tnew BlobLibraryCacheManager(\n+\t\t\t\t\tblobServer,\n+\t\t\t\t\tFlinkUserCodeClassLoaders.ResolveOrder.CHILD_FIRST,\n+\t\t\t\t\tnew String[0]),\n \t\t\t\tmock(RestartStrategyFactory.class),\n \t\t\t\ttestingTimeout,\n \t\t\t\tnull,\n",
    "projectName": "apache.flink",
    "bugLineNum": 215,
    "bugNodeStartChar": 9289,
    "bugNodeLength": 91,
    "fixLineNum": 215,
    "fixNodeStartChar": 9289,
    "fixNodeLength": 122,
    "sourceBeforeFix": "new BlobLibraryCacheManager(blobServer,FlinkUserCodeClassLoaders.ResolveOrder.CHILD_FIRST)",
    "sourceAfterFix": "new BlobLibraryCacheManager(blobServer,FlinkUserCodeClassLoaders.ResolveOrder.CHILD_FIRST,new String[0])"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "867c0124e2959ea3c90dab13cc12ba43c2eb0f64",
    "fixCommitParentSHA1": "2f651e9a69a9929ef154e7bf6fcba624b0e8b9a1",
    "bugFilePath": "flink-streaming-java/src/test/java/org/apache/flink/streaming/util/AbstractStreamOperatorTestHarness.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/AbstractStreamOperatorTestHarness.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/AbstractStreamOperatorTestHarness.java\nindex 793e8f6..3d1b6fd 100644\n--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/AbstractStreamOperatorTestHarness.java\n+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/util/AbstractStreamOperatorTestHarness.java\n@@ -120,7 +120,7 @@\n \tpublic AbstractStreamOperatorTestHarness(\n \t\tStreamOperator<OUT> operator,\n \t\tint maxParallelism,\n-\t\tint numSubtasks,\n+\t\tint parallelism,\n \t\tint subtaskIndex) throws Exception {\n \n \t\tthis(\n@@ -133,7 +133,7 @@\n \t\t\t\tnew Configuration(),\n \t\t\t\tnew ExecutionConfig(),\n \t\t\t\tmaxParallelism,\n-\t\t\t\tnumSubtasks,\n+\t\t\t\tparallelism,\n \t\t\t\tsubtaskIndex));\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 128,
    "bugNodeStartChar": 5209,
    "bugNodeLength": 208,
    "fixLineNum": 128,
    "fixNodeStartChar": 5209,
    "fixNodeLength": 208,
    "sourceBeforeFix": "new MockEnvironment(\"MockTask\",3 * 1024 * 1024,new MockInputSplitProvider(),1024,new Configuration(),new ExecutionConfig(),maxParallelism,numSubtasks,subtaskIndex)",
    "sourceAfterFix": "new MockEnvironment(\"MockTask\",3 * 1024 * 1024,new MockInputSplitProvider(),1024,new Configuration(),new ExecutionConfig(),maxParallelism,parallelism,subtaskIndex)"
  },
  {
    "bugType": "CHANGE_OPERATOR",
    "fixCommitSHA1": "edc53a74553f19c2ec83ab7c12945758ba6084bf",
    "fixCommitParentSHA1": "3581a3350892a5c584a617d1b66cffdada4e17dd",
    "bugFilePath": "flink-java/src/main/java/org/apache/flink/api/java/Utils.java",
    "fixPatch": "diff --git a/flink-java/src/main/java/org/apache/flink/api/java/Utils.java b/flink-java/src/main/java/org/apache/flink/api/java/Utils.java\nindex 44e176c..0184e58 100644\n--- a/flink-java/src/main/java/org/apache/flink/api/java/Utils.java\n+++ b/flink-java/src/main/java/org/apache/flink/api/java/Utils.java\n@@ -53,7 +53,7 @@\n \tpublic static String getCallLocationName(int depth) {\n \t\tStackTraceElement[] stackTrace = Thread.currentThread().getStackTrace();\n \n-\t\tif (stackTrace.length < depth) {\n+\t\tif (stackTrace.length <= depth) {\n \t\t\treturn \"<unknown>\";\n \t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 56,
    "bugNodeStartChar": 2083,
    "bugNodeLength": 25,
    "fixLineNum": 56,
    "fixNodeStartChar": 2083,
    "fixNodeLength": 26,
    "sourceBeforeFix": "stackTrace.length < depth",
    "sourceAfterFix": "stackTrace.length <= depth"
  },
  {
    "bugType": "CHANGE_OPERATOR",
    "fixCommitSHA1": "edc53a74553f19c2ec83ab7c12945758ba6084bf",
    "fixCommitParentSHA1": "3581a3350892a5c584a617d1b66cffdada4e17dd",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/query/netty/message/KvStateRequestSerializer.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/query/netty/message/KvStateRequestSerializer.java b/flink-runtime/src/main/java/org/apache/flink/runtime/query/netty/message/KvStateRequestSerializer.java\nindex f0cc94a..68f06e3 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/query/netty/message/KvStateRequestSerializer.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/query/netty/message/KvStateRequestSerializer.java\n@@ -224,7 +224,7 @@\n \t\t// Get the message type\n \t\tint msgType = buf.readInt();\n \t\tKvStateRequestType[] values = KvStateRequestType.values();\n-\t\tif (msgType >= 0 && msgType <= values.length) {\n+\t\tif (msgType >= 0 && msgType < values.length) {\n \t\t\treturn values[msgType];\n \t\t} else {\n \t\t\tthrow new IllegalArgumentException(\"Illegal message type with index \" + msgType);\n",
    "projectName": "apache.flink",
    "bugLineNum": 227,
    "bugNodeStartChar": 7709,
    "bugNodeLength": 24,
    "fixLineNum": 227,
    "fixNodeStartChar": 7709,
    "fixNodeLength": 23,
    "sourceBeforeFix": "msgType <= values.length",
    "sourceAfterFix": "msgType < values.length"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "3407a44922289dc4ddb3ff87687b8766affeaad9",
    "fixCommitParentSHA1": "738dbbe57b1476f4d24ba8863d021ece11a6f514",
    "bugFilePath": "flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java",
    "fixPatch": "diff --git a/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java b/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java\nindex ba3499f..92bf6d6 100644\n--- a/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java\n+++ b/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java\n@@ -163,7 +163,14 @@\n \n \t// ------------------------------------------------------------------------\n \n-\tprivate static class SpeedTestProducer extends AbstractInvokable {\n+\t/**\n+\t * Invokable that produces records and allows slowdown via {@link #IS_SLOW_EVERY_NUM_RECORDS}\n+\t * and {@link #IS_SLOW_SENDER_CONFIG_KEY} and creates records of different data sizes via {@link\n+\t * #DATA_VOLUME_GB_CONFIG_KEY}.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> so that a task can be run with this!\n+\t */\n+\tpublic static class SpeedTestProducer extends AbstractInvokable {\n \n \t\t@Override\n \t\tpublic void invoke() throws Exception {\n@@ -198,7 +205,12 @@\n \t\t}\n \t}\n \n-\tprivate static class SpeedTestForwarder extends AbstractInvokable {\n+\t/**\n+\t * Invokable that forwards incoming records.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> so that a task can be run with this!\n+\t */\n+\tpublic static class SpeedTestForwarder extends AbstractInvokable {\n \n \t\t@Override\n \t\tpublic void invoke() throws Exception {\n@@ -222,7 +234,13 @@\n \t\t}\n \t}\n \n-\tprivate static class SpeedTestConsumer extends AbstractInvokable {\n+\t/**\n+\t * Invokable that consumes incoming records and allows slowdown via {@link\n+\t * #IS_SLOW_EVERY_NUM_RECORDS}.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> so that a task can be run with this!\n+\t */\n+\tpublic static class SpeedTestConsumer extends AbstractInvokable {\n \n \t\t@Override\n \t\tpublic void invoke() throws Exception {\n@@ -247,7 +265,12 @@\n \t\t}\n \t}\n \n-\tprivate static class SpeedTestRecord implements IOReadableWritable {\n+\t/**\n+\t * Record type for the speed test.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> to allow deserialization!\n+\t */\n+\tpublic static class SpeedTestRecord implements IOReadableWritable {\n \n \t\tprivate static final int RECORD_SIZE = 128;\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 166,
    "bugNodeStartChar": 6396,
    "bugNodeLength": 1235,
    "fixLineNum": 166,
    "fixNodeStartChar": 6396,
    "fixNodeLength": 1552,
    "sourceBeforeFix": "10",
    "sourceAfterFix": "9"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "3407a44922289dc4ddb3ff87687b8766affeaad9",
    "fixCommitParentSHA1": "738dbbe57b1476f4d24ba8863d021ece11a6f514",
    "bugFilePath": "flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java",
    "fixPatch": "diff --git a/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java b/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java\nindex ba3499f..92bf6d6 100644\n--- a/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java\n+++ b/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java\n@@ -163,7 +163,14 @@\n \n \t// ------------------------------------------------------------------------\n \n-\tprivate static class SpeedTestProducer extends AbstractInvokable {\n+\t/**\n+\t * Invokable that produces records and allows slowdown via {@link #IS_SLOW_EVERY_NUM_RECORDS}\n+\t * and {@link #IS_SLOW_SENDER_CONFIG_KEY} and creates records of different data sizes via {@link\n+\t * #DATA_VOLUME_GB_CONFIG_KEY}.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> so that a task can be run with this!\n+\t */\n+\tpublic static class SpeedTestProducer extends AbstractInvokable {\n \n \t\t@Override\n \t\tpublic void invoke() throws Exception {\n@@ -198,7 +205,12 @@\n \t\t}\n \t}\n \n-\tprivate static class SpeedTestForwarder extends AbstractInvokable {\n+\t/**\n+\t * Invokable that forwards incoming records.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> so that a task can be run with this!\n+\t */\n+\tpublic static class SpeedTestForwarder extends AbstractInvokable {\n \n \t\t@Override\n \t\tpublic void invoke() throws Exception {\n@@ -222,7 +234,13 @@\n \t\t}\n \t}\n \n-\tprivate static class SpeedTestConsumer extends AbstractInvokable {\n+\t/**\n+\t * Invokable that consumes incoming records and allows slowdown via {@link\n+\t * #IS_SLOW_EVERY_NUM_RECORDS}.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> so that a task can be run with this!\n+\t */\n+\tpublic static class SpeedTestConsumer extends AbstractInvokable {\n \n \t\t@Override\n \t\tpublic void invoke() throws Exception {\n@@ -247,7 +265,12 @@\n \t\t}\n \t}\n \n-\tprivate static class SpeedTestRecord implements IOReadableWritable {\n+\t/**\n+\t * Record type for the speed test.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> to allow deserialization!\n+\t */\n+\tpublic static class SpeedTestRecord implements IOReadableWritable {\n \n \t\tprivate static final int RECORD_SIZE = 128;\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 201,
    "bugNodeStartChar": 7634,
    "bugNodeLength": 604,
    "fixLineNum": 201,
    "fixNodeStartChar": 7634,
    "fixNodeLength": 741,
    "sourceBeforeFix": "10",
    "sourceAfterFix": "9"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "3407a44922289dc4ddb3ff87687b8766affeaad9",
    "fixCommitParentSHA1": "738dbbe57b1476f4d24ba8863d021ece11a6f514",
    "bugFilePath": "flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java",
    "fixPatch": "diff --git a/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java b/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java\nindex ba3499f..92bf6d6 100644\n--- a/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java\n+++ b/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java\n@@ -163,7 +163,14 @@\n \n \t// ------------------------------------------------------------------------\n \n-\tprivate static class SpeedTestProducer extends AbstractInvokable {\n+\t/**\n+\t * Invokable that produces records and allows slowdown via {@link #IS_SLOW_EVERY_NUM_RECORDS}\n+\t * and {@link #IS_SLOW_SENDER_CONFIG_KEY} and creates records of different data sizes via {@link\n+\t * #DATA_VOLUME_GB_CONFIG_KEY}.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> so that a task can be run with this!\n+\t */\n+\tpublic static class SpeedTestProducer extends AbstractInvokable {\n \n \t\t@Override\n \t\tpublic void invoke() throws Exception {\n@@ -198,7 +205,12 @@\n \t\t}\n \t}\n \n-\tprivate static class SpeedTestForwarder extends AbstractInvokable {\n+\t/**\n+\t * Invokable that forwards incoming records.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> so that a task can be run with this!\n+\t */\n+\tpublic static class SpeedTestForwarder extends AbstractInvokable {\n \n \t\t@Override\n \t\tpublic void invoke() throws Exception {\n@@ -222,7 +234,13 @@\n \t\t}\n \t}\n \n-\tprivate static class SpeedTestConsumer extends AbstractInvokable {\n+\t/**\n+\t * Invokable that consumes incoming records and allows slowdown via {@link\n+\t * #IS_SLOW_EVERY_NUM_RECORDS}.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> so that a task can be run with this!\n+\t */\n+\tpublic static class SpeedTestConsumer extends AbstractInvokable {\n \n \t\t@Override\n \t\tpublic void invoke() throws Exception {\n@@ -247,7 +265,12 @@\n \t\t}\n \t}\n \n-\tprivate static class SpeedTestRecord implements IOReadableWritable {\n+\t/**\n+\t * Record type for the speed test.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> to allow deserialization!\n+\t */\n+\tpublic static class SpeedTestRecord implements IOReadableWritable {\n \n \t\tprivate static final int RECORD_SIZE = 128;\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 225,
    "bugNodeStartChar": 8241,
    "bugNodeLength": 656,
    "fixLineNum": 225,
    "fixNodeStartChar": 8241,
    "fixNodeLength": 856,
    "sourceBeforeFix": "10",
    "sourceAfterFix": "9"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "3407a44922289dc4ddb3ff87687b8766affeaad9",
    "fixCommitParentSHA1": "738dbbe57b1476f4d24ba8863d021ece11a6f514",
    "bugFilePath": "flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java",
    "fixPatch": "diff --git a/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java b/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java\nindex ba3499f..92bf6d6 100644\n--- a/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java\n+++ b/flink-tests/src/test/java/org/apache/flink/test/runtime/NetworkStackThroughputITCase.java\n@@ -163,7 +163,14 @@\n \n \t// ------------------------------------------------------------------------\n \n-\tprivate static class SpeedTestProducer extends AbstractInvokable {\n+\t/**\n+\t * Invokable that produces records and allows slowdown via {@link #IS_SLOW_EVERY_NUM_RECORDS}\n+\t * and {@link #IS_SLOW_SENDER_CONFIG_KEY} and creates records of different data sizes via {@link\n+\t * #DATA_VOLUME_GB_CONFIG_KEY}.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> so that a task can be run with this!\n+\t */\n+\tpublic static class SpeedTestProducer extends AbstractInvokable {\n \n \t\t@Override\n \t\tpublic void invoke() throws Exception {\n@@ -198,7 +205,12 @@\n \t\t}\n \t}\n \n-\tprivate static class SpeedTestForwarder extends AbstractInvokable {\n+\t/**\n+\t * Invokable that forwards incoming records.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> so that a task can be run with this!\n+\t */\n+\tpublic static class SpeedTestForwarder extends AbstractInvokable {\n \n \t\t@Override\n \t\tpublic void invoke() throws Exception {\n@@ -222,7 +234,13 @@\n \t\t}\n \t}\n \n-\tprivate static class SpeedTestConsumer extends AbstractInvokable {\n+\t/**\n+\t * Invokable that consumes incoming records and allows slowdown via {@link\n+\t * #IS_SLOW_EVERY_NUM_RECORDS}.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> so that a task can be run with this!\n+\t */\n+\tpublic static class SpeedTestConsumer extends AbstractInvokable {\n \n \t\t@Override\n \t\tpublic void invoke() throws Exception {\n@@ -247,7 +265,12 @@\n \t\t}\n \t}\n \n-\tprivate static class SpeedTestRecord implements IOReadableWritable {\n+\t/**\n+\t * Record type for the speed test.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> to allow deserialization!\n+\t */\n+\tpublic static class SpeedTestRecord implements IOReadableWritable {\n \n \t\tprivate static final int RECORD_SIZE = 128;\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 250,
    "bugNodeStartChar": 8900,
    "bugNodeLength": 493,
    "fixLineNum": 250,
    "fixNodeStartChar": 8900,
    "fixNodeLength": 609,
    "sourceBeforeFix": "10",
    "sourceAfterFix": "9"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "738dbbe57b1476f4d24ba8863d021ece11a6f514",
    "fixCommitParentSHA1": "a4a4e0b9e5eff3d578fde7e2fca3e84661d84b40",
    "bugFilePath": "flink-tests/src/test/java/org/apache/flink/test/example/client/JobRetrievalITCase.java",
    "fixPatch": "diff --git a/flink-tests/src/test/java/org/apache/flink/test/example/client/JobRetrievalITCase.java b/flink-tests/src/test/java/org/apache/flink/test/example/client/JobRetrievalITCase.java\nindex 6ce4f76..39eeccb 100644\n--- a/flink-tests/src/test/java/org/apache/flink/test/example/client/JobRetrievalITCase.java\n+++ b/flink-tests/src/test/java/org/apache/flink/test/example/client/JobRetrievalITCase.java\n@@ -132,7 +132,12 @@\n \t\t}\n \t}\n \n-\tprivate static class SemaphoreInvokable extends AbstractInvokable {\n+\t/**\n+\t * Invokable that waits on {@link #lock} to be released and finishes afterwards.\n+\t *\n+\t * <p>NOTE: needs to be <tt>public</tt> so that a task can be run with this!\n+\t */\n+\tpublic static class SemaphoreInvokable extends AbstractInvokable {\n \n \t\t@Override\n \t\tpublic void invoke() throws Exception {\n",
    "projectName": "apache.flink",
    "bugLineNum": 135,
    "bugNodeStartChar": 4255,
    "bugNodeLength": 148,
    "fixLineNum": 135,
    "fixNodeStartChar": 4255,
    "fixNodeLength": 321,
    "sourceBeforeFix": "10",
    "sourceAfterFix": "9"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "788d9452d8541b3ec7a11cae5d9fa605c2626fc1",
    "fixCommitParentSHA1": "8624c2904f2840b901e805f1e0b33b6977e581ca",
    "bugFilePath": "flink-core/src/main/java/org/apache/flink/api/common/Plan.java",
    "fixPatch": "diff --git a/flink-core/src/main/java/org/apache/flink/api/common/Plan.java b/flink-core/src/main/java/org/apache/flink/api/common/Plan.java\nindex 0235af0..db10ce4 100644\n--- a/flink-core/src/main/java/org/apache/flink/api/common/Plan.java\n+++ b/flink-core/src/main/java/org/apache/flink/api/common/Plan.java\n@@ -201,7 +201,7 @@\n \t * @param sink The data sink to add.\n \t */\n \tpublic void addDataSink(GenericDataSinkBase<?> sink) {\n-\t\tcheckNotNull(jobName, \"The data sink must not be null.\");\n+\t\tcheckNotNull(sink, \"The data sink must not be null.\");\n \t\t\n \t\tif (!this.sinks.contains(sink)) {\n \t\t\tthis.sinks.add(sink);\n",
    "projectName": "apache.flink",
    "bugLineNum": 204,
    "bugNodeStartChar": 7861,
    "bugNodeLength": 56,
    "fixLineNum": 204,
    "fixNodeStartChar": 7861,
    "fixNodeLength": 53,
    "sourceBeforeFix": "checkNotNull(jobName,\"The data sink must not be null.\")",
    "sourceAfterFix": "checkNotNull(sink,\"The data sink must not be null.\")"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "0c4f7988dc8c947eb7bda3afa8c58ace04d4d1d8",
    "fixCommitParentSHA1": "47862afbef98faee61e07ca4a00f41f34a764cf5",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java\nindex 4ea6511..69f1a49 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java\n@@ -126,12 +126,12 @@\n \t\t\t\t\tbreak;\n \t\t\t\t} else {\n \t\t\t\t\t// Retry\n-\t\t\t\t\tThread.sleep(500);\n+\t\t\t\t\tThread.sleep(500L);\n \t\t\t\t}\n \t\t\t}\n \n \t\t\t// Verify that async producer is in blocking request\n-\t\t\tassertTrue(\"Producer thread is not blocked: \" + Arrays.toString(ASYNC_CONSUMER_THREAD.getStackTrace()), producerBlocked);\n+\t\t\tassertTrue(\"Producer thread is not blocked: \" + Arrays.toString(ASYNC_PRODUCER_THREAD.getStackTrace()), producerBlocked);\n \n \t\t\tboolean consumerWaiting = false;\n \t\t\tfor (int i = 0; i < 50; i++) {\n@@ -145,7 +145,7 @@\n \t\t\t\t\tbreak;\n \t\t\t\t} else {\n \t\t\t\t\t// Retry\n-\t\t\t\t\tThread.sleep(500);\n+\t\t\t\t\tThread.sleep(500L);\n \t\t\t\t}\n \t\t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 129,
    "bugNodeStartChar": 5465,
    "bugNodeLength": 17,
    "fixLineNum": 129,
    "fixNodeStartChar": 5465,
    "fixNodeLength": 18,
    "sourceBeforeFix": "Thread.sleep(500)",
    "sourceAfterFix": "Thread.sleep(500L)"
  },
  {
    "bugType": "CHANGE_CALLER_IN_FUNCTION_CALL",
    "fixCommitSHA1": "0c4f7988dc8c947eb7bda3afa8c58ace04d4d1d8",
    "fixCommitParentSHA1": "47862afbef98faee61e07ca4a00f41f34a764cf5",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java\nindex 4ea6511..69f1a49 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java\n@@ -126,12 +126,12 @@\n \t\t\t\t\tbreak;\n \t\t\t\t} else {\n \t\t\t\t\t// Retry\n-\t\t\t\t\tThread.sleep(500);\n+\t\t\t\t\tThread.sleep(500L);\n \t\t\t\t}\n \t\t\t}\n \n \t\t\t// Verify that async producer is in blocking request\n-\t\t\tassertTrue(\"Producer thread is not blocked: \" + Arrays.toString(ASYNC_CONSUMER_THREAD.getStackTrace()), producerBlocked);\n+\t\t\tassertTrue(\"Producer thread is not blocked: \" + Arrays.toString(ASYNC_PRODUCER_THREAD.getStackTrace()), producerBlocked);\n \n \t\t\tboolean consumerWaiting = false;\n \t\t\tfor (int i = 0; i < 50; i++) {\n@@ -145,7 +145,7 @@\n \t\t\t\t\tbreak;\n \t\t\t\t} else {\n \t\t\t\t\t// Retry\n-\t\t\t\t\tThread.sleep(500);\n+\t\t\t\t\tThread.sleep(500L);\n \t\t\t\t}\n \t\t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 134,
    "bugNodeStartChar": 5619,
    "bugNodeLength": 37,
    "fixLineNum": 134,
    "fixNodeStartChar": 5619,
    "fixNodeLength": 37,
    "sourceBeforeFix": "ASYNC_CONSUMER_THREAD.getStackTrace()",
    "sourceAfterFix": "ASYNC_PRODUCER_THREAD.getStackTrace()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "0c4f7988dc8c947eb7bda3afa8c58ace04d4d1d8",
    "fixCommitParentSHA1": "47862afbef98faee61e07ca4a00f41f34a764cf5",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java\nindex 4ea6511..69f1a49 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java\n@@ -126,12 +126,12 @@\n \t\t\t\t\tbreak;\n \t\t\t\t} else {\n \t\t\t\t\t// Retry\n-\t\t\t\t\tThread.sleep(500);\n+\t\t\t\t\tThread.sleep(500L);\n \t\t\t\t}\n \t\t\t}\n \n \t\t\t// Verify that async producer is in blocking request\n-\t\t\tassertTrue(\"Producer thread is not blocked: \" + Arrays.toString(ASYNC_CONSUMER_THREAD.getStackTrace()), producerBlocked);\n+\t\t\tassertTrue(\"Producer thread is not blocked: \" + Arrays.toString(ASYNC_PRODUCER_THREAD.getStackTrace()), producerBlocked);\n \n \t\t\tboolean consumerWaiting = false;\n \t\t\tfor (int i = 0; i < 50; i++) {\n@@ -145,7 +145,7 @@\n \t\t\t\t\tbreak;\n \t\t\t\t} else {\n \t\t\t\t\t// Retry\n-\t\t\t\t\tThread.sleep(500);\n+\t\t\t\t\tThread.sleep(500L);\n \t\t\t\t}\n \t\t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 134,
    "bugNodeStartChar": 5619,
    "bugNodeLength": 37,
    "fixLineNum": 134,
    "fixNodeStartChar": 5619,
    "fixNodeLength": 37,
    "sourceBeforeFix": "ASYNC_CONSUMER_THREAD.getStackTrace()",
    "sourceAfterFix": "ASYNC_PRODUCER_THREAD.getStackTrace()"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "0c4f7988dc8c947eb7bda3afa8c58ace04d4d1d8",
    "fixCommitParentSHA1": "47862afbef98faee61e07ca4a00f41f34a764cf5",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java\nindex 4ea6511..69f1a49 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskCancelAsyncProducerConsumerITCase.java\n@@ -126,12 +126,12 @@\n \t\t\t\t\tbreak;\n \t\t\t\t} else {\n \t\t\t\t\t// Retry\n-\t\t\t\t\tThread.sleep(500);\n+\t\t\t\t\tThread.sleep(500L);\n \t\t\t\t}\n \t\t\t}\n \n \t\t\t// Verify that async producer is in blocking request\n-\t\t\tassertTrue(\"Producer thread is not blocked: \" + Arrays.toString(ASYNC_CONSUMER_THREAD.getStackTrace()), producerBlocked);\n+\t\t\tassertTrue(\"Producer thread is not blocked: \" + Arrays.toString(ASYNC_PRODUCER_THREAD.getStackTrace()), producerBlocked);\n \n \t\t\tboolean consumerWaiting = false;\n \t\t\tfor (int i = 0; i < 50; i++) {\n@@ -145,7 +145,7 @@\n \t\t\t\t\tbreak;\n \t\t\t\t} else {\n \t\t\t\t\t// Retry\n-\t\t\t\t\tThread.sleep(500);\n+\t\t\t\t\tThread.sleep(500L);\n \t\t\t\t}\n \t\t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 148,
    "bugNodeStartChar": 5982,
    "bugNodeLength": 17,
    "fixLineNum": 148,
    "fixNodeStartChar": 5982,
    "fixNodeLength": 18,
    "sourceBeforeFix": "Thread.sleep(500)",
    "sourceAfterFix": "Thread.sleep(500L)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "a448fc53877d025130f05b8738d8104c66139fbb",
    "fixCommitParentSHA1": "926547582d62733bdedb1cdcc277653b2ff7b4e7",
    "bugFilePath": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorMigrationTest.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorMigrationTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorMigrationTest.java\nindex 19fa04f..1168eb0 100644\n--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorMigrationTest.java\n+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorMigrationTest.java\n@@ -826,7 +826,7 @@\n \t\t\t\t\tif (comparison != 0) {\n \t\t\t\t\t\treturn comparison;\n \t\t\t\t\t}\n-\t\t\t\t\treturn (int) (sr0.getValue().f1 - sr1.getValue().f1);\n+\t\t\t\t\treturn (int) (sr0.getValue().f2 - sr1.getValue().f2);\n \t\t\t\t}\n \t\t\t}\n \t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 829,
    "bugNodeStartChar": 37736,
    "bugNodeLength": 17,
    "fixLineNum": 829,
    "fixNodeStartChar": 37736,
    "fixNodeLength": 17,
    "sourceBeforeFix": "sr0.getValue().f1",
    "sourceAfterFix": "sr0.getValue().f2"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "a448fc53877d025130f05b8738d8104c66139fbb",
    "fixCommitParentSHA1": "926547582d62733bdedb1cdcc277653b2ff7b4e7",
    "bugFilePath": "flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorTest.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorTest.java b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorTest.java\nindex 1137c6a..474548c 100644\n--- a/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorTest.java\n+++ b/flink-streaming-java/src/test/java/org/apache/flink/streaming/runtime/operators/windowing/WindowOperatorTest.java\n@@ -2560,7 +2560,7 @@\n \t\t\t\t\tif (comparison != 0) {\n \t\t\t\t\t\treturn comparison;\n \t\t\t\t\t}\n-\t\t\t\t\treturn (int) (sr0.getValue().f1 - sr1.getValue().f1);\n+\t\t\t\t\treturn (int) (sr0.getValue().f2 - sr1.getValue().f2);\n \t\t\t\t}\n \t\t\t}\n \t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 2563,
    "bugNodeStartChar": 112290,
    "bugNodeLength": 17,
    "fixLineNum": 2563,
    "fixNodeStartChar": 112290,
    "fixNodeLength": 17,
    "sourceBeforeFix": "sr0.getValue().f1",
    "sourceAfterFix": "sr0.getValue().f2"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "33abc25f0f7f15433a81147cbaf815d87808486f",
    "fixCommitParentSHA1": "82a258bf63260a723a76fc4bb42ecac50a66ff99",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/query/QueryableStateClient.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/query/QueryableStateClient.java b/flink-runtime/src/main/java/org/apache/flink/runtime/query/QueryableStateClient.java\nindex 7ba3199..5d9db19 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/query/QueryableStateClient.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/query/QueryableStateClient.java\n@@ -92,7 +92,7 @@\n \tprivate final ConcurrentMap<Tuple2<JobID, String>, Future<KvStateLocation>> lookupCache =\n \t\t\tnew ConcurrentHashMap<>();\n \n-\t/** This is != null, iff we started the actor system. */\n+\t/** This is != null, if we started the actor system. */\n \tprivate final ActorSystem actorSystem;\n \n \t/**\n@@ -210,7 +210,7 @@\n \t\t\ttry {\n \t\t\t\tactorSystem.shutdown();\n \t\t\t} catch (Throwable t) {\n-\t\t\t\tLOG.error(\"Failed to shut down ActorSystem\");\n+\t\t\t\tLOG.error(\"Failed to shut down ActorSystem\", t);\n \t\t\t}\n \t\t}\n \t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 213,
    "bugNodeStartChar": 7471,
    "bugNodeLength": 44,
    "fixLineNum": 213,
    "fixNodeStartChar": 7471,
    "fixNodeLength": 47,
    "sourceBeforeFix": "LOG.error(\"Failed to shut down ActorSystem\")",
    "sourceAfterFix": "LOG.error(\"Failed to shut down ActorSystem\",t)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "85f75a5999cfeabb82f24f1f3c4cd998a4c5b348",
    "fixCommitParentSHA1": "60895a3ccd83609088be6ecef3445f7c78c9955a",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java\nindex 1c7b1c8..0564fd0 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java\n@@ -1090,11 +1090,10 @@\n \n \t/**\n \t * For testing: This waits until the job execution has finished.\n-\t * @throws InterruptedException\n \t */\n \tpublic void waitUntilFinished() throws InterruptedException {\n \t\tsynchronized (progressLock) {\n-\t\t\twhile (!state.isGloballyTerminalState()) {\n+\t\t\twhile (!state.isTerminalState()) {\n \t\t\t\tprogressLock.wait();\n \t\t\t}\n \t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 1097,
    "bugNodeStartChar": 38338,
    "bugNodeLength": 31,
    "fixLineNum": 1097,
    "fixNodeStartChar": 38338,
    "fixNodeLength": 23,
    "sourceBeforeFix": "state.isGloballyTerminalState()",
    "sourceAfterFix": "state.isTerminalState()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "85f75a5999cfeabb82f24f1f3c4cd998a4c5b348",
    "fixCommitParentSHA1": "60895a3ccd83609088be6ecef3445f7c78c9955a",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java\nindex 1c7b1c8..0564fd0 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/executiongraph/ExecutionGraph.java\n@@ -1090,11 +1090,10 @@\n \n \t/**\n \t * For testing: This waits until the job execution has finished.\n-\t * @throws InterruptedException\n \t */\n \tpublic void waitUntilFinished() throws InterruptedException {\n \t\tsynchronized (progressLock) {\n-\t\t\twhile (!state.isGloballyTerminalState()) {\n+\t\t\twhile (!state.isTerminalState()) {\n \t\t\t\tprogressLock.wait();\n \t\t\t}\n \t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 1097,
    "bugNodeStartChar": 38338,
    "bugNodeLength": 31,
    "fixLineNum": 1097,
    "fixNodeStartChar": 38338,
    "fixNodeLength": 23,
    "sourceBeforeFix": "state.isGloballyTerminalState()",
    "sourceAfterFix": "state.isTerminalState()"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "20420fc6ee153c7171265dda7bf7d593c17fb375",
    "fixCommitParentSHA1": "70475b367d0dfcf2d467ba3d15ccb544b6076a73",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java b/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java\nindex 136df09..aeaa602 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java\n@@ -124,7 +124,7 @@\n \tprivate static File getCacheDirectory(File storageDir) {\n \t\tfinal File cacheDirectory = new File(storageDir, \"cache\");\n \n-\t\tif (!cacheDirectory.exists() && !cacheDirectory.mkdirs()) {\n+\t\tif (!cacheDirectory.mkdirs() && !cacheDirectory.exists()) {\n \t\t\tthrow new RuntimeException(\"Could not create cache directory '\" + cacheDirectory.getAbsolutePath() + \"'.\");\n \t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 127,
    "bugNodeStartChar": 3949,
    "bugNodeLength": 23,
    "fixLineNum": 127,
    "fixNodeStartChar": 3949,
    "fixNodeLength": 23,
    "sourceBeforeFix": "cacheDirectory.exists()",
    "sourceAfterFix": "cacheDirectory.mkdirs()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "20420fc6ee153c7171265dda7bf7d593c17fb375",
    "fixCommitParentSHA1": "70475b367d0dfcf2d467ba3d15ccb544b6076a73",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java b/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java\nindex 136df09..aeaa602 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java\n@@ -124,7 +124,7 @@\n \tprivate static File getCacheDirectory(File storageDir) {\n \t\tfinal File cacheDirectory = new File(storageDir, \"cache\");\n \n-\t\tif (!cacheDirectory.exists() && !cacheDirectory.mkdirs()) {\n+\t\tif (!cacheDirectory.mkdirs() && !cacheDirectory.exists()) {\n \t\t\tthrow new RuntimeException(\"Could not create cache directory '\" + cacheDirectory.getAbsolutePath() + \"'.\");\n \t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 127,
    "bugNodeStartChar": 3949,
    "bugNodeLength": 23,
    "fixLineNum": 127,
    "fixNodeStartChar": 3949,
    "fixNodeLength": 23,
    "sourceBeforeFix": "cacheDirectory.exists()",
    "sourceAfterFix": "cacheDirectory.mkdirs()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "16ee4a5ce717987d824593b1f9fdaafa44d98632",
    "fixCommitParentSHA1": "3458a6615cf1db66f32650bbaae2841b570fa647",
    "bugFilePath": "flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/JoinedStreams.java",
    "fixPatch": "diff --git a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/JoinedStreams.java b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/JoinedStreams.java\nindex c005310..ed1cbd7 100644\n--- a/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/JoinedStreams.java\n+++ b/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/JoinedStreams.java\n@@ -306,7 +306,7 @@\n \t\tpublic <T> DataStream<T> apply(FlatJoinFunction<T1, T2, T> function) {\n \t\t\tTypeInformation<T> resultType = TypeExtractor.getBinaryOperatorReturnType(\n \t\t\t\t\tfunction,\n-\t\t\t\t\tJoinFunction.class,\n+\t\t\t\t\tFlatJoinFunction.class,\n \t\t\t\t\ttrue,\n \t\t\t\t\ttrue,\n \t\t\t\t\tinput1.getType(),\n",
    "projectName": "apache.flink",
    "bugLineNum": 309,
    "bugNodeStartChar": 11605,
    "bugNodeLength": 18,
    "fixLineNum": 309,
    "fixNodeStartChar": 11605,
    "fixNodeLength": 22,
    "sourceBeforeFix": "JoinFunction.class",
    "sourceAfterFix": "FlatJoinFunction.class"
  },
  {
    "bugType": "SWAP_ARGUMENTS",
    "fixCommitSHA1": "a88b6132e5bc4f7126303ffa4cda4c1e535f8022",
    "fixCommitParentSHA1": "014595055aad3e044d17e1290b94e8f6846276ce",
    "bugFilePath": "flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java",
    "fixPatch": "diff --git a/flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java b/flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java\nindex c7050e5..ca69e80 100644\n--- a/flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java\n+++ b/flink-tests/src/test/java/org/apache/flink/test/classloading/ClassLoaderITCase.java\n@@ -312,7 +312,7 @@\n \t\t\t}\n \t\t}\n \n-\t\tassertNotNull(savepointPath, \"Failed to trigger savepoint\");\n+\t\tassertNotNull(\"Failed to trigger savepoint\", savepointPath);\n \n \t\t// Upload JAR\n \t\tLOG.info(\"Uploading JAR \" + CUSTOM_KV_STATE_JAR_PATH + \" for savepoint disposal.\");\n",
    "projectName": "apache.flink",
    "bugLineNum": 315,
    "bugNodeStartChar": 11350,
    "bugNodeLength": 59,
    "fixLineNum": 315,
    "fixNodeStartChar": 11350,
    "fixNodeLength": 59,
    "sourceBeforeFix": "assertNotNull(savepointPath,\"Failed to trigger savepoint\")",
    "sourceAfterFix": "assertNotNull(\"Failed to trigger savepoint\",savepointPath)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "f4c336b16bb9219f438551ba069707c21e555de5",
    "fixCommitParentSHA1": "bb26c9e1b7ed16c78cf4c3b3ae2d4483c526e5c3",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoader.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoader.java b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoader.java\nindex 1819120..66740c7 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoader.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoader.java\n@@ -74,12 +74,12 @@\n \t\t\t\t\tString msg = String.format(\"Failed to rollback to savepoint %s. \" +\n \t\t\t\t\t\t\t\t\t\"Max parallelism mismatch between savepoint state and new program. \" +\n \t\t\t\t\t\t\t\t\t\"Cannot map operator %s with max parallelism %d to new program with \" +\n-\t\t\t\t\t\t\t\t\t\"parallelism %d. This indicates that the program has been changed \" +\n+\t\t\t\t\t\t\t\t\t\"max parallelism %d. This indicates that the program has been changed \" +\n \t\t\t\t\t\t\t\t\t\"in a non-compatible way after the savepoint.\",\n \t\t\t\t\t\t\tsavepoint,\n \t\t\t\t\t\t\ttaskState.getJobVertexID(),\n \t\t\t\t\t\t\ttaskState.getMaxParallelism(),\n-\t\t\t\t\t\t\texecutionJobVertex.getParallelism());\n+\t\t\t\t\t\t\texecutionJobVertex.getMaxParallelism());\n \n \t\t\t\t\tthrow new IllegalStateException(msg);\n \t\t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 82,
    "bugNodeStartChar": 3457,
    "bugNodeLength": 35,
    "fixLineNum": 82,
    "fixNodeStartChar": 3457,
    "fixNodeLength": 38,
    "sourceBeforeFix": "executionJobVertex.getParallelism()",
    "sourceAfterFix": "executionJobVertex.getMaxParallelism()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "f4c336b16bb9219f438551ba069707c21e555de5",
    "fixCommitParentSHA1": "bb26c9e1b7ed16c78cf4c3b3ae2d4483c526e5c3",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoader.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoader.java b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoader.java\nindex 1819120..66740c7 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoader.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointLoader.java\n@@ -74,12 +74,12 @@\n \t\t\t\t\tString msg = String.format(\"Failed to rollback to savepoint %s. \" +\n \t\t\t\t\t\t\t\t\t\"Max parallelism mismatch between savepoint state and new program. \" +\n \t\t\t\t\t\t\t\t\t\"Cannot map operator %s with max parallelism %d to new program with \" +\n-\t\t\t\t\t\t\t\t\t\"parallelism %d. This indicates that the program has been changed \" +\n+\t\t\t\t\t\t\t\t\t\"max parallelism %d. This indicates that the program has been changed \" +\n \t\t\t\t\t\t\t\t\t\"in a non-compatible way after the savepoint.\",\n \t\t\t\t\t\t\tsavepoint,\n \t\t\t\t\t\t\ttaskState.getJobVertexID(),\n \t\t\t\t\t\t\ttaskState.getMaxParallelism(),\n-\t\t\t\t\t\t\texecutionJobVertex.getParallelism());\n+\t\t\t\t\t\t\texecutionJobVertex.getMaxParallelism());\n \n \t\t\t\t\tthrow new IllegalStateException(msg);\n \t\t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 82,
    "bugNodeStartChar": 3457,
    "bugNodeLength": 35,
    "fixLineNum": 82,
    "fixNodeStartChar": 3457,
    "fixNodeLength": 38,
    "sourceBeforeFix": "executionJobVertex.getParallelism()",
    "sourceAfterFix": "executionJobVertex.getMaxParallelism()"
  },
  {
    "bugType": "CHANGE_OPERATOR",
    "fixCommitSHA1": "335a2826481725aabc86de8068909060894a274b",
    "fixCommitParentSHA1": "181b54515ed2701a25d4a71fa7ed52394b2aeb66",
    "bugFilePath": "flink-batch-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCOutputFormat.java",
    "fixPatch": "diff --git a/flink-batch-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCOutputFormat.java b/flink-batch-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCOutputFormat.java\nindex 5464a94..da4b1ad 100644\n--- a/flink-batch-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCOutputFormat.java\n+++ b/flink-batch-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCOutputFormat.java\n@@ -108,7 +108,7 @@\n \t@Override\n \tpublic void writeRecord(Row row) throws IOException {\n \n-\t\tif (typesArray != null && typesArray.length > 0 && typesArray.length == row.productArity()) {\n+\t\tif (typesArray != null && typesArray.length > 0 && typesArray.length != row.productArity()) {\n \t\t\tLOG.warn(\"Column SQL types array doesn't match arity of passed Row! Check the passed array...\");\n \t\t} \n \t\ttry {\n",
    "projectName": "apache.flink",
    "bugLineNum": 111,
    "bugNodeStartChar": 3592,
    "bugNodeLength": 39,
    "fixLineNum": 111,
    "fixNodeStartChar": 3592,
    "fixNodeLength": 39,
    "sourceBeforeFix": "typesArray.length == row.productArity()",
    "sourceAfterFix": "typesArray.length != row.productArity()"
  },
  {
    "bugType": "CHANGE_OPERAND",
    "fixCommitSHA1": "7a25bf5cee9ab94525ed0284cbf399d2c33f70cf",
    "fixCommitParentSHA1": "0975d9f11dc09f8b1ea420d660175874d423cac3",
    "bugFilePath": "flink-core/src/main/java/org/apache/flink/core/memory/MemorySegment.java",
    "fixPatch": "diff --git a/flink-core/src/main/java/org/apache/flink/core/memory/MemorySegment.java b/flink-core/src/main/java/org/apache/flink/core/memory/MemorySegment.java\nindex af3efe7..d8315c5 100644\n--- a/flink-core/src/main/java/org/apache/flink/core/memory/MemorySegment.java\n+++ b/flink-core/src/main/java/org/apache/flink/core/memory/MemorySegment.java\n@@ -161,7 +161,7 @@\n \t\t}\n \t\tif (offHeapAddress >= Long.MAX_VALUE - Integer.MAX_VALUE) {\n \t\t\t// this is necessary to make sure the collapsed checks are safe against numeric overflows\n-\t\t\tthrow new IllegalArgumentException(\"Segment initialized with too large address: \" + address\n+\t\t\tthrow new IllegalArgumentException(\"Segment initialized with too large address: \" + offHeapAddress\n \t\t\t\t\t+ \" ; Max allowed address is \" + (Long.MAX_VALUE - Integer.MAX_VALUE - 1));\n \t\t}\n \t\t\n",
    "projectName": "apache.flink",
    "bugLineNum": 164,
    "bugNodeStartChar": 8059,
    "bugNodeLength": 135,
    "fixLineNum": 164,
    "fixNodeStartChar": 8059,
    "fixNodeLength": 142,
    "sourceBeforeFix": "\"Segment initialized with too large address: \" + address + \" ; Max allowed address is \"+ (Long.MAX_VALUE - Integer.MAX_VALUE - 1)",
    "sourceAfterFix": "\"Segment initialized with too large address: \" + offHeapAddress + \" ; Max allowed address is \"+ (Long.MAX_VALUE - Integer.MAX_VALUE - 1)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "7a25bf5cee9ab94525ed0284cbf399d2c33f70cf",
    "fixCommitParentSHA1": "0975d9f11dc09f8b1ea420d660175874d423cac3",
    "bugFilePath": "flink-core/src/main/java/org/apache/flink/core/memory/MemorySegment.java",
    "fixPatch": "diff --git a/flink-core/src/main/java/org/apache/flink/core/memory/MemorySegment.java b/flink-core/src/main/java/org/apache/flink/core/memory/MemorySegment.java\nindex af3efe7..d8315c5 100644\n--- a/flink-core/src/main/java/org/apache/flink/core/memory/MemorySegment.java\n+++ b/flink-core/src/main/java/org/apache/flink/core/memory/MemorySegment.java\n@@ -161,7 +161,7 @@\n \t\t}\n \t\tif (offHeapAddress >= Long.MAX_VALUE - Integer.MAX_VALUE) {\n \t\t\t// this is necessary to make sure the collapsed checks are safe against numeric overflows\n-\t\t\tthrow new IllegalArgumentException(\"Segment initialized with too large address: \" + address\n+\t\t\tthrow new IllegalArgumentException(\"Segment initialized with too large address: \" + offHeapAddress\n \t\t\t\t\t+ \" ; Max allowed address is \" + (Long.MAX_VALUE - Integer.MAX_VALUE - 1));\n \t\t}\n \t\t\n",
    "projectName": "apache.flink",
    "bugLineNum": 164,
    "bugNodeStartChar": 8059,
    "bugNodeLength": 135,
    "fixLineNum": 164,
    "fixNodeStartChar": 8059,
    "fixNodeLength": 142,
    "sourceBeforeFix": "\"Segment initialized with too large address: \" + address + \" ; Max allowed address is \"+ (Long.MAX_VALUE - Integer.MAX_VALUE - 1)",
    "sourceAfterFix": "\"Segment initialized with too large address: \" + offHeapAddress + \" ; Max allowed address is \"+ (Long.MAX_VALUE - Integer.MAX_VALUE - 1)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "3f3bab10b9ca68eb31a7ef5a31e49145b51006fd",
    "fixCommitParentSHA1": "cd98e85ddd3c35e5900713266fc38916b53f172d",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java\nindex 8de29a6..fcdc2ca 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java\n@@ -138,7 +138,7 @@\n \t\t\t\t\tserializedValue = new SerializedValue<>(null);\n \t\t\t\t} else {\n \t\t\t\t\tbyte[] serializedData = new byte[length];\n-\t\t\t\t\tdis.read(serializedData, 0, length);\n+\t\t\t\t\tdis.readFully(serializedData, 0, length);\n \t\t\t\t\tserializedValue = SerializedValue.fromBytes(serializedData);\n \t\t\t\t}\n \n@@ -165,7 +165,7 @@\n \t\t\t\t\tserializedValue = new SerializedValue<>(null);\n \t\t\t\t} else {\n \t\t\t\t\tbyte[] serializedData = new byte[length];\n-\t\t\t\t\tdis.read(serializedData, 0, length);\n+\t\t\t\t\tdis.readFully(serializedData, 0, length);\n \t\t\t\t\tserializedValue = SerializedValue.fromBytes(serializedData);\n \t\t\t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 141,
    "bugNodeStartChar": 4586,
    "bugNodeLength": 35,
    "fixLineNum": 141,
    "fixNodeStartChar": 4586,
    "fixNodeLength": 40,
    "sourceBeforeFix": "dis.read(serializedData,0,length)",
    "sourceAfterFix": "dis.readFully(serializedData,0,length)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "3f3bab10b9ca68eb31a7ef5a31e49145b51006fd",
    "fixCommitParentSHA1": "cd98e85ddd3c35e5900713266fc38916b53f172d",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java\nindex 8de29a6..fcdc2ca 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java\n@@ -138,7 +138,7 @@\n \t\t\t\t\tserializedValue = new SerializedValue<>(null);\n \t\t\t\t} else {\n \t\t\t\t\tbyte[] serializedData = new byte[length];\n-\t\t\t\t\tdis.read(serializedData, 0, length);\n+\t\t\t\t\tdis.readFully(serializedData, 0, length);\n \t\t\t\t\tserializedValue = SerializedValue.fromBytes(serializedData);\n \t\t\t\t}\n \n@@ -165,7 +165,7 @@\n \t\t\t\t\tserializedValue = new SerializedValue<>(null);\n \t\t\t\t} else {\n \t\t\t\t\tbyte[] serializedData = new byte[length];\n-\t\t\t\t\tdis.read(serializedData, 0, length);\n+\t\t\t\t\tdis.readFully(serializedData, 0, length);\n \t\t\t\t\tserializedValue = SerializedValue.fromBytes(serializedData);\n \t\t\t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 141,
    "bugNodeStartChar": 4586,
    "bugNodeLength": 35,
    "fixLineNum": 141,
    "fixNodeStartChar": 4586,
    "fixNodeLength": 40,
    "sourceBeforeFix": "dis.read(serializedData,0,length)",
    "sourceAfterFix": "dis.readFully(serializedData,0,length)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "3f3bab10b9ca68eb31a7ef5a31e49145b51006fd",
    "fixCommitParentSHA1": "cd98e85ddd3c35e5900713266fc38916b53f172d",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java\nindex 8de29a6..fcdc2ca 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java\n@@ -138,7 +138,7 @@\n \t\t\t\t\tserializedValue = new SerializedValue<>(null);\n \t\t\t\t} else {\n \t\t\t\t\tbyte[] serializedData = new byte[length];\n-\t\t\t\t\tdis.read(serializedData, 0, length);\n+\t\t\t\t\tdis.readFully(serializedData, 0, length);\n \t\t\t\t\tserializedValue = SerializedValue.fromBytes(serializedData);\n \t\t\t\t}\n \n@@ -165,7 +165,7 @@\n \t\t\t\t\tserializedValue = new SerializedValue<>(null);\n \t\t\t\t} else {\n \t\t\t\t\tbyte[] serializedData = new byte[length];\n-\t\t\t\t\tdis.read(serializedData, 0, length);\n+\t\t\t\t\tdis.readFully(serializedData, 0, length);\n \t\t\t\t\tserializedValue = SerializedValue.fromBytes(serializedData);\n \t\t\t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 168,
    "bugNodeStartChar": 5305,
    "bugNodeLength": 35,
    "fixLineNum": 168,
    "fixNodeStartChar": 5305,
    "fixNodeLength": 40,
    "sourceBeforeFix": "dis.read(serializedData,0,length)",
    "sourceAfterFix": "dis.readFully(serializedData,0,length)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "3f3bab10b9ca68eb31a7ef5a31e49145b51006fd",
    "fixCommitParentSHA1": "cd98e85ddd3c35e5900713266fc38916b53f172d",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java\nindex 8de29a6..fcdc2ca 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/savepoint/SavepointV1Serializer.java\n@@ -138,7 +138,7 @@\n \t\t\t\t\tserializedValue = new SerializedValue<>(null);\n \t\t\t\t} else {\n \t\t\t\t\tbyte[] serializedData = new byte[length];\n-\t\t\t\t\tdis.read(serializedData, 0, length);\n+\t\t\t\t\tdis.readFully(serializedData, 0, length);\n \t\t\t\t\tserializedValue = SerializedValue.fromBytes(serializedData);\n \t\t\t\t}\n \n@@ -165,7 +165,7 @@\n \t\t\t\t\tserializedValue = new SerializedValue<>(null);\n \t\t\t\t} else {\n \t\t\t\t\tbyte[] serializedData = new byte[length];\n-\t\t\t\t\tdis.read(serializedData, 0, length);\n+\t\t\t\t\tdis.readFully(serializedData, 0, length);\n \t\t\t\t\tserializedValue = SerializedValue.fromBytes(serializedData);\n \t\t\t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 168,
    "bugNodeStartChar": 5305,
    "bugNodeLength": 35,
    "fixLineNum": 168,
    "fixNodeStartChar": 5305,
    "fixNodeLength": 40,
    "sourceBeforeFix": "dis.read(serializedData,0,length)",
    "sourceAfterFix": "dis.readFully(serializedData,0,length)"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "0360fb968378b3439c9c8724e5f3183de06d3c80",
    "fixCommitParentSHA1": "177168b2fdefc3ca961e9615ffe04d5fe08eca08",
    "bugFilePath": "flink-libraries/flink-gelly/src/main/java/org/apache/flink/graph/generator/random/AbstractGeneratorFactory.java",
    "fixPatch": "diff --git a/flink-libraries/flink-gelly/src/main/java/org/apache/flink/graph/generator/random/AbstractGeneratorFactory.java b/flink-libraries/flink-gelly/src/main/java/org/apache/flink/graph/generator/random/AbstractGeneratorFactory.java\nindex 3bb904e..fc9e1ba 100644\n--- a/flink-libraries/flink-gelly/src/main/java/org/apache/flink/graph/generator/random/AbstractGeneratorFactory.java\n+++ b/flink-libraries/flink-gelly/src/main/java/org/apache/flink/graph/generator/random/AbstractGeneratorFactory.java\n@@ -35,7 +35,7 @@\n \t// A large computation will run in parallel but blocks are generated on\n \t// and distributed from a single node. This limit should be greater\n \t// than the maximum expected parallelism.\n-\tpublic static final int MAXIMUM_BLOCK_COUNT = 1 << 20;\n+\tpublic static final int MAXIMUM_BLOCK_COUNT = 1 << 15;\n \n \t// This should be sufficiently large relative to the cost of instantiating\n \t// and initializing the random generator and sufficiently small relative to\n",
    "projectName": "apache.flink",
    "bugLineNum": 38,
    "bugNodeStartChar": 1505,
    "bugNodeLength": 7,
    "fixLineNum": 38,
    "fixNodeStartChar": 1505,
    "fixNodeLength": 7,
    "sourceBeforeFix": "1 << 20",
    "sourceAfterFix": "1 << 15"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "ccc86e9f1cddebc731ac1ccabdd469df11d72d8b",
    "fixCommitParentSHA1": "6cc1c179af5e48b97d66fd3bcbd23f6704d43f62",
    "bugFilePath": "flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java",
    "fixPatch": "diff --git a/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java b/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java\nindex 3ba8cff..660f24c 100644\n--- a/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java\n+++ b/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java\n@@ -357,7 +357,7 @@\n \t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(\"localhost\", flinkPort);\n \t\tenv.enableCheckpointing(500);\n \t\tenv.setParallelism(parallelism);\n-\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 0));\n+\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0));\n \t\tenv.getConfig().disableSysoutLogging();\n \n \t\tFlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, standardProps);\n@@ -402,7 +402,7 @@\n \t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(\"localhost\", flinkPort);\n \t\tenv.enableCheckpointing(500);\n \t\tenv.setParallelism(parallelism);\n-\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 0));\n+\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0));\n \t\tenv.getConfig().disableSysoutLogging();\n \n \t\tFlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, standardProps);\n@@ -447,7 +447,8 @@\n \t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(\"localhost\", flinkPort);\n \t\tenv.enableCheckpointing(500);\n \t\tenv.setParallelism(parallelism);\n-\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 0));\n+\t\t// set the number of restarts to one. The failing mapper will fail once, then it's only success exceptions.\n+\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0));\n \t\tenv.getConfig().disableSysoutLogging();\n \t\tenv.setBufferTimeout(0);\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 360,
    "bugNodeStartChar": 15034,
    "bugNodeLength": 41,
    "fixLineNum": 360,
    "fixNodeStartChar": 15034,
    "fixNodeLength": 41,
    "sourceBeforeFix": "RestartStrategies.fixedDelayRestart(3,0)",
    "sourceAfterFix": "RestartStrategies.fixedDelayRestart(1,0)"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "ccc86e9f1cddebc731ac1ccabdd469df11d72d8b",
    "fixCommitParentSHA1": "6cc1c179af5e48b97d66fd3bcbd23f6704d43f62",
    "bugFilePath": "flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java",
    "fixPatch": "diff --git a/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java b/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java\nindex 3ba8cff..660f24c 100644\n--- a/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java\n+++ b/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java\n@@ -357,7 +357,7 @@\n \t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(\"localhost\", flinkPort);\n \t\tenv.enableCheckpointing(500);\n \t\tenv.setParallelism(parallelism);\n-\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 0));\n+\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0));\n \t\tenv.getConfig().disableSysoutLogging();\n \n \t\tFlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, standardProps);\n@@ -402,7 +402,7 @@\n \t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(\"localhost\", flinkPort);\n \t\tenv.enableCheckpointing(500);\n \t\tenv.setParallelism(parallelism);\n-\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 0));\n+\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0));\n \t\tenv.getConfig().disableSysoutLogging();\n \n \t\tFlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, standardProps);\n@@ -447,7 +447,8 @@\n \t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(\"localhost\", flinkPort);\n \t\tenv.enableCheckpointing(500);\n \t\tenv.setParallelism(parallelism);\n-\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 0));\n+\t\t// set the number of restarts to one. The failing mapper will fail once, then it's only success exceptions.\n+\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0));\n \t\tenv.getConfig().disableSysoutLogging();\n \t\tenv.setBufferTimeout(0);\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 405,
    "bugNodeStartChar": 16746,
    "bugNodeLength": 41,
    "fixLineNum": 405,
    "fixNodeStartChar": 16746,
    "fixNodeLength": 41,
    "sourceBeforeFix": "RestartStrategies.fixedDelayRestart(3,0)",
    "sourceAfterFix": "RestartStrategies.fixedDelayRestart(1,0)"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "ccc86e9f1cddebc731ac1ccabdd469df11d72d8b",
    "fixCommitParentSHA1": "6cc1c179af5e48b97d66fd3bcbd23f6704d43f62",
    "bugFilePath": "flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java",
    "fixPatch": "diff --git a/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java b/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java\nindex 3ba8cff..660f24c 100644\n--- a/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java\n+++ b/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java\n@@ -357,7 +357,7 @@\n \t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(\"localhost\", flinkPort);\n \t\tenv.enableCheckpointing(500);\n \t\tenv.setParallelism(parallelism);\n-\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 0));\n+\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0));\n \t\tenv.getConfig().disableSysoutLogging();\n \n \t\tFlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, standardProps);\n@@ -402,7 +402,7 @@\n \t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(\"localhost\", flinkPort);\n \t\tenv.enableCheckpointing(500);\n \t\tenv.setParallelism(parallelism);\n-\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 0));\n+\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0));\n \t\tenv.getConfig().disableSysoutLogging();\n \n \t\tFlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, standardProps);\n@@ -447,7 +447,8 @@\n \t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment(\"localhost\", flinkPort);\n \t\tenv.enableCheckpointing(500);\n \t\tenv.setParallelism(parallelism);\n-\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(3, 0));\n+\t\t// set the number of restarts to one. The failing mapper will fail once, then it's only success exceptions.\n+\t\tenv.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0));\n \t\tenv.getConfig().disableSysoutLogging();\n \t\tenv.setBufferTimeout(0);\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 450,
    "bugNodeStartChar": 18480,
    "bugNodeLength": 41,
    "fixLineNum": 451,
    "fixNodeStartChar": 18590,
    "fixNodeLength": 41,
    "sourceBeforeFix": "RestartStrategies.fixedDelayRestart(3,0)",
    "sourceAfterFix": "RestartStrategies.fixedDelayRestart(1,0)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "a24df987d4ad2f5661f723d49d44a0e125bc26cb",
    "fixCommitParentSHA1": "f916ef41a52a7befdf08da6c2cef7f82ada379c3",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java b/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java\nindex 99237a3..830269d 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java\n@@ -108,7 +108,7 @@\n \tstatic File getIncomingDirectory(File storageDir) {\n \t\tfinal File incomingDir = new File(storageDir, \"incoming\");\n \n-\t\tif (!incomingDir.exists() && !incomingDir.mkdirs()) {\n+\t\tif (!incomingDir.mkdirs() && !incomingDir.exists()) {\n \t\t\tthrow new RuntimeException(\"Cannot create directory for incoming files \" + incomingDir.getAbsolutePath());\n \t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 111,
    "bugNodeStartChar": 3424,
    "bugNodeLength": 20,
    "fixLineNum": 111,
    "fixNodeStartChar": 3424,
    "fixNodeLength": 20,
    "sourceBeforeFix": "incomingDir.exists()",
    "sourceAfterFix": "incomingDir.mkdirs()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "a24df987d4ad2f5661f723d49d44a0e125bc26cb",
    "fixCommitParentSHA1": "f916ef41a52a7befdf08da6c2cef7f82ada379c3",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java b/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java\nindex 99237a3..830269d 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/blob/BlobUtils.java\n@@ -108,7 +108,7 @@\n \tstatic File getIncomingDirectory(File storageDir) {\n \t\tfinal File incomingDir = new File(storageDir, \"incoming\");\n \n-\t\tif (!incomingDir.exists() && !incomingDir.mkdirs()) {\n+\t\tif (!incomingDir.mkdirs() && !incomingDir.exists()) {\n \t\t\tthrow new RuntimeException(\"Cannot create directory for incoming files \" + incomingDir.getAbsolutePath());\n \t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 111,
    "bugNodeStartChar": 3424,
    "bugNodeLength": 20,
    "fixLineNum": 111,
    "fixNodeStartChar": 3424,
    "fixNodeLength": 20,
    "sourceBeforeFix": "incomingDir.exists()",
    "sourceAfterFix": "incomingDir.mkdirs()"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "bd1c245cbffa4781672c63d144277bb4e244d7f6",
    "fixCommitParentSHA1": "c71675f7cbe7e538d62bf1491aff69b369eda9eb",
    "bugFilePath": "flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java",
    "fixPatch": "diff --git a/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java b/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java\nindex cc9205c..3ba8cff 100644\n--- a/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java\n+++ b/flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java\n@@ -840,7 +840,7 @@\n \n \t\t// add producing topology\n \t\tProperties producerProps = new Properties();\n-\t\tproducerProps.setProperty(\"max.request.size\", Integer.toString(1024 * 1024 * 14));\n+\t\tproducerProps.setProperty(\"max.request.size\", Integer.toString(1024 * 1024 * 15));\n \t\tproducerProps.setProperty(\"retries\", \"3\");\n \t\tproducerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings);\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 843,
    "bugNodeStartChar": 31073,
    "bugNodeLength": 16,
    "fixLineNum": 843,
    "fixNodeStartChar": 31073,
    "fixNodeLength": 16,
    "sourceBeforeFix": "1024 * 1024 * 14",
    "sourceAfterFix": "1024 * 1024 * 15"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "96b353d98f6b6d441ebedf69ec12cfa333a1d7c9",
    "fixCommitParentSHA1": "48b469ad4f0da466b347071cea82913965645de3",
    "bugFilePath": "flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/GroupReduceITCase.java",
    "fixPatch": "diff --git a/flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/GroupReduceITCase.java b/flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/GroupReduceITCase.java\nindex 075e60c..6f93722 100644\n--- a/flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/GroupReduceITCase.java\n+++ b/flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/GroupReduceITCase.java\n@@ -1033,7 +1033,7 @@\n \n \t\t// check if automatic type registration with Kryo worked\n \t\tAssert.assertTrue(ec.getRegisteredKryoTypes().contains(BigInt.class));\n-\t\tAssert.assertTrue(ec.getRegisteredKryoTypes().contains(java.sql.Date.class));\n+\t\tAssert.assertFalse(ec.getRegisteredKryoTypes().contains(java.sql.Date.class));\n \n \t\tString expected = null;\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 1036,
    "bugNodeStartChar": 34872,
    "bugNodeLength": 76,
    "fixLineNum": 1036,
    "fixNodeStartChar": 34872,
    "fixNodeLength": 77,
    "sourceBeforeFix": "Assert.assertTrue(ec.getRegisteredKryoTypes().contains(java.sql.Date.class))",
    "sourceAfterFix": "Assert.assertFalse(ec.getRegisteredKryoTypes().contains(java.sql.Date.class))"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "96b353d98f6b6d441ebedf69ec12cfa333a1d7c9",
    "fixCommitParentSHA1": "48b469ad4f0da466b347071cea82913965645de3",
    "bugFilePath": "flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/GroupReduceITCase.java",
    "fixPatch": "diff --git a/flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/GroupReduceITCase.java b/flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/GroupReduceITCase.java\nindex 075e60c..6f93722 100644\n--- a/flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/GroupReduceITCase.java\n+++ b/flink-tests/src/test/java/org/apache/flink/test/javaApiOperators/GroupReduceITCase.java\n@@ -1033,7 +1033,7 @@\n \n \t\t// check if automatic type registration with Kryo worked\n \t\tAssert.assertTrue(ec.getRegisteredKryoTypes().contains(BigInt.class));\n-\t\tAssert.assertTrue(ec.getRegisteredKryoTypes().contains(java.sql.Date.class));\n+\t\tAssert.assertFalse(ec.getRegisteredKryoTypes().contains(java.sql.Date.class));\n \n \t\tString expected = null;\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 1036,
    "bugNodeStartChar": 34872,
    "bugNodeLength": 76,
    "fixLineNum": 1036,
    "fixNodeStartChar": 34872,
    "fixNodeLength": 77,
    "sourceBeforeFix": "Assert.assertTrue(ec.getRegisteredKryoTypes().contains(java.sql.Date.class))",
    "sourceAfterFix": "Assert.assertFalse(ec.getRegisteredKryoTypes().contains(java.sql.Date.class))"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "016644ac3c91e516f93fd863a3d91e81a4bd8f3c",
    "fixCommitParentSHA1": "677425e529d80f024b6dbcdeec23506482a511f8",
    "bugFilePath": "flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/graph/PageRank.java",
    "fixPatch": "diff --git a/flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/graph/PageRank.java b/flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/graph/PageRank.java\nindex f8a5c83..c613a7e 100644\n--- a/flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/graph/PageRank.java\n+++ b/flink-examples/flink-examples-batch/src/main/java/org/apache/flink/examples/java/graph/PageRank.java\n@@ -85,7 +85,7 @@\n \t\tParameterTool params = ParameterTool.fromArgs(args);\n \t\tSystem.out.println(\"Usage: PageRankBasic --pages <path> --links <path> --output <path> --numPages <n> --iterations <n>\");\n \n-\t\tfinal int numPages = params.getInt(\"numPages\");\n+\t\tfinal int numPages = params.getInt(\"numPages\", PageRankData.getNumberOfPages());\n \t\tfinal int maxIterations = params.getInt(\"iterations\", 10);\n \t\t\n \t\t// set up execution environment\n",
    "projectName": "apache.flink",
    "bugLineNum": 88,
    "bugNodeStartChar": 4201,
    "bugNodeLength": 25,
    "fixLineNum": 88,
    "fixNodeStartChar": 4201,
    "fixNodeLength": 58,
    "sourceBeforeFix": "params.getInt(\"numPages\")",
    "sourceAfterFix": "params.getInt(\"numPages\",PageRankData.getNumberOfPages())"
  },
  {
    "bugType": "SWAP_BOOLEAN_LITERAL",
    "fixCommitSHA1": "a5b05566ebeb1504da6579b21f424fbb45a56148",
    "fixCommitParentSHA1": "a065ed0910fa5636a26c47b3ece5cb48cdbf33f7",
    "bugFilePath": "flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontendParser.java",
    "fixPatch": "diff --git a/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontendParser.java b/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontendParser.java\nindex 4e081fd..07d409e 100644\n--- a/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontendParser.java\n+++ b/flink-clients/src/main/java/org/apache/flink/client/cli/CliFrontendParser.java\n@@ -339,7 +339,7 @@\n \tpublic static InfoOptions parseInfoCommand(String[] args) throws CliArgsException {\n \t\ttry {\n \t\t\tPosixParser parser = new PosixParser();\n-\t\t\tCommandLine line = parser.parse(INFO_OPTIONS, args, false);\n+\t\t\tCommandLine line = parser.parse(INFO_OPTIONS, args, true);\n \t\t\treturn new InfoOptions(line);\n \t\t}\n \t\tcatch (ParseException e) {\n",
    "projectName": "apache.flink",
    "bugLineNum": 342,
    "bugNodeStartChar": 12825,
    "bugNodeLength": 39,
    "fixLineNum": 342,
    "fixNodeStartChar": 12825,
    "fixNodeLength": 38,
    "sourceBeforeFix": "parser.parse(INFO_OPTIONS,args,false)",
    "sourceAfterFix": "parser.parse(INFO_OPTIONS,args,true)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "6019f085f0d3dadc20a11f900784ce2796a89ce0",
    "fixCommitParentSHA1": "976bacc65908cc35382ddbbcdc80249a700bc2d3",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/broadcast/BroadcastVariableMaterialization.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/broadcast/BroadcastVariableMaterialization.java b/flink-runtime/src/main/java/org/apache/flink/runtime/broadcast/BroadcastVariableMaterialization.java\nindex 86e0111..eb7e311 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/broadcast/BroadcastVariableMaterialization.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/broadcast/BroadcastVariableMaterialization.java\n@@ -137,7 +137,7 @@\n \t\t\t\twhile ((element = readerIterator.next(element)) != null);\n \t\t\t\t\n \t\t\t\tsynchronized (materializationMonitor) {\n-\t\t\t\t\twhile (!this.materialized) {\n+\t\t\t\t\twhile (!this.materialized && !disposed) {\n \t\t\t\t\t\tmaterializationMonitor.wait();\n \t\t\t\t\t}\n \t\t\t\t}\n@@ -209,7 +209,7 @@\n \t\t\tthrow new IllegalStateException(\"The Broadcast Variable has been disposed\");\n \t\t}\n \t\t\n-\t\tsynchronized (this) {\n+\t\tsynchronized (references) {\n \t\t\tif (transformed != null) {\n \t\t\t\tif (transformed instanceof List) {\n \t\t\t\t\t@SuppressWarnings(\"unchecked\")\n@@ -233,7 +233,7 @@\n \t\t\tthrow new IllegalStateException(\"The Broadcast Variable has been disposed\");\n \t\t}\n \t\t\n-\t\tsynchronized (this) {\n+\t\tsynchronized (references) {\n \t\t\tif (transformed == null) {\n \t\t\t\ttransformed = initializer.initializeBroadcastVariable(data);\n \t\t\t\tdata = null;\n",
    "projectName": "apache.flink",
    "bugLineNum": 212,
    "bugNodeStartChar": 6905,
    "bugNodeLength": 4,
    "fixLineNum": 212,
    "fixNodeStartChar": 6905,
    "fixNodeLength": 10,
    "sourceBeforeFix": "synchronized (this) {   if (transformed != null) {     if (transformed instanceof List) {       @SuppressWarnings(\"unchecked\") List<T> casted=(List<T>)transformed;       return casted;     }  else {       throw new InitializationTypeConflictException(transformed.getClass());     }   }  else {     return data;   } } ",
    "sourceAfterFix": "synchronized (references) {   if (transformed != null) {     if (transformed instanceof List) {       @SuppressWarnings(\"unchecked\") List<T> casted=(List<T>)transformed;       return casted;     }  else {       throw new InitializationTypeConflictException(transformed.getClass());     }   }  else {     return data;   } } "
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "6019f085f0d3dadc20a11f900784ce2796a89ce0",
    "fixCommitParentSHA1": "976bacc65908cc35382ddbbcdc80249a700bc2d3",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/broadcast/BroadcastVariableMaterialization.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/broadcast/BroadcastVariableMaterialization.java b/flink-runtime/src/main/java/org/apache/flink/runtime/broadcast/BroadcastVariableMaterialization.java\nindex 86e0111..eb7e311 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/broadcast/BroadcastVariableMaterialization.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/broadcast/BroadcastVariableMaterialization.java\n@@ -137,7 +137,7 @@\n \t\t\t\twhile ((element = readerIterator.next(element)) != null);\n \t\t\t\t\n \t\t\t\tsynchronized (materializationMonitor) {\n-\t\t\t\t\twhile (!this.materialized) {\n+\t\t\t\t\twhile (!this.materialized && !disposed) {\n \t\t\t\t\t\tmaterializationMonitor.wait();\n \t\t\t\t\t}\n \t\t\t\t}\n@@ -209,7 +209,7 @@\n \t\t\tthrow new IllegalStateException(\"The Broadcast Variable has been disposed\");\n \t\t}\n \t\t\n-\t\tsynchronized (this) {\n+\t\tsynchronized (references) {\n \t\t\tif (transformed != null) {\n \t\t\t\tif (transformed instanceof List) {\n \t\t\t\t\t@SuppressWarnings(\"unchecked\")\n@@ -233,7 +233,7 @@\n \t\t\tthrow new IllegalStateException(\"The Broadcast Variable has been disposed\");\n \t\t}\n \t\t\n-\t\tsynchronized (this) {\n+\t\tsynchronized (references) {\n \t\t\tif (transformed == null) {\n \t\t\t\ttransformed = initializer.initializeBroadcastVariable(data);\n \t\t\t\tdata = null;\n",
    "projectName": "apache.flink",
    "bugLineNum": 236,
    "bugNodeStartChar": 7537,
    "bugNodeLength": 4,
    "fixLineNum": 236,
    "fixNodeStartChar": 7537,
    "fixNodeLength": 10,
    "sourceBeforeFix": "synchronized (this) {   if (transformed == null) {     transformed=initializer.initializeBroadcastVariable(data);     data=null;   }   return transformed; } ",
    "sourceAfterFix": "synchronized (references) {   if (transformed == null) {     transformed=initializer.initializeBroadcastVariable(data);     data=null;   }   return transformed; } "
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "30c46ed134b6660f4099973832074ab809ffa0d1",
    "fixCommitParentSHA1": "4c1cffd9d02d9ddaaf433a1882098c8423d97c28",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyClient.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyClient.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyClient.java\nindex fd6d980..6cab15d 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyClient.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyClient.java\n@@ -147,7 +147,7 @@\n \t\t// multiple clients running on the same host.\n \t\tString name = NettyConfig.CLIENT_THREAD_GROUP_NAME + \" (\" + config.getServerPort() + \")\";\n \n-\t\tEpollEventLoopGroup epollGroup = new EpollEventLoopGroup(config.getServerNumThreads(), NettyServer.getNamedThreadFactory(name));\n+\t\tEpollEventLoopGroup epollGroup = new EpollEventLoopGroup(config.getClientNumThreads(), NettyServer.getNamedThreadFactory(name));\n \t\tbootstrap.group(epollGroup).channel(EpollSocketChannel.class);\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 150,
    "bugNodeStartChar": 5016,
    "bugNodeLength": 28,
    "fixLineNum": 150,
    "fixNodeStartChar": 5016,
    "fixNodeLength": 28,
    "sourceBeforeFix": "config.getServerNumThreads()",
    "sourceAfterFix": "config.getClientNumThreads()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "30c46ed134b6660f4099973832074ab809ffa0d1",
    "fixCommitParentSHA1": "4c1cffd9d02d9ddaaf433a1882098c8423d97c28",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyClient.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyClient.java b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyClient.java\nindex fd6d980..6cab15d 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyClient.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/io/network/netty/NettyClient.java\n@@ -147,7 +147,7 @@\n \t\t// multiple clients running on the same host.\n \t\tString name = NettyConfig.CLIENT_THREAD_GROUP_NAME + \" (\" + config.getServerPort() + \")\";\n \n-\t\tEpollEventLoopGroup epollGroup = new EpollEventLoopGroup(config.getServerNumThreads(), NettyServer.getNamedThreadFactory(name));\n+\t\tEpollEventLoopGroup epollGroup = new EpollEventLoopGroup(config.getClientNumThreads(), NettyServer.getNamedThreadFactory(name));\n \t\tbootstrap.group(epollGroup).channel(EpollSocketChannel.class);\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 150,
    "bugNodeStartChar": 5016,
    "bugNodeLength": 28,
    "fixLineNum": 150,
    "fixNodeStartChar": 5016,
    "fixNodeLength": 28,
    "sourceBeforeFix": "config.getServerNumThreads()",
    "sourceAfterFix": "config.getClientNumThreads()"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "e9cad4da3a814bcde46c10c0e4da2fc84f99d815",
    "fixCommitParentSHA1": "20a464f5bd2a911d93262638d76015083ef49dd7",
    "bugFilePath": "flink-contrib/flink-storm-compatibility/flink-storm-compatibility-core/src/main/java/org/apache/flink/stormcompatibility/wrappers/AbstractStormSpoutWrapper.java",
    "fixPatch": "diff --git a/flink-contrib/flink-storm-compatibility/flink-storm-compatibility-core/src/main/java/org/apache/flink/stormcompatibility/wrappers/AbstractStormSpoutWrapper.java b/flink-contrib/flink-storm-compatibility/flink-storm-compatibility-core/src/main/java/org/apache/flink/stormcompatibility/wrappers/AbstractStormSpoutWrapper.java\nindex 65cde55..3021bcb 100644\n--- a/flink-contrib/flink-storm-compatibility/flink-storm-compatibility-core/src/main/java/org/apache/flink/stormcompatibility/wrappers/AbstractStormSpoutWrapper.java\n+++ b/flink-contrib/flink-storm-compatibility/flink-storm-compatibility-core/src/main/java/org/apache/flink/stormcompatibility/wrappers/AbstractStormSpoutWrapper.java\n@@ -52,7 +52,7 @@\n \t/**\n \t * Indicates, if the source is still running or was canceled.\n \t */\n-\tprotected boolean isRunning = true;\n+\tprotected volatile boolean isRunning = true;\n \n \t/**\n \t * Instantiates a new {@link AbstractStormSpoutWrapper} that wraps the given Storm {@link IRichSpout spout} such\n",
    "projectName": "apache.flink",
    "bugLineNum": 52,
    "bugNodeStartChar": 2312,
    "bugNodeLength": 108,
    "fixLineNum": 52,
    "fixNodeStartChar": 2312,
    "fixNodeLength": 117,
    "sourceBeforeFix": "4",
    "sourceAfterFix": "68"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "e9cad4da3a814bcde46c10c0e4da2fc84f99d815",
    "fixCommitParentSHA1": "20a464f5bd2a911d93262638d76015083ef49dd7",
    "bugFilePath": "flink-contrib/flink-storm-compatibility/flink-storm-compatibility-examples/src/main/java/org/apache/flink/stormcompatibility/excamation/StormExclamationLocal.java",
    "fixPatch": "diff --git a/flink-contrib/flink-storm-compatibility/flink-storm-compatibility-examples/src/main/java/org/apache/flink/stormcompatibility/excamation/StormExclamationLocal.java b/flink-contrib/flink-storm-compatibility/flink-storm-compatibility-examples/src/main/java/org/apache/flink/stormcompatibility/excamation/StormExclamationLocal.java\nindex c87fe8f..a25e5e0 100644\n--- a/flink-contrib/flink-storm-compatibility/flink-storm-compatibility-examples/src/main/java/org/apache/flink/stormcompatibility/excamation/StormExclamationLocal.java\n+++ b/flink-contrib/flink-storm-compatibility/flink-storm-compatibility-examples/src/main/java/org/apache/flink/stormcompatibility/excamation/StormExclamationLocal.java\n@@ -42,7 +42,7 @@\n \t\tfinal FlinkLocalCluster cluster = FlinkLocalCluster.getLocalCluster();\n \t\tcluster.submitTopology(topologyId, null, builder.createTopology());\n \n-\t\tUtils.sleep(5 * 1000);\n+\t\tUtils.sleep(10 * 1000);\n \n \t\t// TODO kill does no do anything so far\n \t\tcluster.killTopology(topologyId);\n",
    "projectName": "apache.flink",
    "bugLineNum": 45,
    "bugNodeStartChar": 1736,
    "bugNodeLength": 8,
    "fixLineNum": 45,
    "fixNodeStartChar": 1736,
    "fixNodeLength": 9,
    "sourceBeforeFix": "5 * 1000",
    "sourceAfterFix": "10 * 1000"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "e9cad4da3a814bcde46c10c0e4da2fc84f99d815",
    "fixCommitParentSHA1": "20a464f5bd2a911d93262638d76015083ef49dd7",
    "bugFilePath": "flink-contrib/flink-storm-compatibility/flink-storm-compatibility-examples/src/main/java/org/apache/flink/stormcompatibility/wordcount/StormWordCountLocal.java",
    "fixPatch": "diff --git a/flink-contrib/flink-storm-compatibility/flink-storm-compatibility-examples/src/main/java/org/apache/flink/stormcompatibility/wordcount/StormWordCountLocal.java b/flink-contrib/flink-storm-compatibility/flink-storm-compatibility-examples/src/main/java/org/apache/flink/stormcompatibility/wordcount/StormWordCountLocal.java\nindex 7b4f471..3fbd5b7 100644\n--- a/flink-contrib/flink-storm-compatibility/flink-storm-compatibility-examples/src/main/java/org/apache/flink/stormcompatibility/wordcount/StormWordCountLocal.java\n+++ b/flink-contrib/flink-storm-compatibility/flink-storm-compatibility-examples/src/main/java/org/apache/flink/stormcompatibility/wordcount/StormWordCountLocal.java\n@@ -65,7 +65,7 @@\n \t\tfinal FlinkLocalCluster cluster = FlinkLocalCluster.getLocalCluster();\n \t\tcluster.submitTopology(topologyId, null, builder.createTopology());\n \n-\t\tUtils.sleep(5 * 1000);\n+\t\tUtils.sleep(10 * 1000);\n \n \t\t// TODO kill does no do anything so far\n \t\tcluster.killTopology(topologyId);\n",
    "projectName": "apache.flink",
    "bugLineNum": 68,
    "bugNodeStartChar": 2747,
    "bugNodeLength": 8,
    "fixLineNum": 68,
    "fixNodeStartChar": 2747,
    "fixNodeLength": 9,
    "sourceBeforeFix": "5 * 1000",
    "sourceAfterFix": "10 * 1000"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "b7945bf9cd17ca5da324a05bef6afe2e15e5f22b",
    "fixCommitParentSHA1": "5c1aa3469414b3193cf9ae831452015635e23f60",
    "bugFilePath": "flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java",
    "fixPatch": "diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java\nindex 4c6ec1a..17ca34d 100644\n--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java\n+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java\n@@ -48,11 +48,11 @@\n  *      private volatile boolean isRunning;\n  *\n  *      @Override\n- *      public void run(Object checkpointLock, Collector<T> out) {\n+ *      public void run(SourceContext<T> ctx) {\n  *          isRunning = true;\n  *          while (isRunning && count < 1000) {\n- *              synchronized (checkpointLock) {\n- *                  out.collect(count);\n+ *              synchronized (ctx.getCheckpointLock()) {\n+ *                  ctx.collect(count);\n  *                  count++;\n  *              }\n  *          }\n@@ -104,17 +104,17 @@\n \t *\n \t * @param <T> The type of the elements produced by the source.\n \t */\n-\tpublic static interface SourceContext<T> {\n+\tinterface SourceContext<T> {\n \n \t\t/**\n \t\t * Emits one element from the source.\n \t\t */\n-\t\tpublic void collect(T element);\n+\t\tvoid collect(T element);\n \n \t\t/**\n \t\t * Returns the checkpoint lock. Please refer to the explanation about checkpointed sources\n \t\t * in {@link org.apache.flink.streaming.api.functions.source.SourceFunction}.\n \t\t */\n-\t\tpublic Object getCheckpointLock();\n+\t\tObject getCheckpointLock();\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 98,
    "bugNodeStartChar": 3786,
    "bugNodeLength": 882,
    "fixLineNum": 98,
    "fixNodeStartChar": 3786,
    "fixNodeLength": 868,
    "sourceBeforeFix": "9",
    "sourceAfterFix": "0"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "b7945bf9cd17ca5da324a05bef6afe2e15e5f22b",
    "fixCommitParentSHA1": "5c1aa3469414b3193cf9ae831452015635e23f60",
    "bugFilePath": "flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java",
    "fixPatch": "diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java\nindex 4c6ec1a..17ca34d 100644\n--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java\n+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java\n@@ -48,11 +48,11 @@\n  *      private volatile boolean isRunning;\n  *\n  *      @Override\n- *      public void run(Object checkpointLock, Collector<T> out) {\n+ *      public void run(SourceContext<T> ctx) {\n  *          isRunning = true;\n  *          while (isRunning && count < 1000) {\n- *              synchronized (checkpointLock) {\n- *                  out.collect(count);\n+ *              synchronized (ctx.getCheckpointLock()) {\n+ *                  ctx.collect(count);\n  *                  count++;\n  *              }\n  *          }\n@@ -104,17 +104,17 @@\n \t *\n \t * @param <T> The type of the elements produced by the source.\n \t */\n-\tpublic static interface SourceContext<T> {\n+\tinterface SourceContext<T> {\n \n \t\t/**\n \t\t * Emits one element from the source.\n \t\t */\n-\t\tpublic void collect(T element);\n+\t\tvoid collect(T element);\n \n \t\t/**\n \t\t * Returns the checkpoint lock. Please refer to the explanation about checkpointed sources\n \t\t * in {@link org.apache.flink.streaming.api.functions.source.SourceFunction}.\n \t\t */\n-\t\tpublic Object getCheckpointLock();\n+\t\tObject getCheckpointLock();\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 109,
    "bugNodeStartChar": 4359,
    "bugNodeLength": 83,
    "fixLineNum": 109,
    "fixNodeStartChar": 4359,
    "fixNodeLength": 76,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "0"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "b7945bf9cd17ca5da324a05bef6afe2e15e5f22b",
    "fixCommitParentSHA1": "5c1aa3469414b3193cf9ae831452015635e23f60",
    "bugFilePath": "flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java",
    "fixPatch": "diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java\nindex 4c6ec1a..17ca34d 100644\n--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java\n+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java\n@@ -48,11 +48,11 @@\n  *      private volatile boolean isRunning;\n  *\n  *      @Override\n- *      public void run(Object checkpointLock, Collector<T> out) {\n+ *      public void run(SourceContext<T> ctx) {\n  *          isRunning = true;\n  *          while (isRunning && count < 1000) {\n- *              synchronized (checkpointLock) {\n- *                  out.collect(count);\n+ *              synchronized (ctx.getCheckpointLock()) {\n+ *                  ctx.collect(count);\n  *                  count++;\n  *              }\n  *          }\n@@ -104,17 +104,17 @@\n \t *\n \t * @param <T> The type of the elements produced by the source.\n \t */\n-\tpublic static interface SourceContext<T> {\n+\tinterface SourceContext<T> {\n \n \t\t/**\n \t\t * Emits one element from the source.\n \t\t */\n-\t\tpublic void collect(T element);\n+\t\tvoid collect(T element);\n \n \t\t/**\n \t\t * Returns the checkpoint lock. Please refer to the explanation about checkpointed sources\n \t\t * in {@link org.apache.flink.streaming.api.functions.source.SourceFunction}.\n \t\t */\n-\t\tpublic Object getCheckpointLock();\n+\t\tObject getCheckpointLock();\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 114,
    "bugNodeStartChar": 4446,
    "bugNodeLength": 219,
    "fixLineNum": 114,
    "fixNodeStartChar": 4446,
    "fixNodeLength": 212,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "0"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "eba6f9a2f7bfd15a9adf15d356e699a0143bb5a1",
    "fixCommitParentSHA1": "0437722449c59fdc21fc5d18a59e5e0b961208af",
    "bugFilePath": "flink-staging/flink-tez/src/main/java/org/apache/flink/tez/runtime/TezRuntimeEnvironment.java",
    "fixPatch": "diff --git a/flink-staging/flink-tez/src/main/java/org/apache/flink/tez/runtime/TezRuntimeEnvironment.java b/flink-staging/flink-tez/src/main/java/org/apache/flink/tez/runtime/TezRuntimeEnvironment.java\nindex 39386e6..4cf855a 100644\n--- a/flink-staging/flink-tez/src/main/java/org/apache/flink/tez/runtime/TezRuntimeEnvironment.java\n+++ b/flink-staging/flink-tez/src/main/java/org/apache/flink/tez/runtime/TezRuntimeEnvironment.java\n@@ -36,7 +36,7 @@\n \tpublic TezRuntimeEnvironment(long totalMemory) {\n \t\tint pageSize = DEFAULT_PAGE_SIZE;\n \t\tint numSlots = DEFAULT_NUM_SLOTS;\n-\t\tthis.memoryManager = new DefaultMemoryManager(totalMemory, numSlots, pageSize);\n+\t\tthis.memoryManager = new DefaultMemoryManager(totalMemory, numSlots, pageSize, true);\n \t\tthis.ioManager = new IOManagerAsync();\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 39,
    "bugNodeStartChar": 1461,
    "bugNodeLength": 57,
    "fixLineNum": 39,
    "fixNodeStartChar": 1461,
    "fixNodeLength": 63,
    "sourceBeforeFix": "new DefaultMemoryManager(totalMemory,numSlots,pageSize)",
    "sourceAfterFix": "new DefaultMemoryManager(totalMemory,numSlots,pageSize,true)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "fdac963d5ccdb0397cc069d701bccd6bd04eae5e",
    "fixCommitParentSHA1": "0cfa43d79faebe7000cedfc71a8fa5de062422ff",
    "bugFilePath": "flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/graph/StreamGraph.java",
    "fixPatch": "diff --git a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/graph/StreamGraph.java b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/graph/StreamGraph.java\nindex 593b476..ffc7032 100644\n--- a/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/graph/StreamGraph.java\n+++ b/flink-staging/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/graph/StreamGraph.java\n@@ -196,7 +196,7 @@\n \n \t\tchaining = false;\n \n-\t\tStreamLoop iteration = new StreamLoop(iterationID, getStreamNode(iterationHead), timeOut);\n+\t\tStreamLoop iteration = new StreamLoop(iterationID, getStreamNode(vertexID), timeOut);\n \t\tstreamLoops.put(iterationID, iteration);\n \t\tvertexIDtoLoop.put(vertexID, iteration);\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 199,
    "bugNodeStartChar": 7065,
    "bugNodeLength": 28,
    "fixLineNum": 199,
    "fixNodeStartChar": 7065,
    "fixNodeLength": 23,
    "sourceBeforeFix": "getStreamNode(iterationHead)",
    "sourceAfterFix": "getStreamNode(vertexID)"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "d50e8bff665f2272ffc93e15099db69d4292a130",
    "fixCommitParentSHA1": "226e90584d8aca92a172f8856877eb0839b34cf4",
    "bugFilePath": "flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java",
    "fixPatch": "diff --git a/flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java b/flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java\nindex b51631c..3d2c2fa 100644\n--- a/flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java\n+++ b/flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java\n@@ -52,7 +52,7 @@\n \t\n \tprivate static final Logger LOG = LoggerFactory.getLogger(Utils.class);\n \n-\tprivate static final int DEFAULT_HEAP_LIMIT_CAP = 500;\n+\tprivate static final int DEFAULT_HEAP_LIMIT_CAP = 700;\n \tprivate static final float DEFAULT_YARN_HEAP_CUTOFF_RATIO = 0.8f;\n \n \t/**\n",
    "projectName": "apache.flink",
    "bugLineNum": 55,
    "bugNodeStartChar": 2265,
    "bugNodeLength": 28,
    "fixLineNum": 55,
    "fixNodeStartChar": 2265,
    "fixNodeLength": 28,
    "sourceBeforeFix": "DEFAULT_HEAP_LIMIT_CAP=500",
    "sourceAfterFix": "DEFAULT_HEAP_LIMIT_CAP=700"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "d50e8bff665f2272ffc93e15099db69d4292a130",
    "fixCommitParentSHA1": "226e90584d8aca92a172f8856877eb0839b34cf4",
    "bugFilePath": "flink-yarn/src/test/java/org/apache/flink/yarn/UtilsTests.java",
    "fixPatch": "diff --git a/flink-yarn/src/test/java/org/apache/flink/yarn/UtilsTests.java b/flink-yarn/src/test/java/org/apache/flink/yarn/UtilsTests.java\nindex 1d01b03..39a9c02 100644\n--- a/flink-yarn/src/test/java/org/apache/flink/yarn/UtilsTests.java\n+++ b/flink-yarn/src/test/java/org/apache/flink/yarn/UtilsTests.java\n@@ -27,6 +27,6 @@\n \n \t\t// ASSUMES DEFAULT Configuration values.\n \t\tAssert.assertEquals(800, Utils.calculateHeapSize(1000) );\n-\t\tAssert.assertEquals(9500, Utils.calculateHeapSize(10000) );\n+\t\tAssert.assertEquals(9300, Utils.calculateHeapSize(10000) );\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 30,
    "bugNodeStartChar": 1059,
    "bugNodeLength": 58,
    "fixLineNum": 30,
    "fixNodeStartChar": 1059,
    "fixNodeLength": 58,
    "sourceBeforeFix": "Assert.assertEquals(9500,Utils.calculateHeapSize(10000))",
    "sourceAfterFix": "Assert.assertEquals(9300,Utils.calculateHeapSize(10000))"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "dec5e333dfd7b5e4397a14f7a384a93508035638",
    "fixCommitParentSHA1": "6b79aca670485e4d617c910d282e81b225305f30",
    "bugFilePath": "flink-java/src/main/java/org/apache/flink/api/java/typeutils/AvroTypeInfo.java",
    "fixPatch": "diff --git a/flink-java/src/main/java/org/apache/flink/api/java/typeutils/AvroTypeInfo.java b/flink-java/src/main/java/org/apache/flink/api/java/typeutils/AvroTypeInfo.java\nindex bf580d1..ed03627 100644\n--- a/flink-java/src/main/java/org/apache/flink/api/java/typeutils/AvroTypeInfo.java\n+++ b/flink-java/src/main/java/org/apache/flink/api/java/typeutils/AvroTypeInfo.java\n@@ -54,7 +54,7 @@\n \t\tPojoTypeInfo pti =  (PojoTypeInfo) ti;\n \t\tList<PojoField> newFields = new ArrayList<PojoField>(pti.getTotalFields());\n \n-\t\tfor(int i = 0; i < pti.getTotalFields(); i++) {\n+\t\tfor(int i = 0; i < pti.getArity(); i++) {\n \t\t\tPojoField f = pti.getPojoFieldAt(i);\n \t\t\tTypeInformation newType = f.type;\n \t\t\t// check if type is a CharSequence\n",
    "projectName": "apache.flink",
    "bugLineNum": 57,
    "bugNodeStartChar": 2434,
    "bugNodeLength": 20,
    "fixLineNum": 57,
    "fixNodeStartChar": 2434,
    "fixNodeLength": 14,
    "sourceBeforeFix": "pti.getTotalFields()",
    "sourceAfterFix": "pti.getArity()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "dec5e333dfd7b5e4397a14f7a384a93508035638",
    "fixCommitParentSHA1": "6b79aca670485e4d617c910d282e81b225305f30",
    "bugFilePath": "flink-java/src/main/java/org/apache/flink/api/java/typeutils/AvroTypeInfo.java",
    "fixPatch": "diff --git a/flink-java/src/main/java/org/apache/flink/api/java/typeutils/AvroTypeInfo.java b/flink-java/src/main/java/org/apache/flink/api/java/typeutils/AvroTypeInfo.java\nindex bf580d1..ed03627 100644\n--- a/flink-java/src/main/java/org/apache/flink/api/java/typeutils/AvroTypeInfo.java\n+++ b/flink-java/src/main/java/org/apache/flink/api/java/typeutils/AvroTypeInfo.java\n@@ -54,7 +54,7 @@\n \t\tPojoTypeInfo pti =  (PojoTypeInfo) ti;\n \t\tList<PojoField> newFields = new ArrayList<PojoField>(pti.getTotalFields());\n \n-\t\tfor(int i = 0; i < pti.getTotalFields(); i++) {\n+\t\tfor(int i = 0; i < pti.getArity(); i++) {\n \t\t\tPojoField f = pti.getPojoFieldAt(i);\n \t\t\tTypeInformation newType = f.type;\n \t\t\t// check if type is a CharSequence\n",
    "projectName": "apache.flink",
    "bugLineNum": 57,
    "bugNodeStartChar": 2434,
    "bugNodeLength": 20,
    "fixLineNum": 57,
    "fixNodeStartChar": 2434,
    "fixNodeLength": 14,
    "sourceBeforeFix": "pti.getTotalFields()",
    "sourceAfterFix": "pti.getArity()"
  },
  {
    "bugType": "SWAP_BOOLEAN_LITERAL",
    "fixCommitSHA1": "d18401cef96b492b5a43c3eb7bbe0fe039582896",
    "fixCommitParentSHA1": "64c302f8f4d211a2cd10764e3090b4f9bdde436c",
    "bugFilePath": "flink-core/src/main/java/org/apache/flink/configuration/ConfigConstants.java",
    "fixPatch": "diff --git a/flink-core/src/main/java/org/apache/flink/configuration/ConfigConstants.java b/flink-core/src/main/java/org/apache/flink/configuration/ConfigConstants.java\nindex 767648a..e5da77f 100644\n--- a/flink-core/src/main/java/org/apache/flink/configuration/ConfigConstants.java\n+++ b/flink-core/src/main/java/org/apache/flink/configuration/ConfigConstants.java\n@@ -563,7 +563,7 @@\n \n \tpublic static int DEFAULT_AKKA_DISPATCHER_THROUGHPUT = 15;\n \n-\tpublic static boolean DEFAULT_AKKA_LOG_LIFECYCLE_EVENTS = false;\n+\tpublic static boolean DEFAULT_AKKA_LOG_LIFECYCLE_EVENTS = true;\n \n \tpublic static String DEFAULT_AKKA_FRAMESIZE = \"10485760b\";\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 566,
    "bugNodeStartChar": 19277,
    "bugNodeLength": 41,
    "fixLineNum": 566,
    "fixNodeStartChar": 19277,
    "fixNodeLength": 40,
    "sourceBeforeFix": "DEFAULT_AKKA_LOG_LIFECYCLE_EVENTS=false",
    "sourceAfterFix": "DEFAULT_AKKA_LOG_LIFECYCLE_EVENTS=true"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "055f6dc3eb8705dbbfb04e44965a85cbfc0b10db",
    "fixCommitParentSHA1": "1b40386d39d7848eff0d1e6cec7fa17a96ef4f6e",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java\nindex 961a464..ffb0dd4 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java\n@@ -127,10 +127,10 @@\n \t\t\t\t\tthrow new RuntimeException(\"RequestJob requires a response of type JobResponse. \" +\n \t\t\t\t\t\t\t\"Instead the response is of type \" + result.getClass());\n \t\t\t\t}else {\n-\t\t\t\t\tfinal JobResponse jobResponse = (JobResponse) response;\n+\t\t\t\t\tfinal JobResponse jobResponse = (JobResponse) result;\n \n \t\t\t\t\tif(jobResponse instanceof JobFound){\n-\t\t\t\t\t\tExecutionGraph archivedJob = ((JobFound)response).executionGraph();\n+\t\t\t\t\t\tExecutionGraph archivedJob = ((JobFound)result).executionGraph();\n \t\t\t\t\t\twriteJsonForArchivedJob(resp.getWriter(), archivedJob);\n \t\t\t\t} else {\n \t\t\t\t\t\tLOG.warn(\"DoGet:job: Could not find job for job ID \" + jobId);\n",
    "projectName": "apache.flink",
    "bugLineNum": 130,
    "bugNodeStartChar": 4998,
    "bugNodeLength": 22,
    "fixLineNum": 130,
    "fixNodeStartChar": 4998,
    "fixNodeLength": 20,
    "sourceBeforeFix": "(JobResponse)response",
    "sourceAfterFix": "(JobResponse)result"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "055f6dc3eb8705dbbfb04e44965a85cbfc0b10db",
    "fixCommitParentSHA1": "1b40386d39d7848eff0d1e6cec7fa17a96ef4f6e",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java\nindex 961a464..ffb0dd4 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/web/JobManagerInfoServlet.java\n@@ -127,10 +127,10 @@\n \t\t\t\t\tthrow new RuntimeException(\"RequestJob requires a response of type JobResponse. \" +\n \t\t\t\t\t\t\t\"Instead the response is of type \" + result.getClass());\n \t\t\t\t}else {\n-\t\t\t\t\tfinal JobResponse jobResponse = (JobResponse) response;\n+\t\t\t\t\tfinal JobResponse jobResponse = (JobResponse) result;\n \n \t\t\t\t\tif(jobResponse instanceof JobFound){\n-\t\t\t\t\t\tExecutionGraph archivedJob = ((JobFound)response).executionGraph();\n+\t\t\t\t\t\tExecutionGraph archivedJob = ((JobFound)result).executionGraph();\n \t\t\t\t\t\twriteJsonForArchivedJob(resp.getWriter(), archivedJob);\n \t\t\t\t} else {\n \t\t\t\t\t\tLOG.warn(\"DoGet:job: Could not find job for job ID \" + jobId);\n",
    "projectName": "apache.flink",
    "bugLineNum": 133,
    "bugNodeStartChar": 5101,
    "bugNodeLength": 18,
    "fixLineNum": 133,
    "fixNodeStartChar": 5101,
    "fixNodeLength": 16,
    "sourceBeforeFix": "(JobFound)response",
    "sourceAfterFix": "(JobFound)result"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "59970361c11b8177edbf56786e896ce4da82c6ba",
    "fixCommitParentSHA1": "47ec09e10ea6807f6c7f6a17273b3eeffe5958b7",
    "bugFilePath": "flink-java/src/main/java/org/apache/flink/api/java/typeutils/ValueTypeInfo.java",
    "fixPatch": "diff --git a/flink-java/src/main/java/org/apache/flink/api/java/typeutils/ValueTypeInfo.java b/flink-java/src/main/java/org/apache/flink/api/java/typeutils/ValueTypeInfo.java\nindex b3c25e4..2e40ff9 100644\n--- a/flink-java/src/main/java/org/apache/flink/api/java/typeutils/ValueTypeInfo.java\n+++ b/flink-java/src/main/java/org/apache/flink/api/java/typeutils/ValueTypeInfo.java\n@@ -96,10 +96,10 @@\n \t\t}\n \t\t\n \t\tif (CopyableValue.class.isAssignableFrom(type)) {\n-\t\t\treturn (TypeComparator<T>) new ValueComparator(sortOrderAscending, type);\n+\t\t\treturn (TypeComparator<T>) new CopyableValueComparator(sortOrderAscending, type);\n \t\t}\n \t\telse {\n-\t\t\treturn (TypeComparator<T>) new CopyableValueComparator(sortOrderAscending, type);\n+\t\t\treturn (TypeComparator<T>) new ValueComparator(sortOrderAscending, type);\n \t\t}\n \t}\n \t\n",
    "projectName": "apache.flink",
    "bugLineNum": 99,
    "bugNodeStartChar": 3021,
    "bugNodeLength": 45,
    "fixLineNum": 99,
    "fixNodeStartChar": 3021,
    "fixNodeLength": 53,
    "sourceBeforeFix": "new ValueComparator(sortOrderAscending,type)",
    "sourceAfterFix": "new CopyableValueComparator(sortOrderAscending,type)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "59970361c11b8177edbf56786e896ce4da82c6ba",
    "fixCommitParentSHA1": "47ec09e10ea6807f6c7f6a17273b3eeffe5958b7",
    "bugFilePath": "flink-java/src/main/java/org/apache/flink/api/java/typeutils/ValueTypeInfo.java",
    "fixPatch": "diff --git a/flink-java/src/main/java/org/apache/flink/api/java/typeutils/ValueTypeInfo.java b/flink-java/src/main/java/org/apache/flink/api/java/typeutils/ValueTypeInfo.java\nindex b3c25e4..2e40ff9 100644\n--- a/flink-java/src/main/java/org/apache/flink/api/java/typeutils/ValueTypeInfo.java\n+++ b/flink-java/src/main/java/org/apache/flink/api/java/typeutils/ValueTypeInfo.java\n@@ -96,10 +96,10 @@\n \t\t}\n \t\t\n \t\tif (CopyableValue.class.isAssignableFrom(type)) {\n-\t\t\treturn (TypeComparator<T>) new ValueComparator(sortOrderAscending, type);\n+\t\t\treturn (TypeComparator<T>) new CopyableValueComparator(sortOrderAscending, type);\n \t\t}\n \t\telse {\n-\t\t\treturn (TypeComparator<T>) new CopyableValueComparator(sortOrderAscending, type);\n+\t\t\treturn (TypeComparator<T>) new ValueComparator(sortOrderAscending, type);\n \t\t}\n \t}\n \t\n",
    "projectName": "apache.flink",
    "bugLineNum": 102,
    "bugNodeStartChar": 3111,
    "bugNodeLength": 53,
    "fixLineNum": 102,
    "fixNodeStartChar": 3111,
    "fixNodeLength": 45,
    "sourceBeforeFix": "new CopyableValueComparator(sortOrderAscending,type)",
    "sourceAfterFix": "new ValueComparator(sortOrderAscending,type)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "624fe959786106291c0b422f5e3d95f16ce9ff1f",
    "fixCommitParentSHA1": "578e406afdcc8fc28ddbe215be286becc11854fa",
    "bugFilePath": "flink-core/src/main/java/org/apache/flink/api/common/functions/util/FunctionUtils.java",
    "fixPatch": "diff --git a/flink-core/src/main/java/org/apache/flink/api/common/functions/util/FunctionUtils.java b/flink-core/src/main/java/org/apache/flink/api/common/functions/util/FunctionUtils.java\nindex 8ab2184..2486074 100644\n--- a/flink-core/src/main/java/org/apache/flink/api/common/functions/util/FunctionUtils.java\n+++ b/flink-core/src/main/java/org/apache/flink/api/common/functions/util/FunctionUtils.java\n@@ -99,7 +99,7 @@\n \t\t\tString className = (String) implClassMethod.invoke(serializedLambda);\n \t\t\tString methodName = (String) implMethodNameMethod.invoke(serializedLambda);\n \n-\t\t\tClass<?> implClass = Class.forName(className.replace('/', '.'));\n+\t\t\tClass<?> implClass = Class.forName(className.replace('/', '.'), true, Thread.currentThread().getContextClassLoader());\n \n \t\t\tMethod[] methods = implClass.getDeclaredMethods();\n \t\t\tMethod parameterizedMethod = null;\n@@ -121,7 +121,7 @@\n \t\t\treturn parameterizedMethod;\n \t\t}\n \t\tcatch(Exception e) {\n-\t\t\tthrow new RuntimeException(\"Could not extract lambda method out of function.\", e);\n+\t\t\tthrow new RuntimeException(\"Could not extract lambda method out of function: \" + e.getClass().getSimpleName() + \" - \" + e.getMessage(), e);\n \t\t}\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 102,
    "bugNodeStartChar": 3660,
    "bugNodeLength": 42,
    "fixLineNum": 102,
    "fixNodeStartChar": 3660,
    "fixNodeLength": 96,
    "sourceBeforeFix": "Class.forName(className.replace('/','.'))",
    "sourceAfterFix": "Class.forName(className.replace('/','.'),true,Thread.currentThread().getContextClassLoader())"
  },
  {
    "bugType": "MORE_SPECIFIC_IF",
    "fixCommitSHA1": "cb19c749d2f312320b819c9b201f2eb6f692dafc",
    "fixCommitParentSHA1": "aea6a6d428a4a202557a0fce0c3c73916c34a0dd",
    "bugFilePath": "flink-core/src/main/java/org/apache/flink/api/common/operators/util/ListKeyGroupedIterator.java",
    "fixPatch": "diff --git a/flink-core/src/main/java/org/apache/flink/api/common/operators/util/ListKeyGroupedIterator.java b/flink-core/src/main/java/org/apache/flink/api/common/operators/util/ListKeyGroupedIterator.java\nindex e530f8a..673440d 100644\n--- a/flink-core/src/main/java/org/apache/flink/api/common/operators/util/ListKeyGroupedIterator.java\n+++ b/flink-core/src/main/java/org/apache/flink/api/common/operators/util/ListKeyGroupedIterator.java\n@@ -89,7 +89,7 @@\n \t\t\t// Required if user code / reduce() method did not read the whole value iterator.\n \t\t\tE next;\n \t\t\twhile (true) {\n-\t\t\t\tif ((next = this.input.get(currentPosition++)) != null) {\n+\t\t\t\tif (currentPosition < input.size() && (next = this.input.get(currentPosition++)) != null) {\n \t\t\t\t\tif (!this.comparator.equalToReference(next)) {\n \t\t\t\t\t\t// the keys do not match, so we have a new group. store the current key\n \t\t\t\t\t\tthis.comparator.setReference(next);\n",
    "projectName": "apache.flink",
    "bugLineNum": 92,
    "bugNodeStartChar": 3060,
    "bugNodeLength": 50,
    "fixLineNum": 92,
    "fixNodeStartChar": 3060,
    "fixNodeLength": 84,
    "sourceBeforeFix": "(next=this.input.get(currentPosition++)) != null",
    "sourceAfterFix": "currentPosition < input.size() && (next=this.input.get(currentPosition++)) != null"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "cd699c562ec26d3909af08afb04de6080e4ad18f",
    "fixCommitParentSHA1": "73ebd3e0b1039d9509499f9953424e07286ea50d",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java\nindex ba08a9f..2da8b9f 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java\n@@ -387,7 +387,7 @@\n \t}\n \t\n \tpublic static TaskManager createTaskManager(JobManager jm) throws Exception {\n-\t\tInetAddress localhost = InetAddress.getLoopbackAddress();\n+\t\tInetAddress localhost = InetAddress.getLocalHost();\n \t\tInetSocketAddress jmMockAddress = new InetSocketAddress(localhost, 55443);\n \t\t\n \t\tConfiguration cfg = new Configuration();\n",
    "projectName": "apache.flink",
    "bugLineNum": 390,
    "bugNodeStartChar": 14227,
    "bugNodeLength": 32,
    "fixLineNum": 390,
    "fixNodeStartChar": 14227,
    "fixNodeLength": 26,
    "sourceBeforeFix": "InetAddress.getLoopbackAddress()",
    "sourceAfterFix": "InetAddress.getLocalHost()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "cd699c562ec26d3909af08afb04de6080e4ad18f",
    "fixCommitParentSHA1": "73ebd3e0b1039d9509499f9953424e07286ea50d",
    "bugFilePath": "flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java",
    "fixPatch": "diff --git a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java\nindex ba08a9f..2da8b9f 100644\n--- a/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java\n+++ b/flink-runtime/src/test/java/org/apache/flink/runtime/taskmanager/TaskManagerTest.java\n@@ -387,7 +387,7 @@\n \t}\n \t\n \tpublic static TaskManager createTaskManager(JobManager jm) throws Exception {\n-\t\tInetAddress localhost = InetAddress.getLoopbackAddress();\n+\t\tInetAddress localhost = InetAddress.getLocalHost();\n \t\tInetSocketAddress jmMockAddress = new InetSocketAddress(localhost, 55443);\n \t\t\n \t\tConfiguration cfg = new Configuration();\n",
    "projectName": "apache.flink",
    "bugLineNum": 390,
    "bugNodeStartChar": 14227,
    "bugNodeLength": 32,
    "fixLineNum": 390,
    "fixNodeStartChar": 14227,
    "fixNodeLength": 26,
    "sourceBeforeFix": "InetAddress.getLoopbackAddress()",
    "sourceAfterFix": "InetAddress.getLocalHost()"
  },
  {
    "bugType": "MORE_SPECIFIC_IF",
    "fixCommitSHA1": "ef7957f008f98491c3c525a224fec9107dca9179",
    "fixCommitParentSHA1": "8998a30ea036e6770c9a6b6442ffe01e64720c66",
    "bugFilePath": "flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/JobManager.java",
    "fixPatch": "diff --git a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/JobManager.java b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/JobManager.java\nindex 113f8fd..f79fecb 100644\n--- a/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/JobManager.java\n+++ b/flink-runtime/src/main/java/org/apache/flink/runtime/jobmanager/JobManager.java\n@@ -661,7 +661,7 @@\n \n \t@Override\n \tpublic InstanceID registerTaskManager(InstanceConnectionInfo instanceConnectionInfo, HardwareDescription hardwareDescription, int numberOfSlots) {\n-\t\tif (this.instanceManager != null) {\n+\t\tif (this.instanceManager != null && this.scheduler != null) {\n \t\t\treturn this.instanceManager.registerTaskManager(instanceConnectionInfo, hardwareDescription, numberOfSlots);\n \t\t} else {\n \t\t\treturn null;\n",
    "projectName": "apache.flink",
    "bugLineNum": 664,
    "bugNodeStartChar": 23642,
    "bugNodeLength": 28,
    "fixLineNum": 664,
    "fixNodeStartChar": 23642,
    "fixNodeLength": 54,
    "sourceBeforeFix": "this.instanceManager != null",
    "sourceAfterFix": "this.instanceManager != null && this.scheduler != null"
  },
  {
    "bugType": "SWAP_BOOLEAN_LITERAL",
    "fixCommitSHA1": "80af60b986f4aaf57ab56b243d3c897dabb0bddd",
    "fixCommitParentSHA1": "dac281f407a15020620ca74bc56303b3a4c3c850",
    "bugFilePath": "flink-compiler/src/main/java/org/apache/flink/compiler/plantranslate/NepheleJobGraphGenerator.java",
    "fixPatch": "diff --git a/flink-compiler/src/main/java/org/apache/flink/compiler/plantranslate/NepheleJobGraphGenerator.java b/flink-compiler/src/main/java/org/apache/flink/compiler/plantranslate/NepheleJobGraphGenerator.java\nindex 1eabb83..fcf9f8d 100644\n--- a/flink-compiler/src/main/java/org/apache/flink/compiler/plantranslate/NepheleJobGraphGenerator.java\n+++ b/flink-compiler/src/main/java/org/apache/flink/compiler/plantranslate/NepheleJobGraphGenerator.java\n@@ -101,7 +101,7 @@\n \t\n \tpublic static final String MERGE_ITERATION_AUX_TASKS_KEY = \"compiler.merge-iteration-aux\";\n \t\n-\tprivate static final boolean mergeIterationAuxTasks = GlobalConfiguration.getBoolean(MERGE_ITERATION_AUX_TASKS_KEY, true);\n+\tprivate static final boolean mergeIterationAuxTasks = GlobalConfiguration.getBoolean(MERGE_ITERATION_AUX_TASKS_KEY, false);\n \t\n //\tprivate static final Log LOG = LogFactory.getLog(NepheleJobGraphGenerator.class);\n \t\n",
    "projectName": "apache.flink",
    "bugLineNum": 104,
    "bugNodeStartChar": 5422,
    "bugNodeLength": 67,
    "fixLineNum": 104,
    "fixNodeStartChar": 5422,
    "fixNodeLength": 68,
    "sourceBeforeFix": "GlobalConfiguration.getBoolean(MERGE_ITERATION_AUX_TASKS_KEY,true)",
    "sourceAfterFix": "GlobalConfiguration.getBoolean(MERGE_ITERATION_AUX_TASKS_KEY,false)"
  },
  {
    "bugType": "MORE_SPECIFIC_IF",
    "fixCommitSHA1": "00840599a7a498cbd19d524ab5ad698365cbab4f",
    "fixCommitParentSHA1": "f96a7e0e57c382d2154fa7b004d18e6a6e2eaa89",
    "bugFilePath": "flink-java/src/main/java/org/apache/flink/api/java/operators/JoinOperator.java",
    "fixPatch": "diff --git a/flink-java/src/main/java/org/apache/flink/api/java/operators/JoinOperator.java b/flink-java/src/main/java/org/apache/flink/api/java/operators/JoinOperator.java\nindex 6ffbd1b..2efe7e9 100644\n--- a/flink-java/src/main/java/org/apache/flink/api/java/operators/JoinOperator.java\n+++ b/flink-java/src/main/java/org/apache/flink/api/java/operators/JoinOperator.java\n@@ -925,13 +925,13 @@\n \t\tpublic void join(T1 in1, T2 in2, Collector<R> out) {\n \t\t\tfor(int i=0; i<fields.length; i++) {\n \t\t\t\tif(isFromFirst[i]) {\n-\t\t\t\t\tif(fields[i] >= 0) {\n+\t\t\t\t\tif(fields[i] >= 0 && in1 != null) {\n \t\t\t\t\t\toutTuple.setField(((Tuple)in1).getField(fields[i]), i);\n \t\t\t\t\t} else {\n \t\t\t\t\t\toutTuple.setField(in1, i);\n \t\t\t\t\t}\n \t\t\t\t} else {\n-\t\t\t\t\tif(fields[i] >= 0) {\n+\t\t\t\t\tif(fields[i] >= 0 && in2 != null) {\n \t\t\t\t\t\toutTuple.setField(((Tuple)in2).getField(fields[i]), i);\n \t\t\t\t\t} else {\n \t\t\t\t\t\toutTuple.setField(in2, i);\n",
    "projectName": "apache.flink",
    "bugLineNum": 928,
    "bugNodeStartChar": 41211,
    "bugNodeLength": 14,
    "fixLineNum": 928,
    "fixNodeStartChar": 41211,
    "fixNodeLength": 29,
    "sourceBeforeFix": "fields[i] >= 0",
    "sourceAfterFix": "fields[i] >= 0 && in1 != null"
  },
  {
    "bugType": "MORE_SPECIFIC_IF",
    "fixCommitSHA1": "00840599a7a498cbd19d524ab5ad698365cbab4f",
    "fixCommitParentSHA1": "f96a7e0e57c382d2154fa7b004d18e6a6e2eaa89",
    "bugFilePath": "flink-java/src/main/java/org/apache/flink/api/java/operators/JoinOperator.java",
    "fixPatch": "diff --git a/flink-java/src/main/java/org/apache/flink/api/java/operators/JoinOperator.java b/flink-java/src/main/java/org/apache/flink/api/java/operators/JoinOperator.java\nindex 6ffbd1b..2efe7e9 100644\n--- a/flink-java/src/main/java/org/apache/flink/api/java/operators/JoinOperator.java\n+++ b/flink-java/src/main/java/org/apache/flink/api/java/operators/JoinOperator.java\n@@ -925,13 +925,13 @@\n \t\tpublic void join(T1 in1, T2 in2, Collector<R> out) {\n \t\t\tfor(int i=0; i<fields.length; i++) {\n \t\t\t\tif(isFromFirst[i]) {\n-\t\t\t\t\tif(fields[i] >= 0) {\n+\t\t\t\t\tif(fields[i] >= 0 && in1 != null) {\n \t\t\t\t\t\toutTuple.setField(((Tuple)in1).getField(fields[i]), i);\n \t\t\t\t\t} else {\n \t\t\t\t\t\toutTuple.setField(in1, i);\n \t\t\t\t\t}\n \t\t\t\t} else {\n-\t\t\t\t\tif(fields[i] >= 0) {\n+\t\t\t\t\tif(fields[i] >= 0 && in2 != null) {\n \t\t\t\t\t\toutTuple.setField(((Tuple)in2).getField(fields[i]), i);\n \t\t\t\t\t} else {\n \t\t\t\t\t\toutTuple.setField(in2, i);\n",
    "projectName": "apache.flink",
    "bugLineNum": 934,
    "bugNodeStartChar": 41366,
    "bugNodeLength": 14,
    "fixLineNum": 934,
    "fixNodeStartChar": 41366,
    "fixNodeLength": 29,
    "sourceBeforeFix": "fields[i] >= 0",
    "sourceAfterFix": "fields[i] >= 0 && in2 != null"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 356,
    "bugNodeStartChar": 13646,
    "bugNodeLength": 15,
    "fixLineNum": 356,
    "fixNodeStartChar": 13646,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 359,
    "bugNodeStartChar": 13790,
    "bugNodeLength": 15,
    "fixLineNum": 359,
    "fixNodeStartChar": 13790,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 376,
    "bugNodeStartChar": 14284,
    "bugNodeLength": 15,
    "fixLineNum": 376,
    "fixNodeStartChar": 14284,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 393,
    "bugNodeStartChar": 14855,
    "bugNodeLength": 15,
    "fixLineNum": 393,
    "fixNodeStartChar": 14855,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 411,
    "bugNodeStartChar": 15512,
    "bugNodeLength": 15,
    "fixLineNum": 411,
    "fixNodeStartChar": 15512,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 433,
    "bugNodeStartChar": 16377,
    "bugNodeLength": 15,
    "fixLineNum": 433,
    "fixNodeStartChar": 16377,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 456,
    "bugNodeStartChar": 17321,
    "bugNodeLength": 15,
    "fixLineNum": 456,
    "fixNodeStartChar": 17321,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 478,
    "bugNodeStartChar": 18211,
    "bugNodeLength": 15,
    "fixLineNum": 478,
    "fixNodeStartChar": 18211,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 480,
    "bugNodeStartChar": 18346,
    "bugNodeLength": 15,
    "fixLineNum": 480,
    "fixNodeStartChar": 18346,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 506,
    "bugNodeStartChar": 19437,
    "bugNodeLength": 15,
    "fixLineNum": 506,
    "fixNodeStartChar": 19437,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 508,
    "bugNodeStartChar": 19574,
    "bugNodeLength": 15,
    "fixLineNum": 508,
    "fixNodeStartChar": 19574,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 525,
    "bugNodeStartChar": 20190,
    "bugNodeLength": 15,
    "fixLineNum": 525,
    "fixNodeStartChar": 20190,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 542,
    "bugNodeStartChar": 20758,
    "bugNodeLength": 15,
    "fixLineNum": 542,
    "fixNodeStartChar": 20758,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 560,
    "bugNodeStartChar": 21412,
    "bugNodeLength": 15,
    "fixLineNum": 560,
    "fixNodeStartChar": 21412,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 582,
    "bugNodeStartChar": 22274,
    "bugNodeLength": 15,
    "fixLineNum": 582,
    "fixNodeStartChar": 22274,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 605,
    "bugNodeStartChar": 23215,
    "bugNodeLength": 15,
    "fixLineNum": 605,
    "fixNodeStartChar": 23215,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 630,
    "bugNodeStartChar": 24224,
    "bugNodeLength": 15,
    "fixLineNum": 630,
    "fixNodeStartChar": 24224,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 632,
    "bugNodeStartChar": 24357,
    "bugNodeLength": 15,
    "fixLineNum": 632,
    "fixNodeStartChar": 24357,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 658,
    "bugNodeStartChar": 25441,
    "bugNodeLength": 15,
    "fixLineNum": 658,
    "fixNodeStartChar": 25441,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 660,
    "bugNodeStartChar": 25576,
    "bugNodeLength": 15,
    "fixLineNum": 660,
    "fixNodeStartChar": 25576,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 795,
    "bugNodeStartChar": 30563,
    "bugNodeLength": 15,
    "fixLineNum": 795,
    "fixNodeStartChar": 30563,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 799,
    "bugNodeStartChar": 30679,
    "bugNodeLength": 15,
    "fixLineNum": 799,
    "fixNodeStartChar": 30679,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 804,
    "bugNodeStartChar": 30915,
    "bugNodeLength": 15,
    "fixLineNum": 804,
    "fixNodeStartChar": 30915,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "74d3742b6e39c174111d3b0a6177c25b93f9c926",
    "fixCommitParentSHA1": "1b31f4d19df1fabc7aaee5837f4d2c3439819495",
    "bugFilePath": "flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java",
    "fixPatch": "diff --git a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\nindex 7cab2df..86b3322 100644\n--- a/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n+++ b/flink-addons/flink-streaming/flink-streaming-core/src/main/java/org/apache/flink/streaming/api/datastream/DataStream.java\n@@ -353,10 +353,10 @@\n \t * \n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> print() {\n+\tpublic DataStreamSink<OUT> print() {\n \t\tDataStream<OUT> inputStream = this.copy();\n \t\tPrintSinkFunction<OUT> printFunction = new PrintSinkFunction<OUT>();\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, printFunction, null);\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, printFunction, null);\n \n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \n@@ -373,7 +373,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), 1, null);\n \t}\n \n@@ -390,7 +390,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, null);\n \t}\n \n@@ -408,7 +408,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, null);\n \t}\n \n@@ -430,7 +430,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, long millis, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), millis, endTuple);\n \t}\n \n@@ -453,7 +453,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsText(String path, int batchSize, OUT endTuple) {\n \t\treturn writeAsText(this, path, new WriteFormatAsText<OUT>(), batchSize, endTuple);\n \t}\n \n@@ -475,9 +475,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -503,9 +503,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsText(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsText<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -522,7 +522,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), 1, null);\n \t}\n \n@@ -539,7 +539,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, null);\n \t}\n \n@@ -557,7 +557,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), batchSize, null);\n \t}\n \n@@ -579,7 +579,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, long millis, OUT endTuple) {\n \t\treturn writeAsCsv(this, path, new WriteFormatAsCsv<OUT>(), millis, endTuple);\n \t}\n \n@@ -602,7 +602,7 @@\n \t * \n \t * @return The closed DataStream\n \t */\n-\tpublic DataStream<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n+\tpublic DataStreamSink<OUT> writeAsCsv(String path, int batchSize, OUT endTuple) {\n \t\tif (this instanceof SingleOutputStreamOperator) {\n \t\t\t((SingleOutputStreamOperator<?, ?>) this).setMutability(false);\n \t\t}\n@@ -627,9 +627,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, long millis, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByMillis<OUT>(\n \t\t\t\tpath, format, millis, endTuple));\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -655,9 +655,9 @@\n \t * \n \t * @return the data stream constructed\n \t */\n-\tprivate DataStream<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n+\tprivate DataStreamSink<OUT> writeAsCsv(DataStream<OUT> inputStream, String path,\n \t\t\tWriteFormatAsCsv<OUT> format, int batchSize, OUT endTuple) {\n-\t\tDataStream<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n+\t\tDataStreamSink<OUT> returnStream = addSink(inputStream, new WriteSinkFunctionByBatches<OUT>(\n \t\t\t\tpath, format, batchSize, endTuple), null);\n \t\tjobGraphBuilder.setBytesFrom(inputStream.getId(), returnStream.getId());\n \t\tjobGraphBuilder.setMutability(returnStream.getId(), false);\n@@ -792,18 +792,18 @@\n \t *            The object containing the sink's invoke function.\n \t * @return The closed DataStream.\n \t */\n-\tpublic DataStream<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n+\tpublic DataStreamSink<OUT> addSink(SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(this.copy(), sinkFunction);\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction) {\n \t\treturn addSink(inputStream, sinkFunction, new FunctionTypeWrapper<OUT, Tuple, OUT>(\n \t\t\t\tsinkFunction, SinkFunction.class, 0, -1, 0));\n \t}\n \n-\tprivate DataStream<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n+\tprivate DataStreamSink<OUT> addSink(DataStream<OUT> inputStream, SinkFunction<OUT> sinkFunction,\n \t\t\tTypeSerializerWrapper<OUT, Tuple, OUT> typeWrapper) {\n-\t\tDataStream<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n+\t\tDataStreamSink<OUT> returnStream = new DataStreamSink<OUT>(environment, \"sink\");\n \n \t\ttry {\n \t\t\tjobGraphBuilder.addSink(returnStream.getId(), new SinkInvokable<OUT>(sinkFunction),\n",
    "projectName": "apache.flink",
    "bugLineNum": 806,
    "bugNodeStartChar": 31059,
    "bugNodeLength": 15,
    "fixLineNum": 806,
    "fixNodeStartChar": 31059,
    "fixNodeLength": 19,
    "sourceBeforeFix": "DataStream<OUT>",
    "sourceAfterFix": "DataStreamSink<OUT>"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "eb51c8806e1e415162b7999c4d9b39e952fccc61",
    "fixCommitParentSHA1": "9d8f4920e4159c96e17bde82d517fa6d12449b74",
    "bugFilePath": "flink-java/src/main/java/org/apache/flink/api/java/typeutils/runtime/RuntimeStatefulSerializerFactory.java",
    "fixPatch": "diff --git a/flink-java/src/main/java/org/apache/flink/api/java/typeutils/runtime/RuntimeStatefulSerializerFactory.java b/flink-java/src/main/java/org/apache/flink/api/java/typeutils/runtime/RuntimeStatefulSerializerFactory.java\nindex 7288d73..f65ba4b 100644\n--- a/flink-java/src/main/java/org/apache/flink/api/java/typeutils/runtime/RuntimeStatefulSerializerFactory.java\n+++ b/flink-java/src/main/java/org/apache/flink/api/java/typeutils/runtime/RuntimeStatefulSerializerFactory.java\n@@ -38,7 +38,7 @@\n \t\n \tprivate TypeSerializer<T> serializer;\t\t// only for equality comparisons\n \t\n-\tprivate ClassLoader loader;\n+\tprivate transient ClassLoader loader;\n \n \tprivate Class<T> clazz;\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 41,
    "bugNodeStartChar": 1522,
    "bugNodeLength": 27,
    "fixLineNum": 41,
    "fixNodeStartChar": 1522,
    "fixNodeLength": 37,
    "sourceBeforeFix": "2",
    "sourceAfterFix": "130"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "8a8060eaa3a12703b594d4f4c581fa14bd846b19",
    "fixCommitParentSHA1": "3c8bdee86132e48f99825706b97e24b4e119dc1e",
    "bugFilePath": "stratosphere-java/src/main/java/eu/stratosphere/api/java/IterativeDataSet.java",
    "fixPatch": "diff --git a/stratosphere-java/src/main/java/eu/stratosphere/api/java/IterativeDataSet.java b/stratosphere-java/src/main/java/eu/stratosphere/api/java/IterativeDataSet.java\nindex bab990d..be3fdf7 100644\n--- a/stratosphere-java/src/main/java/eu/stratosphere/api/java/IterativeDataSet.java\n+++ b/stratosphere-java/src/main/java/eu/stratosphere/api/java/IterativeDataSet.java\n@@ -137,6 +137,6 @@\n \t@Override\n \tprotected eu.stratosphere.api.common.operators.SingleInputOperator<T, T, ?> translateToDataFlow(Operator<T> input) {\n \t\t// All the translation magic happens when the iteration end is encountered.\n-\t\tthrow new UnsupportedOperationException(\"This should never happen.\");\n+\t\tthrow new RuntimeException(\"Error while creating the data flow plan for an iteration: The iteration end was not specified correctly.\");\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 140,
    "bugNodeStartChar": 6629,
    "bugNodeLength": 62,
    "fixLineNum": 140,
    "fixNodeStartChar": 6629,
    "fixNodeLength": 128,
    "sourceBeforeFix": "new UnsupportedOperationException(\"This should never happen.\")",
    "sourceAfterFix": "new RuntimeException(\"Error while creating the data flow plan for an iteration: The iteration end was not specified correctly.\")"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "cbed2a13b8417c5f78f4f0ac1cea2f9b6e4f5526",
    "fixCommitParentSHA1": "1c142484a3716712920867dd6faf8cd933696408",
    "bugFilePath": "stratosphere-java/src/main/java/eu/stratosphere/api/java/tuple/Tuple.java",
    "fixPatch": "diff --git a/stratosphere-java/src/main/java/eu/stratosphere/api/java/tuple/Tuple.java b/stratosphere-java/src/main/java/eu/stratosphere/api/java/tuple/Tuple.java\nindex fc8a07d..e0ecb40 100644\n--- a/stratosphere-java/src/main/java/eu/stratosphere/api/java/tuple/Tuple.java\n+++ b/stratosphere-java/src/main/java/eu/stratosphere/api/java/tuple/Tuple.java\n@@ -15,8 +15,8 @@\n /**\n  * The base class of all tuples. Tuples have a fix length and contain a set of fields,\n  * which may all be of different types. Because Tuples are strongly typed, each distinct\n- * tuple length is represented by its own class. Tuples exists with up to 22 fields and\n- * are described in the classes {@link Tuple1} to {@link Tuple22}.\n+ * tuple length is represented by its own class. Tuples exists with up to 25 fields and\n+ * are described in the classes {@link Tuple1} to {@link Tuple25}.\n  * <p>\n  * The fields in the tuples may be accessed directly a public fields, or via position (zero indexed)\n  * {@link #getField(int)}.\n@@ -28,7 +28,7 @@\n \t\n \tprivate static final long serialVersionUID = 1L;\n \t\n-\tpublic static final int MAX_ARITY = 22;\n+\tpublic static final int MAX_ARITY = 25;\n \t\n \t\n \t/**\n",
    "projectName": "apache.flink",
    "bugLineNum": 31,
    "bugNodeStartChar": 1668,
    "bugNodeLength": 14,
    "fixLineNum": 31,
    "fixNodeStartChar": 1668,
    "fixNodeLength": 14,
    "sourceBeforeFix": "MAX_ARITY=22",
    "sourceAfterFix": "MAX_ARITY=25"
  },
  {
    "bugType": "CHANGE_OPERATOR",
    "fixCommitSHA1": "cbed2a13b8417c5f78f4f0ac1cea2f9b6e4f5526",
    "fixCommitParentSHA1": "1c142484a3716712920867dd6faf8cd933696408",
    "bugFilePath": "stratosphere-java/src/main/java/eu/stratosphere/api/java/typeutils/TupleTypeInfo.java",
    "fixPatch": "diff --git a/stratosphere-java/src/main/java/eu/stratosphere/api/java/typeutils/TupleTypeInfo.java b/stratosphere-java/src/main/java/eu/stratosphere/api/java/typeutils/TupleTypeInfo.java\nindex 6630386..ea26376 100644\n--- a/stratosphere-java/src/main/java/eu/stratosphere/api/java/typeutils/TupleTypeInfo.java\n+++ b/stratosphere-java/src/main/java/eu/stratosphere/api/java/typeutils/TupleTypeInfo.java\n@@ -32,7 +32,7 @@\n \tprivate final Class<T> tupleType;\n \t\n \tpublic TupleTypeInfo(Class<T> tupleType, TypeInformation<?>... types) {\n-\t\tif (types == null || types.length == 0 || types.length >= Tuple.MAX_ARITY) {\n+\t\tif (types == null || types.length == 0 || types.length > Tuple.MAX_ARITY) {\n \t\t\tthrow new IllegalArgumentException();\n \t\t}\n \t\t\n",
    "projectName": "apache.flink",
    "bugLineNum": 35,
    "bugNodeStartChar": 1718,
    "bugNodeLength": 31,
    "fixLineNum": 35,
    "fixNodeStartChar": 1718,
    "fixNodeLength": 30,
    "sourceBeforeFix": "types.length >= Tuple.MAX_ARITY",
    "sourceAfterFix": "types.length > Tuple.MAX_ARITY"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "385b42d9fd284c23c48987dae294449740a70853",
    "fixCommitParentSHA1": "49d97ff424b4bde551d2d236dccf72c94bd91d4b",
    "bugFilePath": "stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java",
    "fixPatch": "diff --git a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java\nindex 51a14d7..406d7be 100644\n--- a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java\n+++ b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java\n@@ -949,10 +949,10 @@\n \t\t\t\t\tresponder.doRespond(call);\n \t\t\t\t} catch (InterruptedException e) {\n \t\t\t\t\tif (running) { // unexpected -- log it\n-\t\t\t\t\t\tLOG.info(getName() + \" caught: \", e);\n+\t\t\t\t\t\tLOG.error(getName() + \" caught: \", e);\n \t\t\t\t\t}\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tLOG.info(getName() + \" caught: \", e);\n+\t\t\t\t\tLOG.error(getName() + \" caught: \", e);\n \t\t\t\t}\n \t\t\t}\n \t\t\tLOG.debug(getName() + \": exiting\");\n",
    "projectName": "apache.flink",
    "bugLineNum": 952,
    "bugNodeStartChar": 27307,
    "bugNodeLength": 36,
    "fixLineNum": 952,
    "fixNodeStartChar": 27307,
    "fixNodeLength": 37,
    "sourceBeforeFix": "LOG.info(getName() + \" caught: \",e)",
    "sourceAfterFix": "LOG.error(getName() + \" caught: \",e)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "385b42d9fd284c23c48987dae294449740a70853",
    "fixCommitParentSHA1": "49d97ff424b4bde551d2d236dccf72c94bd91d4b",
    "bugFilePath": "stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java",
    "fixPatch": "diff --git a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java\nindex 51a14d7..406d7be 100644\n--- a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java\n+++ b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java\n@@ -949,10 +949,10 @@\n \t\t\t\t\tresponder.doRespond(call);\n \t\t\t\t} catch (InterruptedException e) {\n \t\t\t\t\tif (running) { // unexpected -- log it\n-\t\t\t\t\t\tLOG.info(getName() + \" caught: \", e);\n+\t\t\t\t\t\tLOG.error(getName() + \" caught: \", e);\n \t\t\t\t\t}\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tLOG.info(getName() + \" caught: \", e);\n+\t\t\t\t\tLOG.error(getName() + \" caught: \", e);\n \t\t\t\t}\n \t\t\t}\n \t\t\tLOG.debug(getName() + \": exiting\");\n",
    "projectName": "apache.flink",
    "bugLineNum": 952,
    "bugNodeStartChar": 27307,
    "bugNodeLength": 36,
    "fixLineNum": 952,
    "fixNodeStartChar": 27307,
    "fixNodeLength": 37,
    "sourceBeforeFix": "LOG.info(getName() + \" caught: \",e)",
    "sourceAfterFix": "LOG.error(getName() + \" caught: \",e)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "385b42d9fd284c23c48987dae294449740a70853",
    "fixCommitParentSHA1": "49d97ff424b4bde551d2d236dccf72c94bd91d4b",
    "bugFilePath": "stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java",
    "fixPatch": "diff --git a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java\nindex 51a14d7..406d7be 100644\n--- a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java\n+++ b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java\n@@ -949,10 +949,10 @@\n \t\t\t\t\tresponder.doRespond(call);\n \t\t\t\t} catch (InterruptedException e) {\n \t\t\t\t\tif (running) { // unexpected -- log it\n-\t\t\t\t\t\tLOG.info(getName() + \" caught: \", e);\n+\t\t\t\t\t\tLOG.error(getName() + \" caught: \", e);\n \t\t\t\t\t}\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tLOG.info(getName() + \" caught: \", e);\n+\t\t\t\t\tLOG.error(getName() + \" caught: \", e);\n \t\t\t\t}\n \t\t\t}\n \t\t\tLOG.debug(getName() + \": exiting\");\n",
    "projectName": "apache.flink",
    "bugLineNum": 955,
    "bugNodeStartChar": 27385,
    "bugNodeLength": 36,
    "fixLineNum": 955,
    "fixNodeStartChar": 27385,
    "fixNodeLength": 37,
    "sourceBeforeFix": "LOG.info(getName() + \" caught: \",e)",
    "sourceAfterFix": "LOG.error(getName() + \" caught: \",e)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "385b42d9fd284c23c48987dae294449740a70853",
    "fixCommitParentSHA1": "49d97ff424b4bde551d2d236dccf72c94bd91d4b",
    "bugFilePath": "stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java",
    "fixPatch": "diff --git a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java\nindex 51a14d7..406d7be 100644\n--- a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java\n+++ b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/ipc/Server.java\n@@ -949,10 +949,10 @@\n \t\t\t\t\tresponder.doRespond(call);\n \t\t\t\t} catch (InterruptedException e) {\n \t\t\t\t\tif (running) { // unexpected -- log it\n-\t\t\t\t\t\tLOG.info(getName() + \" caught: \", e);\n+\t\t\t\t\t\tLOG.error(getName() + \" caught: \", e);\n \t\t\t\t\t}\n \t\t\t\t} catch (Exception e) {\n-\t\t\t\t\tLOG.info(getName() + \" caught: \", e);\n+\t\t\t\t\tLOG.error(getName() + \" caught: \", e);\n \t\t\t\t}\n \t\t\t}\n \t\t\tLOG.debug(getName() + \": exiting\");\n",
    "projectName": "apache.flink",
    "bugLineNum": 955,
    "bugNodeStartChar": 27385,
    "bugNodeLength": 36,
    "fixLineNum": 955,
    "fixNodeStartChar": 27385,
    "fixNodeLength": 37,
    "sourceBeforeFix": "LOG.info(getName() + \" caught: \",e)",
    "sourceAfterFix": "LOG.error(getName() + \" caught: \",e)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "f1024ba0e7d5f1276a285a4218f26c70252719b5",
    "fixCommitParentSHA1": "1aab122b8a467cc53c52df91dd28514ff304076b",
    "bugFilePath": "stratosphere-runtime/src/main/java/eu/stratosphere/nephele/jobmanager/JobManager.java",
    "fixPatch": "diff --git a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/jobmanager/JobManager.java b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/jobmanager/JobManager.java\nindex 207fc6c..81adea0 100644\n--- a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/jobmanager/JobManager.java\n+++ b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/jobmanager/JobManager.java\n@@ -503,7 +503,7 @@\n \n \t\t// Try to create initial execution graph from job graph\n \t\tLOG.info(\"Creating initial execution graph from job graph \" + job.getName());\n-\t\tExecutionGraph eg = null;\n+\t\tExecutionGraph eg;\n \n \t\ttry {\n \t\t\teg = new ExecutionGraph(job, this.instanceManager);\n@@ -589,7 +589,7 @@\n \t\t\tLibraryCacheManager.unregister(executionGraph.getJobID());\n \t\t} catch (IOException ioe) {\n \t\t\tif (LOG.isWarnEnabled()) {\n-\t\t\t\tLOG.warn(StringUtils.stringifyException(ioe));\n+\t\t\t\tLOG.warn(ioe);\n \t\t\t}\n \t\t}\n \t}\n@@ -959,7 +959,7 @@\n \t\t\t\ttry {\n \t\t\t\t\tinstance.killTaskManager();\n \t\t\t\t} catch (IOException ioe) {\n-\t\t\t\t\tLOG.error(StringUtils.stringifyException(ioe));\n+\t\t\t\t\tLOG.error(ioe);\n \t\t\t\t}\n \t\t\t}\n \t\t};\n@@ -1052,7 +1052,7 @@\n \t\t\t\t\t\tit2.next().logBufferUtilization();\n \t\t\t\t\t}\n \t\t\t\t} catch (IOException ioe) {\n-\t\t\t\t\tLOG.error(StringUtils.stringifyException(ioe));\n+\t\t\t\t\tLOG.error(ioe);\n \t\t\t\t}\n \n \t\t\t}\n@@ -1187,9 +1187,9 @@\n \t\t\tserver = new WebInfoServer(config, port, this);\n \t\t\tserver.start();\n \t\t} catch (FileNotFoundException e) {\n-\t\t\tLOG.error(e.getMessage());\n+\t\t\tLOG.error(e.getMessage(), e);\n \t\t} catch (Exception e) {\n-\t\t\tLOG.error(\"Cannot instantiate info server: \" + StringUtils.stringifyException(e));\n+\t\t\tLOG.error(\"Cannot instantiate info server: \" + e.getMessage(), e);\n \t\t}\n \t}\n \t\n",
    "projectName": "apache.flink",
    "bugLineNum": 1190,
    "bugNodeStartChar": 40565,
    "bugNodeLength": 25,
    "fixLineNum": 1190,
    "fixNodeStartChar": 40565,
    "fixNodeLength": 28,
    "sourceBeforeFix": "LOG.error(e.getMessage())",
    "sourceAfterFix": "LOG.error(e.getMessage(),e)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "f1024ba0e7d5f1276a285a4218f26c70252719b5",
    "fixCommitParentSHA1": "1aab122b8a467cc53c52df91dd28514ff304076b",
    "bugFilePath": "stratosphere-runtime/src/main/java/eu/stratosphere/nephele/taskmanager/TaskManager.java",
    "fixPatch": "diff --git a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/taskmanager/TaskManager.java b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/taskmanager/TaskManager.java\nindex 4313755..a31a832 100644\n--- a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/taskmanager/TaskManager.java\n+++ b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/taskmanager/TaskManager.java\n@@ -615,7 +615,7 @@\n \t\t\t\tfinal TaskSubmissionResult result = new TaskSubmissionResult(vertexID,\n \t\t\t\t\tAbstractTaskResult.ReturnCode.DEPLOYMENT_ERROR);\n \t\t\t\tresult.setDescription(StringUtils.stringifyException(t));\n-\t\t\t\tLOG.error(result.getDescription());\n+\t\t\t\tLOG.error(result.getDescription(), t);\n \t\t\t\tsubmissionResultList.add(result);\n \t\t\t\tcontinue;\n \t\t\t}\n@@ -632,7 +632,7 @@\n \t\t\t\tfinal TaskSubmissionResult result = new TaskSubmissionResult(vertexID,\n \t\t\t\t\tAbstractTaskResult.ReturnCode.INSUFFICIENT_RESOURCES);\n \t\t\t\tresult.setDescription(e.getMessage());\n-\t\t\t\tLOG.error(result.getDescription());\n+\t\t\t\tLOG.error(result.getDescription(), e);\n \t\t\t\tsubmissionResultList.add(result);\n \t\t\t\tcontinue;\n \t\t\t}\n@@ -667,8 +667,6 @@\n \t *        the job configuration that has been attached to the original job graph\n \t * @param environment\n \t *        the environment of the task to be registered\n-\t * @param initialCheckpointState\n-\t *        the task's initial checkpoint state\n \t * @param activeOutputChannels\n \t *        the set of initially active output channels\n \t * @return the task to be started or <code>null</code> if a task with the same ID was already running\n@@ -686,7 +684,7 @@\n \t\t}\n \n \t\t// Task creation and registration must be atomic\n-\t\tTask task = null;\n+\t\tTask task;\n \n \t\tsynchronized (this) {\n \t\t\tfinal Task runningTask = this.runningTasks.get(id);\n@@ -810,7 +808,7 @@\n \t\t\t\tthis.jobManager.updateTaskExecutionState(new TaskExecutionState(jobID, id, newExecutionState,\n \t\t\t\t\toptionalDescription));\n \t\t\t} catch (IOException e) {\n-\t\t\t\tLOG.error(StringUtils.stringifyException(e));\n+\t\t\t\tLOG.error(e);\n \t\t\t}\n \t\t}\n \t}\n@@ -867,7 +865,7 @@\n \t\t\t\tthis.executorService.awaitTermination(5000L, TimeUnit.MILLISECONDS);\n \t\t\t} catch (InterruptedException e) {\n \t\t\t\tif (LOG.isDebugEnabled()) {\n-\t\t\t\t\tLOG.debug(StringUtils.stringifyException(e));\n+\t\t\t\t\tLOG.debug(e);\n \t\t\t\t}\n \t\t\t}\n \t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 618,
    "bugNodeStartChar": 23161,
    "bugNodeLength": 34,
    "fixLineNum": 618,
    "fixNodeStartChar": 23161,
    "fixNodeLength": 37,
    "sourceBeforeFix": "LOG.error(result.getDescription())",
    "sourceAfterFix": "LOG.error(result.getDescription(),t)"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "f1024ba0e7d5f1276a285a4218f26c70252719b5",
    "fixCommitParentSHA1": "1aab122b8a467cc53c52df91dd28514ff304076b",
    "bugFilePath": "stratosphere-runtime/src/main/java/eu/stratosphere/nephele/taskmanager/TaskManager.java",
    "fixPatch": "diff --git a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/taskmanager/TaskManager.java b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/taskmanager/TaskManager.java\nindex 4313755..a31a832 100644\n--- a/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/taskmanager/TaskManager.java\n+++ b/stratosphere-runtime/src/main/java/eu/stratosphere/nephele/taskmanager/TaskManager.java\n@@ -615,7 +615,7 @@\n \t\t\t\tfinal TaskSubmissionResult result = new TaskSubmissionResult(vertexID,\n \t\t\t\t\tAbstractTaskResult.ReturnCode.DEPLOYMENT_ERROR);\n \t\t\t\tresult.setDescription(StringUtils.stringifyException(t));\n-\t\t\t\tLOG.error(result.getDescription());\n+\t\t\t\tLOG.error(result.getDescription(), t);\n \t\t\t\tsubmissionResultList.add(result);\n \t\t\t\tcontinue;\n \t\t\t}\n@@ -632,7 +632,7 @@\n \t\t\t\tfinal TaskSubmissionResult result = new TaskSubmissionResult(vertexID,\n \t\t\t\t\tAbstractTaskResult.ReturnCode.INSUFFICIENT_RESOURCES);\n \t\t\t\tresult.setDescription(e.getMessage());\n-\t\t\t\tLOG.error(result.getDescription());\n+\t\t\t\tLOG.error(result.getDescription(), e);\n \t\t\t\tsubmissionResultList.add(result);\n \t\t\t\tcontinue;\n \t\t\t}\n@@ -667,8 +667,6 @@\n \t *        the job configuration that has been attached to the original job graph\n \t * @param environment\n \t *        the environment of the task to be registered\n-\t * @param initialCheckpointState\n-\t *        the task's initial checkpoint state\n \t * @param activeOutputChannels\n \t *        the set of initially active output channels\n \t * @return the task to be started or <code>null</code> if a task with the same ID was already running\n@@ -686,7 +684,7 @@\n \t\t}\n \n \t\t// Task creation and registration must be atomic\n-\t\tTask task = null;\n+\t\tTask task;\n \n \t\tsynchronized (this) {\n \t\t\tfinal Task runningTask = this.runningTasks.get(id);\n@@ -810,7 +808,7 @@\n \t\t\t\tthis.jobManager.updateTaskExecutionState(new TaskExecutionState(jobID, id, newExecutionState,\n \t\t\t\t\toptionalDescription));\n \t\t\t} catch (IOException e) {\n-\t\t\t\tLOG.error(StringUtils.stringifyException(e));\n+\t\t\t\tLOG.error(e);\n \t\t\t}\n \t\t}\n \t}\n@@ -867,7 +865,7 @@\n \t\t\t\tthis.executorService.awaitTermination(5000L, TimeUnit.MILLISECONDS);\n \t\t\t} catch (InterruptedException e) {\n \t\t\t\tif (LOG.isDebugEnabled()) {\n-\t\t\t\t\tLOG.debug(StringUtils.stringifyException(e));\n+\t\t\t\t\tLOG.debug(e);\n \t\t\t\t}\n \t\t\t}\n \t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 635,
    "bugNodeStartChar": 23764,
    "bugNodeLength": 34,
    "fixLineNum": 635,
    "fixNodeStartChar": 23764,
    "fixNodeLength": 37,
    "sourceBeforeFix": "LOG.error(result.getDescription())",
    "sourceAfterFix": "LOG.error(result.getDescription(),e)"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "ebe03bfed0dbdf644542846734931457fcd964b5",
    "fixCommitParentSHA1": "7345107a4c7486f9dced49e7478217ad61f7aea5",
    "bugFilePath": "stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/MessagingFunction.java",
    "fixPatch": "diff --git a/stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/MessagingFunction.java b/stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/MessagingFunction.java\nindex c2edd6e..9ef07c4 100644\n--- a/stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/MessagingFunction.java\n+++ b/stratosphere-addons/spargel/src/main/java/eu/stratosphere/spargel/java/MessagingFunction.java\n@@ -105,7 +105,7 @@\n \t\t\n \t\twhile (edges.hasNext()) {\n \t\t\tTuple next = (Tuple) edges.next();\n-\t\t\tVertexKey k = next.getField(0);\n+\t\t\tVertexKey k = next.getField(1);\n \t\t\toutValue.f0 = k;\n \t\t\tout.collect(outValue);\n \t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 108,
    "bugNodeStartChar": 4690,
    "bugNodeLength": 16,
    "fixLineNum": 108,
    "fixNodeStartChar": 4690,
    "fixNodeLength": 16,
    "sourceBeforeFix": "next.getField(0)",
    "sourceAfterFix": "next.getField(1)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "0559e4a6a1c956394bf3ce184be7bf4f9a91108e",
    "fixCommitParentSHA1": "93ecfc6406298b0e91e5298907e165fe391b3f6f",
    "bugFilePath": "stratosphere-addons/avro/src/test/java/eu/stratosphere/api/avro/AvroExternalJarProgramITCase.java",
    "fixPatch": "diff --git a/stratosphere-addons/avro/src/test/java/eu/stratosphere/api/avro/AvroExternalJarProgramITCase.java b/stratosphere-addons/avro/src/test/java/eu/stratosphere/api/avro/AvroExternalJarProgramITCase.java\nindex a34666d..cb4eb35 100644\n--- a/stratosphere-addons/avro/src/test/java/eu/stratosphere/api/avro/AvroExternalJarProgramITCase.java\n+++ b/stratosphere-addons/avro/src/test/java/eu/stratosphere/api/avro/AvroExternalJarProgramITCase.java\n@@ -36,7 +36,7 @@\n \tprivate static final String TEST_DATA_FILE = \"/testdata.avro\";\n \n \tstatic {\n-\t\tLogUtils.initializeDefaultConsoleLogger();\n+\t\tLogUtils.initializeDefaultTestConsoleLogger();\n \t}\n \t\n \t@Test\n",
    "projectName": "apache.flink",
    "bugLineNum": 39,
    "bugNodeStartChar": 1511,
    "bugNodeLength": 41,
    "fixLineNum": 39,
    "fixNodeStartChar": 1511,
    "fixNodeLength": 45,
    "sourceBeforeFix": "LogUtils.initializeDefaultConsoleLogger()",
    "sourceAfterFix": "LogUtils.initializeDefaultTestConsoleLogger()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "0559e4a6a1c956394bf3ce184be7bf4f9a91108e",
    "fixCommitParentSHA1": "93ecfc6406298b0e91e5298907e165fe391b3f6f",
    "bugFilePath": "stratosphere-addons/avro/src/test/java/eu/stratosphere/api/avro/AvroExternalJarProgramITCase.java",
    "fixPatch": "diff --git a/stratosphere-addons/avro/src/test/java/eu/stratosphere/api/avro/AvroExternalJarProgramITCase.java b/stratosphere-addons/avro/src/test/java/eu/stratosphere/api/avro/AvroExternalJarProgramITCase.java\nindex a34666d..cb4eb35 100644\n--- a/stratosphere-addons/avro/src/test/java/eu/stratosphere/api/avro/AvroExternalJarProgramITCase.java\n+++ b/stratosphere-addons/avro/src/test/java/eu/stratosphere/api/avro/AvroExternalJarProgramITCase.java\n@@ -36,7 +36,7 @@\n \tprivate static final String TEST_DATA_FILE = \"/testdata.avro\";\n \n \tstatic {\n-\t\tLogUtils.initializeDefaultConsoleLogger();\n+\t\tLogUtils.initializeDefaultTestConsoleLogger();\n \t}\n \t\n \t@Test\n",
    "projectName": "apache.flink",
    "bugLineNum": 39,
    "bugNodeStartChar": 1511,
    "bugNodeLength": 41,
    "fixLineNum": 39,
    "fixNodeStartChar": 1511,
    "fixNodeLength": 45,
    "sourceBeforeFix": "LogUtils.initializeDefaultConsoleLogger()",
    "sourceAfterFix": "LogUtils.initializeDefaultTestConsoleLogger()"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "7eab75c2851c3494d6bdea34a4d7e8c594fdb7a1",
    "fixCommitParentSHA1": "4aa4d104847e015cd0aa9cf21cbdb679f26b3674",
    "bugFilePath": "stratosphere-tests/src/test/java/eu/stratosphere/test/exampleScalaPrograms/ConnectedComponentsITCase.java",
    "fixPatch": "diff --git a/stratosphere-tests/src/test/java/eu/stratosphere/test/exampleScalaPrograms/ConnectedComponentsITCase.java b/stratosphere-tests/src/test/java/eu/stratosphere/test/exampleScalaPrograms/ConnectedComponentsITCase.java\nindex 7caa948..039b70d 100644\n--- a/stratosphere-tests/src/test/java/eu/stratosphere/test/exampleScalaPrograms/ConnectedComponentsITCase.java\n+++ b/stratosphere-tests/src/test/java/eu/stratosphere/test/exampleScalaPrograms/ConnectedComponentsITCase.java\n@@ -31,7 +31,7 @@\n         int dop = config.getInteger(\"ConnectedComponents#NumSubtasks\", 1);\n         int maxIterations = config.getInteger(\"ConnectedComponents#NumIterations\", 1);\n         ConnectedComponents cc = new ConnectedComponents();\n-        Plan plan = cc.getPlan(verticesPath, edgesPath, resultPath, maxIterations);\n+        Plan plan = cc.getScalaPlan(verticesPath, edgesPath, resultPath, maxIterations);\n         plan.setDefaultParallelism(dop);\n         return plan;\n     }\n",
    "projectName": "apache.flink",
    "bugLineNum": 34,
    "bugNodeStartChar": 1673,
    "bugNodeLength": 62,
    "fixLineNum": 34,
    "fixNodeStartChar": 1673,
    "fixNodeLength": 67,
    "sourceBeforeFix": "cc.getPlan(verticesPath,edgesPath,resultPath,maxIterations)",
    "sourceAfterFix": "cc.getScalaPlan(verticesPath,edgesPath,resultPath,maxIterations)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "7eab75c2851c3494d6bdea34a4d7e8c594fdb7a1",
    "fixCommitParentSHA1": "4aa4d104847e015cd0aa9cf21cbdb679f26b3674",
    "bugFilePath": "stratosphere-tests/src/test/java/eu/stratosphere/test/exampleScalaPrograms/ConnectedComponentsITCase.java",
    "fixPatch": "diff --git a/stratosphere-tests/src/test/java/eu/stratosphere/test/exampleScalaPrograms/ConnectedComponentsITCase.java b/stratosphere-tests/src/test/java/eu/stratosphere/test/exampleScalaPrograms/ConnectedComponentsITCase.java\nindex 7caa948..039b70d 100644\n--- a/stratosphere-tests/src/test/java/eu/stratosphere/test/exampleScalaPrograms/ConnectedComponentsITCase.java\n+++ b/stratosphere-tests/src/test/java/eu/stratosphere/test/exampleScalaPrograms/ConnectedComponentsITCase.java\n@@ -31,7 +31,7 @@\n         int dop = config.getInteger(\"ConnectedComponents#NumSubtasks\", 1);\n         int maxIterations = config.getInteger(\"ConnectedComponents#NumIterations\", 1);\n         ConnectedComponents cc = new ConnectedComponents();\n-        Plan plan = cc.getPlan(verticesPath, edgesPath, resultPath, maxIterations);\n+        Plan plan = cc.getScalaPlan(verticesPath, edgesPath, resultPath, maxIterations);\n         plan.setDefaultParallelism(dop);\n         return plan;\n     }\n",
    "projectName": "apache.flink",
    "bugLineNum": 34,
    "bugNodeStartChar": 1673,
    "bugNodeLength": 62,
    "fixLineNum": 34,
    "fixNodeStartChar": 1673,
    "fixNodeLength": 67,
    "sourceBeforeFix": "cc.getPlan(verticesPath,edgesPath,resultPath,maxIterations)",
    "sourceAfterFix": "cc.getScalaPlan(verticesPath,edgesPath,resultPath,maxIterations)"
  },
  {
    "bugType": "MORE_SPECIFIC_IF",
    "fixCommitSHA1": "bb5d7708d8ddd7791d61b7a3f145cdbfbefa6fc3",
    "fixCommitParentSHA1": "80551cb783a5e3bf47c20f6c1b4059dd8c691220",
    "bugFilePath": "nephele/nephele-management/src/main/java/eu/stratosphere/nephele/topology/NetworkTopology.java",
    "fixPatch": "diff --git a/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/topology/NetworkTopology.java b/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/topology/NetworkTopology.java\nindex d67fc5d..7b2b370 100644\n--- a/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/topology/NetworkTopology.java\n+++ b/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/topology/NetworkTopology.java\n@@ -61,7 +61,7 @@\n \n \t\ttry {\n \n-\t\t\twhile ((strLine = br.readLine()) != null) {\n+\t\t\twhile ((strLine = br.readLine()) != null && !strLine.isEmpty()) {\n \n \t\t\t\tfinal Matcher m = pattern.matcher(strLine);\n \t\t\t\tif (!m.matches()) {\n",
    "projectName": "apache.flink",
    "bugLineNum": 64,
    "bugNodeStartChar": 2314,
    "bugNodeLength": 33,
    "fixLineNum": 64,
    "fixNodeStartChar": 2314,
    "fixNodeLength": 55,
    "sourceBeforeFix": "(strLine=br.readLine()) != null",
    "sourceAfterFix": "(strLine=br.readLine()) != null && !strLine.isEmpty()"
  },
  {
    "bugType": "LESS_SPECIFIC_IF",
    "fixCommitSHA1": "b08240d13a4208eda702539d0c47a068c67515b2",
    "fixCommitParentSHA1": "31dbfe65e46d8419d0da8d670b5958407adbdbf2",
    "bugFilePath": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plantranslate/NepheleJobGraphGenerator.java",
    "fixPatch": "diff --git a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plantranslate/NepheleJobGraphGenerator.java b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plantranslate/NepheleJobGraphGenerator.java\nindex 7db06db..6ecdb8e 100644\n--- a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plantranslate/NepheleJobGraphGenerator.java\n+++ b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plantranslate/NepheleJobGraphGenerator.java\n@@ -229,7 +229,7 @@\n \t@Override\n \tpublic boolean preVisit(PlanNode node) {\n \t\t// check if we have visited this node before. in non-tree graphs, this happens\n-\t\tif (this.vertices.containsKey(node) || this.chainedTasks.containsKey(node)) {\n+\t\tif (this.vertices.containsKey(node) || this.chainedTasks.containsKey(node) || this.iterations.containsKey(node)) {\n \t\t\t// return false to prevent further descend\n \t\t\treturn false;\n \t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 232,
    "bugNodeStartChar": 10837,
    "bugNodeLength": 70,
    "fixLineNum": 232,
    "fixNodeStartChar": 10837,
    "fixNodeLength": 107,
    "sourceBeforeFix": "this.vertices.containsKey(node) || this.chainedTasks.containsKey(node)",
    "sourceAfterFix": "this.vertices.containsKey(node) || this.chainedTasks.containsKey(node) || this.iterations.containsKey(node)"
  },
  {
    "bugType": "CHANGE_OPERAND",
    "fixCommitSHA1": "0f28095f9f79b49d45905e61709ff4fab6fedd54",
    "fixCommitParentSHA1": "242c704251dc15a0314255c64d5a9c00ac8582b1",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactString.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactString.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactString.java\nindex c550a33..f4793ae 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactString.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactString.java\n@@ -457,7 +457,7 @@\n \tpublic Appendable append(CharSequence csq, int start, int end) {\n \t\tfinal int otherLen = end - start;\n \t\tgrow(this.len + otherLen);\n-\t\tfor (int pos = start; pos < len; pos++)\n+\t\tfor (int pos = start; pos < end; pos++)\n \t\t\tthis.value[this.len + pos] = csq.charAt(pos);\n \t\tthis.len += otherLen;\n \t\treturn this;\n",
    "projectName": "apache.flink",
    "bugLineNum": 460,
    "bugNodeStartChar": 14398,
    "bugNodeLength": 9,
    "fixLineNum": 460,
    "fixNodeStartChar": 14398,
    "fixNodeLength": 9,
    "sourceBeforeFix": "pos < len",
    "sourceAfterFix": "pos < end"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "0f28095f9f79b49d45905e61709ff4fab6fedd54",
    "fixCommitParentSHA1": "242c704251dc15a0314255c64d5a9c00ac8582b1",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactString.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactString.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactString.java\nindex c550a33..f4793ae 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactString.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/base/PactString.java\n@@ -457,7 +457,7 @@\n \tpublic Appendable append(CharSequence csq, int start, int end) {\n \t\tfinal int otherLen = end - start;\n \t\tgrow(this.len + otherLen);\n-\t\tfor (int pos = start; pos < len; pos++)\n+\t\tfor (int pos = start; pos < end; pos++)\n \t\t\tthis.value[this.len + pos] = csq.charAt(pos);\n \t\tthis.len += otherLen;\n \t\treturn this;\n",
    "projectName": "apache.flink",
    "bugLineNum": 460,
    "bugNodeStartChar": 14398,
    "bugNodeLength": 9,
    "fixLineNum": 460,
    "fixNodeStartChar": 14398,
    "fixNodeLength": 9,
    "sourceBeforeFix": "pos < len",
    "sourceAfterFix": "pos < end"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 48,
    "bugNodeStartChar": 2557,
    "bugNodeLength": 826,
    "fixLineNum": 48,
    "fixNodeStartChar": 2557,
    "fixNodeLength": 835,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 66,
    "bugNodeStartChar": 3387,
    "bugNodeLength": 1180,
    "fixLineNum": 66,
    "fixNodeStartChar": 3387,
    "fixNodeLength": 1189,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 93,
    "bugNodeStartChar": 4571,
    "bugNodeLength": 370,
    "fixLineNum": 93,
    "fixNodeStartChar": 4571,
    "fixNodeLength": 379,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 104,
    "bugNodeStartChar": 4945,
    "bugNodeLength": 1550,
    "fixLineNum": 104,
    "fixNodeStartChar": 4945,
    "fixNodeLength": 1559,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 141,
    "bugNodeStartChar": 6499,
    "bugNodeLength": 738,
    "fixLineNum": 141,
    "fixNodeStartChar": 6499,
    "fixNodeLength": 747,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 158,
    "bugNodeStartChar": 7340,
    "bugNodeLength": 255,
    "fixLineNum": 158,
    "fixNodeStartChar": 7340,
    "fixNodeLength": 264,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 165,
    "bugNodeStartChar": 7599,
    "bugNodeLength": 297,
    "fixLineNum": 165,
    "fixNodeStartChar": 7599,
    "fixNodeLength": 306,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 173,
    "bugNodeStartChar": 7899,
    "bugNodeLength": 279,
    "fixLineNum": 173,
    "fixNodeStartChar": 7899,
    "fixNodeLength": 288,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 181,
    "bugNodeStartChar": 8182,
    "bugNodeLength": 580,
    "fixLineNum": 181,
    "fixNodeStartChar": 8182,
    "fixNodeLength": 589,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 192,
    "bugNodeStartChar": 8766,
    "bugNodeLength": 1427,
    "fixLineNum": 192,
    "fixNodeStartChar": 8766,
    "fixNodeLength": 1436,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 215,
    "bugNodeStartChar": 10196,
    "bugNodeLength": 706,
    "fixLineNum": 215,
    "fixNodeStartChar": 10196,
    "fixNodeLength": 715,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 229,
    "bugNodeStartChar": 10906,
    "bugNodeLength": 652,
    "fixLineNum": 229,
    "fixNodeStartChar": 10906,
    "fixNodeLength": 661,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 243,
    "bugNodeStartChar": 11561,
    "bugNodeLength": 335,
    "fixLineNum": 243,
    "fixNodeStartChar": 11561,
    "fixNodeLength": 344,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\nindex a454210..d10e584 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeComparator.java\n@@ -43,7 +43,7 @@\n  * \n  * @param T The data type that the comparator works on.\n  */\n-public interface TypeComparator<T>\n+public abstract class TypeComparator<T>\n {\t\n \t/**\n \t * Computes a hash value for the given record. The hash value should include all fields in the record\n@@ -61,7 +61,7 @@\n \t * \n \t * @see java.lang.Object#hashCode()\n \t */\n-\tpublic int hash(T record);\n+\tpublic abstract int hash(T record);\n \t\n \t/**\n \t * Sets the given element as the comparison reference for future calls to\n@@ -88,7 +88,7 @@\n \t * \n \t * @param toCompare The element to set as the comparison reference.\n \t */\n-\tpublic void setReference(T toCompare);\n+\tpublic abstract void setReference(T toCompare);\n \t\n \t/**\n \t * Checks, whether the given element is equal to the element that has been set as the comparison\n@@ -99,7 +99,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic boolean equalToReference(T candidate);\n+\tpublic abstract boolean equalToReference(T candidate);\n \t\n \t/**\n \t * This method compares the element that has been set as reference in this type accessor, to the\n@@ -136,7 +136,7 @@\n \t * \n \t * @see #setReference(Object)\n \t */\n-\tpublic int compareToReference(TypeComparator<T> referencedComparator);\n+\tpublic abstract int compareToReference(TypeComparator<T> referencedComparator);\n \t\n \t/**\n \t * Compares two records in serialized from. The return value indicates the order of the two in the same way\n@@ -151,7 +151,7 @@\n \t * \n \t *  @see java.util.Comparator#compare(Object, Object)\n \t */\n-\tpublic int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n+\tpublic abstract int compare(DataInputView firstSource, DataInputView secondSource) throws IOException;\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -160,7 +160,7 @@\n \t * \n \t * @return True, if the data type supports the creation of a normalized key for comparison, false otherwise.\n \t */\n-\tpublic boolean supportsNormalizedKey();\n+\tpublic abstract boolean supportsNormalizedKey();\n \t\n \t/**\n \t * Check whether this comparator supports to serialize the record in a format that replaces its keys by a normalized\n@@ -168,7 +168,7 @@\n \t * \n \t * @return True, if the comparator supports that specific form of serialization, false if not.\n \t */\n-\tpublic boolean supportsSerializationWithKeyNormalization();\n+\tpublic abstract boolean supportsSerializationWithKeyNormalization();\n \n \t/**\n \t * Gets the number of bytes that the normalized key would maximally take. A value of\n@@ -176,7 +176,7 @@\n \t * \n \t * @return The number of bytes that the normalized key would maximally take.\n \t */\n-\tpublic int getNormalizeKeyLen();\n+\tpublic abstract int getNormalizeKeyLen();\n \t\n \t/**\n \t * Checks, whether the given number of bytes for a normalized suffice to determine the order of elements\n@@ -187,7 +187,7 @@\n \t * @return True, if the given number of bytes for a normalized suffice to determine the order of elements,\n \t *         false otherwise.\n \t */\n-\tpublic boolean isNormalizedKeyPrefixOnly(int keyBytes);\n+\tpublic abstract boolean isNormalizedKeyPrefixOnly(int keyBytes);\n \t\n \t/**\n \t * Writes a normalized key for the given record into the target byte array, starting at the specified position\n@@ -210,7 +210,7 @@\n \t * \n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n+\tpublic abstract void putNormalizedKey(T record, byte[] target, int offset, int numBytes);\n \n \t/**\n \t * Writes the record in such a fashion that all keys are normalizing and at the beginning of the serialized data.\n@@ -224,7 +224,7 @@\n \t * @see #readWithKeyDenormalization(Object, DataInputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void writeWithKeyNormalization(T record, DataOutputView target) throws IOException;\n \t\n \t/**\n \t * Reads the record back while de-normalizing the key fields. This must only be used when\n@@ -238,7 +238,7 @@\n \t * @see #writeWithKeyNormalization(Object, DataOutputView)\n \t * @see NormalizableKey#copyNormalizedKey(byte[], int, int)\n \t */\n-\tpublic void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n+\tpublic abstract void readWithKeyDenormalization(T record, DataInputView source) throws IOException;\n \n \t/**\n \t * Flag whether normalized key comparisons should be inverted key should be interpreted\n@@ -247,7 +247,7 @@\n \t * @return True, if all normalized key comparisons should invert the sign of the comparison result,\n \t *         false if the normalized key should be used as is.\n \t */\n-\tpublic boolean invertNormalizedKey();\n+\tpublic abstract boolean invertNormalizedKey();\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -257,5 +257,5 @@\n \t * \n \t * @return A deep copy of this comparator instance.\n \t */\n-\tpublic TypeComparator<T> duplicate();\n+\tpublic abstract TypeComparator<T> duplicate();\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 254,
    "bugNodeStartChar": 11999,
    "bugNodeLength": 247,
    "fixLineNum": 254,
    "fixNodeStartChar": 11999,
    "fixNodeLength": 256,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java\nindex 38b6581..a271c6a 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java\n@@ -28,14 +28,14 @@\n  * @param <T1> The class of the first data type.\n  * @param <T2> The class of the second data type. \n  */\n-public interface TypePairComparator<T1, T2>\n+public abstract class TypePairComparator<T1, T2>\n {\n \t/**\n \t * Sets the reference for comparisons.\n \t * \n \t * @param reference The reference instance.\n \t */\n-\tpublic void setReference(T1 reference);\n+\tpublic abstract void setReference(T1 reference);\n \t\n \t/**\n \t * Checks, whether the given candidate instance is equal to the reference instance, with respect\n@@ -44,7 +44,7 @@\n \t * @param candidate The candidate to check.\n \t * @return True, if the candidate is equal to the reference, false otherwise.\n \t */\n-\tpublic boolean equalToReference(T2 candidate);\n+\tpublic abstract boolean equalToReference(T2 candidate);\n \t\n-\tpublic int compareToReference(T2 candidate);\n+\tpublic abstract int compareToReference(T2 candidate);\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 33,
    "bugNodeStartChar": 1671,
    "bugNodeLength": 139,
    "fixLineNum": 33,
    "fixNodeStartChar": 1671,
    "fixNodeLength": 148,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java\nindex 38b6581..a271c6a 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java\n@@ -28,14 +28,14 @@\n  * @param <T1> The class of the first data type.\n  * @param <T2> The class of the second data type. \n  */\n-public interface TypePairComparator<T1, T2>\n+public abstract class TypePairComparator<T1, T2>\n {\n \t/**\n \t * Sets the reference for comparisons.\n \t * \n \t * @param reference The reference instance.\n \t */\n-\tpublic void setReference(T1 reference);\n+\tpublic abstract void setReference(T1 reference);\n \t\n \t/**\n \t * Checks, whether the given candidate instance is equal to the reference instance, with respect\n@@ -44,7 +44,7 @@\n \t * @param candidate The candidate to check.\n \t * @return True, if the candidate is equal to the reference, false otherwise.\n \t */\n-\tpublic boolean equalToReference(T2 candidate);\n+\tpublic abstract boolean equalToReference(T2 candidate);\n \t\n-\tpublic int compareToReference(T2 candidate);\n+\tpublic abstract int compareToReference(T2 candidate);\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 40,
    "bugNodeStartChar": 1814,
    "bugNodeLength": 329,
    "fixLineNum": 40,
    "fixNodeStartChar": 1814,
    "fixNodeLength": 338,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java\nindex 38b6581..a271c6a 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypePairComparator.java\n@@ -28,14 +28,14 @@\n  * @param <T1> The class of the first data type.\n  * @param <T2> The class of the second data type. \n  */\n-public interface TypePairComparator<T1, T2>\n+public abstract class TypePairComparator<T1, T2>\n {\n \t/**\n \t * Sets the reference for comparisons.\n \t * \n \t * @param reference The reference instance.\n \t */\n-\tpublic void setReference(T1 reference);\n+\tpublic abstract void setReference(T1 reference);\n \t\n \t/**\n \t * Checks, whether the given candidate instance is equal to the reference instance, with respect\n@@ -44,7 +44,7 @@\n \t * @param candidate The candidate to check.\n \t * @return True, if the candidate is equal to the reference, false otherwise.\n \t */\n-\tpublic boolean equalToReference(T2 candidate);\n+\tpublic abstract boolean equalToReference(T2 candidate);\n \t\n-\tpublic int compareToReference(T2 candidate);\n+\tpublic abstract int compareToReference(T2 candidate);\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 49,
    "bugNodeStartChar": 2147,
    "bugNodeLength": 44,
    "fixLineNum": 49,
    "fixNodeStartChar": 2147,
    "fixNodeLength": 53,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\nindex 34b1f94..8e0981b 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n@@ -30,14 +30,14 @@\n  * \n  * @param T The data type that the serializer serializes.\n  */\n-public interface TypeSerializer<T>\n+public abstract class TypeSerializer<T>\n {\n \t/**\n \t * Creates a new instance of the data type.\n \t * \n \t * @return A new instance of the data type.\n \t */\n-\tpublic T createInstance();\n+\tpublic abstract T createInstance();\n \t\n \t/**\n \t * Creates a copy from the given element.\n@@ -45,7 +45,7 @@\n \t * @param from The element to copy.\n \t * @return A copy of the given element.\n \t */\n-\tpublic T createCopy(T from);\n+\tpublic abstract T createCopy(T from);\n \t\n \t/**\n \t * Creates a copy from the given element, storing the copied result in the given target element.\n@@ -53,7 +53,7 @@\n \t * @param from The element to be copied.\n \t * @param to The target element.\n \t */\n-\tpublic void copyTo(T from, T to);\n+\tpublic abstract void copyTo(T from, T to);\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -62,7 +62,7 @@\n \t * \n \t * @return The length of the data type, or <code>-1</code> for variable length data types.\n \t */\n-\tpublic int getLength();\n+\tpublic abstract int getLength();\n \t\n \t// --------------------------------------------------------------------------------------------\n \n@@ -76,7 +76,7 @@\n \t * @throws IOException Thrown, if the serialization encountered an I/O related error. Typically raised by the\n \t *                     output view, which may have an underlying I/O channel to which it delegates.\n \t */\n-\tpublic void serialize(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void serialize(T record, DataOutputView target) throws IOException;\n \n \t/**\n \t * De-serializes a record from the given source input view into the given target record instance.\n@@ -87,7 +87,7 @@\n \t * @throws IOException Thrown, if the de-serialization encountered an I/O related error. Typically raised by the\n \t *                     input view, which may have an underlying I/O channel from which it reads.\n \t */\n-\tpublic void deserialize(T target, DataInputView source) throws IOException;\n+\tpublic abstract void deserialize(T target, DataInputView source) throws IOException;\n \t\n \t/**\n \t * Copies exactly one record from the source input view to the target output view. Whether this operation\n@@ -101,5 +101,5 @@\n \t * \n \t * @throws IOException Thrown if any of the two views raises an exception.\n \t */\n-\tpublic void copy(DataInputView source, DataOutputView target) throws IOException;\n+\tpublic abstract void copy(DataInputView source, DataOutputView target) throws IOException;\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 35,
    "bugNodeStartChar": 1642,
    "bugNodeLength": 131,
    "fixLineNum": 35,
    "fixNodeStartChar": 1642,
    "fixNodeLength": 140,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\nindex 34b1f94..8e0981b 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n@@ -30,14 +30,14 @@\n  * \n  * @param T The data type that the serializer serializes.\n  */\n-public interface TypeSerializer<T>\n+public abstract class TypeSerializer<T>\n {\n \t/**\n \t * Creates a new instance of the data type.\n \t * \n \t * @return A new instance of the data type.\n \t */\n-\tpublic T createInstance();\n+\tpublic abstract T createInstance();\n \t\n \t/**\n \t * Creates a copy from the given element.\n@@ -45,7 +45,7 @@\n \t * @param from The element to copy.\n \t * @return A copy of the given element.\n \t */\n-\tpublic T createCopy(T from);\n+\tpublic abstract T createCopy(T from);\n \t\n \t/**\n \t * Creates a copy from the given element, storing the copied result in the given target element.\n@@ -53,7 +53,7 @@\n \t * @param from The element to be copied.\n \t * @param to The target element.\n \t */\n-\tpublic void copyTo(T from, T to);\n+\tpublic abstract void copyTo(T from, T to);\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -62,7 +62,7 @@\n \t * \n \t * @return The length of the data type, or <code>-1</code> for variable length data types.\n \t */\n-\tpublic int getLength();\n+\tpublic abstract int getLength();\n \t\n \t// --------------------------------------------------------------------------------------------\n \n@@ -76,7 +76,7 @@\n \t * @throws IOException Thrown, if the serialization encountered an I/O related error. Typically raised by the\n \t *                     output view, which may have an underlying I/O channel to which it delegates.\n \t */\n-\tpublic void serialize(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void serialize(T record, DataOutputView target) throws IOException;\n \n \t/**\n \t * De-serializes a record from the given source input view into the given target record instance.\n@@ -87,7 +87,7 @@\n \t * @throws IOException Thrown, if the de-serialization encountered an I/O related error. Typically raised by the\n \t *                     input view, which may have an underlying I/O channel from which it reads.\n \t */\n-\tpublic void deserialize(T target, DataInputView source) throws IOException;\n+\tpublic abstract void deserialize(T target, DataInputView source) throws IOException;\n \t\n \t/**\n \t * Copies exactly one record from the source input view to the target output view. Whether this operation\n@@ -101,5 +101,5 @@\n \t * \n \t * @throws IOException Thrown if any of the two views raises an exception.\n \t */\n-\tpublic void copy(DataInputView source, DataOutputView target) throws IOException;\n+\tpublic abstract void copy(DataInputView source, DataOutputView target) throws IOException;\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 42,
    "bugNodeStartChar": 1777,
    "bugNodeLength": 164,
    "fixLineNum": 42,
    "fixNodeStartChar": 1777,
    "fixNodeLength": 173,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\nindex 34b1f94..8e0981b 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n@@ -30,14 +30,14 @@\n  * \n  * @param T The data type that the serializer serializes.\n  */\n-public interface TypeSerializer<T>\n+public abstract class TypeSerializer<T>\n {\n \t/**\n \t * Creates a new instance of the data type.\n \t * \n \t * @return A new instance of the data type.\n \t */\n-\tpublic T createInstance();\n+\tpublic abstract T createInstance();\n \t\n \t/**\n \t * Creates a copy from the given element.\n@@ -45,7 +45,7 @@\n \t * @param from The element to copy.\n \t * @return A copy of the given element.\n \t */\n-\tpublic T createCopy(T from);\n+\tpublic abstract T createCopy(T from);\n \t\n \t/**\n \t * Creates a copy from the given element, storing the copied result in the given target element.\n@@ -53,7 +53,7 @@\n \t * @param from The element to be copied.\n \t * @param to The target element.\n \t */\n-\tpublic void copyTo(T from, T to);\n+\tpublic abstract void copyTo(T from, T to);\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -62,7 +62,7 @@\n \t * \n \t * @return The length of the data type, or <code>-1</code> for variable length data types.\n \t */\n-\tpublic int getLength();\n+\tpublic abstract int getLength();\n \t\n \t// --------------------------------------------------------------------------------------------\n \n@@ -76,7 +76,7 @@\n \t * @throws IOException Thrown, if the serialization encountered an I/O related error. Typically raised by the\n \t *                     output view, which may have an underlying I/O channel to which it delegates.\n \t */\n-\tpublic void serialize(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void serialize(T record, DataOutputView target) throws IOException;\n \n \t/**\n \t * De-serializes a record from the given source input view into the given target record instance.\n@@ -87,7 +87,7 @@\n \t * @throws IOException Thrown, if the de-serialization encountered an I/O related error. Typically raised by the\n \t *                     input view, which may have an underlying I/O channel from which it reads.\n \t */\n-\tpublic void deserialize(T target, DataInputView source) throws IOException;\n+\tpublic abstract void deserialize(T target, DataInputView source) throws IOException;\n \t\n \t/**\n \t * Copies exactly one record from the source input view to the target output view. Whether this operation\n@@ -101,5 +101,5 @@\n \t * \n \t * @throws IOException Thrown if any of the two views raises an exception.\n \t */\n-\tpublic void copy(DataInputView source, DataOutputView target) throws IOException;\n+\tpublic abstract void copy(DataInputView source, DataOutputView target) throws IOException;\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 50,
    "bugNodeStartChar": 1945,
    "bugNodeLength": 222,
    "fixLineNum": 50,
    "fixNodeStartChar": 1945,
    "fixNodeLength": 231,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\nindex 34b1f94..8e0981b 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n@@ -30,14 +30,14 @@\n  * \n  * @param T The data type that the serializer serializes.\n  */\n-public interface TypeSerializer<T>\n+public abstract class TypeSerializer<T>\n {\n \t/**\n \t * Creates a new instance of the data type.\n \t * \n \t * @return A new instance of the data type.\n \t */\n-\tpublic T createInstance();\n+\tpublic abstract T createInstance();\n \t\n \t/**\n \t * Creates a copy from the given element.\n@@ -45,7 +45,7 @@\n \t * @param from The element to copy.\n \t * @return A copy of the given element.\n \t */\n-\tpublic T createCopy(T from);\n+\tpublic abstract T createCopy(T from);\n \t\n \t/**\n \t * Creates a copy from the given element, storing the copied result in the given target element.\n@@ -53,7 +53,7 @@\n \t * @param from The element to be copied.\n \t * @param to The target element.\n \t */\n-\tpublic void copyTo(T from, T to);\n+\tpublic abstract void copyTo(T from, T to);\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -62,7 +62,7 @@\n \t * \n \t * @return The length of the data type, or <code>-1</code> for variable length data types.\n \t */\n-\tpublic int getLength();\n+\tpublic abstract int getLength();\n \t\n \t// --------------------------------------------------------------------------------------------\n \n@@ -76,7 +76,7 @@\n \t * @throws IOException Thrown, if the serialization encountered an I/O related error. Typically raised by the\n \t *                     output view, which may have an underlying I/O channel to which it delegates.\n \t */\n-\tpublic void serialize(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void serialize(T record, DataOutputView target) throws IOException;\n \n \t/**\n \t * De-serializes a record from the given source input view into the given target record instance.\n@@ -87,7 +87,7 @@\n \t * @throws IOException Thrown, if the de-serialization encountered an I/O related error. Typically raised by the\n \t *                     input view, which may have an underlying I/O channel from which it reads.\n \t */\n-\tpublic void deserialize(T target, DataInputView source) throws IOException;\n+\tpublic abstract void deserialize(T target, DataInputView source) throws IOException;\n \t\n \t/**\n \t * Copies exactly one record from the source input view to the target output view. Whether this operation\n@@ -101,5 +101,5 @@\n \t * \n \t * @throws IOException Thrown if any of the two views raises an exception.\n \t */\n-\tpublic void copy(DataInputView source, DataOutputView target) throws IOException;\n+\tpublic abstract void copy(DataInputView source, DataOutputView target) throws IOException;\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 60,
    "bugNodeStartChar": 2270,
    "bugNodeLength": 201,
    "fixLineNum": 60,
    "fixNodeStartChar": 2270,
    "fixNodeLength": 210,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\nindex 34b1f94..8e0981b 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n@@ -30,14 +30,14 @@\n  * \n  * @param T The data type that the serializer serializes.\n  */\n-public interface TypeSerializer<T>\n+public abstract class TypeSerializer<T>\n {\n \t/**\n \t * Creates a new instance of the data type.\n \t * \n \t * @return A new instance of the data type.\n \t */\n-\tpublic T createInstance();\n+\tpublic abstract T createInstance();\n \t\n \t/**\n \t * Creates a copy from the given element.\n@@ -45,7 +45,7 @@\n \t * @param from The element to copy.\n \t * @return A copy of the given element.\n \t */\n-\tpublic T createCopy(T from);\n+\tpublic abstract T createCopy(T from);\n \t\n \t/**\n \t * Creates a copy from the given element, storing the copied result in the given target element.\n@@ -53,7 +53,7 @@\n \t * @param from The element to be copied.\n \t * @param to The target element.\n \t */\n-\tpublic void copyTo(T from, T to);\n+\tpublic abstract void copyTo(T from, T to);\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -62,7 +62,7 @@\n \t * \n \t * @return The length of the data type, or <code>-1</code> for variable length data types.\n \t */\n-\tpublic int getLength();\n+\tpublic abstract int getLength();\n \t\n \t// --------------------------------------------------------------------------------------------\n \n@@ -76,7 +76,7 @@\n \t * @throws IOException Thrown, if the serialization encountered an I/O related error. Typically raised by the\n \t *                     output view, which may have an underlying I/O channel to which it delegates.\n \t */\n-\tpublic void serialize(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void serialize(T record, DataOutputView target) throws IOException;\n \n \t/**\n \t * De-serializes a record from the given source input view into the given target record instance.\n@@ -87,7 +87,7 @@\n \t * @throws IOException Thrown, if the de-serialization encountered an I/O related error. Typically raised by the\n \t *                     input view, which may have an underlying I/O channel from which it reads.\n \t */\n-\tpublic void deserialize(T target, DataInputView source) throws IOException;\n+\tpublic abstract void deserialize(T target, DataInputView source) throws IOException;\n \t\n \t/**\n \t * Copies exactly one record from the source input view to the target output view. Whether this operation\n@@ -101,5 +101,5 @@\n \t * \n \t * @throws IOException Thrown if any of the two views raises an exception.\n \t */\n-\tpublic void copy(DataInputView source, DataOutputView target) throws IOException;\n+\tpublic abstract void copy(DataInputView source, DataOutputView target) throws IOException;\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 69,
    "bugNodeStartChar": 2573,
    "bugNodeLength": 542,
    "fixLineNum": 69,
    "fixNodeStartChar": 2573,
    "fixNodeLength": 551,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\nindex 34b1f94..8e0981b 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n@@ -30,14 +30,14 @@\n  * \n  * @param T The data type that the serializer serializes.\n  */\n-public interface TypeSerializer<T>\n+public abstract class TypeSerializer<T>\n {\n \t/**\n \t * Creates a new instance of the data type.\n \t * \n \t * @return A new instance of the data type.\n \t */\n-\tpublic T createInstance();\n+\tpublic abstract T createInstance();\n \t\n \t/**\n \t * Creates a copy from the given element.\n@@ -45,7 +45,7 @@\n \t * @param from The element to copy.\n \t * @return A copy of the given element.\n \t */\n-\tpublic T createCopy(T from);\n+\tpublic abstract T createCopy(T from);\n \t\n \t/**\n \t * Creates a copy from the given element, storing the copied result in the given target element.\n@@ -53,7 +53,7 @@\n \t * @param from The element to be copied.\n \t * @param to The target element.\n \t */\n-\tpublic void copyTo(T from, T to);\n+\tpublic abstract void copyTo(T from, T to);\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -62,7 +62,7 @@\n \t * \n \t * @return The length of the data type, or <code>-1</code> for variable length data types.\n \t */\n-\tpublic int getLength();\n+\tpublic abstract int getLength();\n \t\n \t// --------------------------------------------------------------------------------------------\n \n@@ -76,7 +76,7 @@\n \t * @throws IOException Thrown, if the serialization encountered an I/O related error. Typically raised by the\n \t *                     output view, which may have an underlying I/O channel to which it delegates.\n \t */\n-\tpublic void serialize(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void serialize(T record, DataOutputView target) throws IOException;\n \n \t/**\n \t * De-serializes a record from the given source input view into the given target record instance.\n@@ -87,7 +87,7 @@\n \t * @throws IOException Thrown, if the de-serialization encountered an I/O related error. Typically raised by the\n \t *                     input view, which may have an underlying I/O channel from which it reads.\n \t */\n-\tpublic void deserialize(T target, DataInputView source) throws IOException;\n+\tpublic abstract void deserialize(T target, DataInputView source) throws IOException;\n \t\n \t/**\n \t * Copies exactly one record from the source input view to the target output view. Whether this operation\n@@ -101,5 +101,5 @@\n \t * \n \t * @throws IOException Thrown if any of the two views raises an exception.\n \t */\n-\tpublic void copy(DataInputView source, DataOutputView target) throws IOException;\n+\tpublic abstract void copy(DataInputView source, DataOutputView target) throws IOException;\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 81,
    "bugNodeStartChar": 3118,
    "bugNodeLength": 543,
    "fixLineNum": 81,
    "fixNodeStartChar": 3118,
    "fixNodeLength": 552,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "8fdd403b6c8331d56890de3165b7a0e77a0eaa42",
    "fixCommitParentSHA1": "b4135dca7a977c67cc8742c360f1a372b40263a9",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\nindex 34b1f94..8e0981b 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/generic/types/TypeSerializer.java\n@@ -30,14 +30,14 @@\n  * \n  * @param T The data type that the serializer serializes.\n  */\n-public interface TypeSerializer<T>\n+public abstract class TypeSerializer<T>\n {\n \t/**\n \t * Creates a new instance of the data type.\n \t * \n \t * @return A new instance of the data type.\n \t */\n-\tpublic T createInstance();\n+\tpublic abstract T createInstance();\n \t\n \t/**\n \t * Creates a copy from the given element.\n@@ -45,7 +45,7 @@\n \t * @param from The element to copy.\n \t * @return A copy of the given element.\n \t */\n-\tpublic T createCopy(T from);\n+\tpublic abstract T createCopy(T from);\n \t\n \t/**\n \t * Creates a copy from the given element, storing the copied result in the given target element.\n@@ -53,7 +53,7 @@\n \t * @param from The element to be copied.\n \t * @param to The target element.\n \t */\n-\tpublic void copyTo(T from, T to);\n+\tpublic abstract void copyTo(T from, T to);\n \t\n \t// --------------------------------------------------------------------------------------------\n \t\n@@ -62,7 +62,7 @@\n \t * \n \t * @return The length of the data type, or <code>-1</code> for variable length data types.\n \t */\n-\tpublic int getLength();\n+\tpublic abstract int getLength();\n \t\n \t// --------------------------------------------------------------------------------------------\n \n@@ -76,7 +76,7 @@\n \t * @throws IOException Thrown, if the serialization encountered an I/O related error. Typically raised by the\n \t *                     output view, which may have an underlying I/O channel to which it delegates.\n \t */\n-\tpublic void serialize(T record, DataOutputView target) throws IOException;\n+\tpublic abstract void serialize(T record, DataOutputView target) throws IOException;\n \n \t/**\n \t * De-serializes a record from the given source input view into the given target record instance.\n@@ -87,7 +87,7 @@\n \t * @throws IOException Thrown, if the de-serialization encountered an I/O related error. Typically raised by the\n \t *                     input view, which may have an underlying I/O channel from which it reads.\n \t */\n-\tpublic void deserialize(T target, DataInputView source) throws IOException;\n+\tpublic abstract void deserialize(T target, DataInputView source) throws IOException;\n \t\n \t/**\n \t * Copies exactly one record from the source input view to the target output view. Whether this operation\n@@ -101,5 +101,5 @@\n \t * \n \t * @throws IOException Thrown if any of the two views raises an exception.\n \t */\n-\tpublic void copy(DataInputView source, DataOutputView target) throws IOException;\n+\tpublic abstract void copy(DataInputView source, DataOutputView target) throws IOException;\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 92,
    "bugNodeStartChar": 3665,
    "bugNodeLength": 757,
    "fixLineNum": 92,
    "fixNodeStartChar": 3665,
    "fixNodeLength": 766,
    "sourceBeforeFix": "1",
    "sourceAfterFix": "1025"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/DifferenceTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/DifferenceTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/DifferenceTest.java\nindex 85188fb..c913bd7 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/DifferenceTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/DifferenceTest.java\n@@ -26,7 +26,7 @@\n \t\t\twithInputs(difference);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 29,
    "bugNodeStartChar": 981,
    "bugNodeLength": 38,
    "fixLineNum": 29,
    "fixNodeStartChar": 981,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/DifferenceTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/DifferenceTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/DifferenceTest.java\nindex 85188fb..c913bd7 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/DifferenceTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/DifferenceTest.java\n@@ -26,7 +26,7 @@\n \t\t\twithInputs(difference);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 29,
    "bugNodeStartChar": 981,
    "bugNodeLength": 38,
    "fixLineNum": 29,
    "fixNodeStartChar": 981,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/FilterTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/FilterTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/FilterTest.java\nindex 8166ee6..208fe2e 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/FilterTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/FilterTest.java\n@@ -53,7 +53,7 @@\n \n \t\t// System.out.println(Sop);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 56,
    "bugNodeStartChar": 2391,
    "bugNodeLength": 38,
    "fixLineNum": 56,
    "fixNodeStartChar": 2391,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/FilterTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/FilterTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/FilterTest.java\nindex 8166ee6..208fe2e 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/FilterTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/FilterTest.java\n@@ -53,7 +53,7 @@\n \n \t\t// System.out.println(Sop);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 56,
    "bugNodeStartChar": 2391,
    "bugNodeLength": 38,
    "fixLineNum": 56,
    "fixNodeStartChar": 2391,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\nindex 9a10a4e..5a77c4d 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n@@ -49,7 +49,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -76,7 +76,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -103,7 +103,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -138,7 +138,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -173,7 +173,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 52,
    "bugNodeStartChar": 2284,
    "bugNodeLength": 38,
    "fixLineNum": 52,
    "fixNodeStartChar": 2284,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\nindex 9a10a4e..5a77c4d 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n@@ -49,7 +49,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -76,7 +76,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -103,7 +103,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -138,7 +138,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -173,7 +173,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 52,
    "bugNodeStartChar": 2284,
    "bugNodeLength": 38,
    "fixLineNum": 52,
    "fixNodeStartChar": 2284,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\nindex 9a10a4e..5a77c4d 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n@@ -49,7 +49,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -76,7 +76,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -103,7 +103,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -138,7 +138,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -173,7 +173,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 79,
    "bugNodeStartChar": 3382,
    "bugNodeLength": 38,
    "fixLineNum": 79,
    "fixNodeStartChar": 3382,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\nindex 9a10a4e..5a77c4d 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n@@ -49,7 +49,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -76,7 +76,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -103,7 +103,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -138,7 +138,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -173,7 +173,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 79,
    "bugNodeStartChar": 3382,
    "bugNodeLength": 38,
    "fixLineNum": 79,
    "fixNodeStartChar": 3382,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\nindex 9a10a4e..5a77c4d 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n@@ -49,7 +49,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -76,7 +76,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -103,7 +103,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -138,7 +138,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -173,7 +173,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 106,
    "bugNodeStartChar": 4492,
    "bugNodeLength": 38,
    "fixLineNum": 106,
    "fixNodeStartChar": 4492,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\nindex 9a10a4e..5a77c4d 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n@@ -49,7 +49,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -76,7 +76,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -103,7 +103,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -138,7 +138,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -173,7 +173,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 106,
    "bugNodeStartChar": 4492,
    "bugNodeLength": 38,
    "fixLineNum": 106,
    "fixNodeStartChar": 4492,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\nindex 9a10a4e..5a77c4d 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n@@ -49,7 +49,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -76,7 +76,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -103,7 +103,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -138,7 +138,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -173,7 +173,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 141,
    "bugNodeStartChar": 6007,
    "bugNodeLength": 38,
    "fixLineNum": 141,
    "fixNodeStartChar": 6007,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\nindex 9a10a4e..5a77c4d 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n@@ -49,7 +49,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -76,7 +76,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -103,7 +103,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -138,7 +138,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -173,7 +173,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 141,
    "bugNodeStartChar": 6007,
    "bugNodeLength": 38,
    "fixLineNum": 141,
    "fixNodeStartChar": 6007,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\nindex 9a10a4e..5a77c4d 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n@@ -49,7 +49,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -76,7 +76,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -103,7 +103,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -138,7 +138,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -173,7 +173,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 176,
    "bugNodeStartChar": 7537,
    "bugNodeLength": 38,
    "fixLineNum": 176,
    "fixNodeStartChar": 7537,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\nindex 9a10a4e..5a77c4d 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/GroupingTest.java\n@@ -49,7 +49,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -76,7 +76,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -103,7 +103,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -138,7 +138,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -173,7 +173,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(selection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 176,
    "bugNodeStartChar": 7537,
    "bugNodeLength": 38,
    "fixLineNum": 176,
    "fixNodeStartChar": 7537,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/IntersectionTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/IntersectionTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/IntersectionTest.java\nindex 5d15b28..6ee1075 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/IntersectionTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/IntersectionTest.java\n@@ -24,7 +24,7 @@\n \t\tfinal Sink output = new Sink(\"newUsers.json\").withInputs(intersection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 27,
    "bugNodeStartChar": 969,
    "bugNodeLength": 38,
    "fixLineNum": 27,
    "fixNodeStartChar": 969,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/IntersectionTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/IntersectionTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/IntersectionTest.java\nindex 5d15b28..6ee1075 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/IntersectionTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/IntersectionTest.java\n@@ -24,7 +24,7 @@\n \t\tfinal Sink output = new Sink(\"newUsers.json\").withInputs(intersection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 27,
    "bugNodeStartChar": 969,
    "bugNodeLength": 38,
    "fixLineNum": 27,
    "fixNodeStartChar": 969,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\nindex 1da230a..bf4a2f2 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\n@@ -37,7 +37,7 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -63,7 +63,7 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -91,6 +91,6 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 40,
    "bugNodeStartChar": 1559,
    "bugNodeLength": 38,
    "fixLineNum": 40,
    "fixNodeStartChar": 1559,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\nindex 1da230a..bf4a2f2 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\n@@ -37,7 +37,7 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -63,7 +63,7 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -91,6 +91,6 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 40,
    "bugNodeStartChar": 1559,
    "bugNodeLength": 38,
    "fixLineNum": 40,
    "fixNodeStartChar": 1559,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\nindex 1da230a..bf4a2f2 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\n@@ -37,7 +37,7 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -63,7 +63,7 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -91,6 +91,6 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 66,
    "bugNodeStartChar": 2580,
    "bugNodeLength": 38,
    "fixLineNum": 66,
    "fixNodeStartChar": 2580,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\nindex 1da230a..bf4a2f2 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\n@@ -37,7 +37,7 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -63,7 +63,7 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -91,6 +91,6 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 66,
    "bugNodeStartChar": 2580,
    "bugNodeLength": 38,
    "fixLineNum": 66,
    "fixNodeStartChar": 2580,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\nindex 1da230a..bf4a2f2 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\n@@ -37,7 +37,7 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -63,7 +63,7 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -91,6 +91,6 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 94,
    "bugNodeStartChar": 3656,
    "bugNodeLength": 38,
    "fixLineNum": 94,
    "fixNodeStartChar": 3656,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\nindex 1da230a..bf4a2f2 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/JoinTest.java\n@@ -37,7 +37,7 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -63,7 +63,7 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -91,6 +91,6 @@\n \t\tfinal Sink result = new Sink(\"result.json\").withInputs(join);\n \t\texpectedPlan.setSinks(result);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 94,
    "bugNodeStartChar": 3656,
    "bugNodeLength": 38,
    "fixLineNum": 94,
    "fixNodeStartChar": 3656,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\nindex 2c73a4d..541cf6f 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\n@@ -32,7 +32,7 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -54,7 +54,7 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -79,6 +79,6 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 35,
    "bugNodeStartChar": 1412,
    "bugNodeLength": 38,
    "fixLineNum": 35,
    "fixNodeStartChar": 1412,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\nindex 2c73a4d..541cf6f 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\n@@ -32,7 +32,7 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -54,7 +54,7 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -79,6 +79,6 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 35,
    "bugNodeStartChar": 1412,
    "bugNodeLength": 38,
    "fixLineNum": 35,
    "fixNodeStartChar": 1412,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\nindex 2c73a4d..541cf6f 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\n@@ -32,7 +32,7 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -54,7 +54,7 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -79,6 +79,6 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 57,
    "bugNodeStartChar": 2331,
    "bugNodeLength": 38,
    "fixLineNum": 57,
    "fixNodeStartChar": 2331,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\nindex 2c73a4d..541cf6f 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\n@@ -32,7 +32,7 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -54,7 +54,7 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -79,6 +79,6 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 57,
    "bugNodeStartChar": 2331,
    "bugNodeLength": 38,
    "fixLineNum": 57,
    "fixNodeStartChar": 2331,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\nindex 2c73a4d..541cf6f 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\n@@ -32,7 +32,7 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -54,7 +54,7 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -79,6 +79,6 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 82,
    "bugNodeStartChar": 3502,
    "bugNodeLength": 38,
    "fixLineNum": 82,
    "fixNodeStartChar": 3502,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\nindex 2c73a4d..541cf6f 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/ReplaceTest.java\n@@ -32,7 +32,7 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -54,7 +54,7 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n \t@Test\n@@ -79,6 +79,6 @@\n \t\tfinal Sink normalizedPersons = new Sink(\"normalizedPersons.json\").withInputs(replace);\n \t\texpectedPlan.setSinks(normalizedPersons);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 82,
    "bugNodeStartChar": 3502,
    "bugNodeLength": 38,
    "fixLineNum": 82,
    "fixNodeStartChar": 3502,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/TransformTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/TransformTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/TransformTest.java\nindex 81fd818..ffe18e7 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/TransformTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/TransformTest.java\n@@ -49,7 +49,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(projection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 52,
    "bugNodeStartChar": 2241,
    "bugNodeLength": 38,
    "fixLineNum": 52,
    "fixNodeStartChar": 2241,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/TransformTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/TransformTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/TransformTest.java\nindex 81fd818..ffe18e7 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/TransformTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/TransformTest.java\n@@ -49,7 +49,7 @@\n \t\tfinal Sink output = new Sink(\"output.json\").withInputs(projection);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n \n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 52,
    "bugNodeStartChar": 2241,
    "bugNodeLength": 38,
    "fixLineNum": 52,
    "fixNodeStartChar": 2241,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionAllTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionAllTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionAllTest.java\nindex 050bb02..c410e65 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionAllTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionAllTest.java\n@@ -24,6 +24,6 @@\n \t\tfinal Sink output = new Sink(\"allUsers.json\").withInputs(union);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 27,
    "bugNodeStartChar": 930,
    "bugNodeLength": 38,
    "fixLineNum": 27,
    "fixNodeStartChar": 930,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionAllTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionAllTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionAllTest.java\nindex 050bb02..c410e65 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionAllTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionAllTest.java\n@@ -24,6 +24,6 @@\n \t\tfinal Sink output = new Sink(\"allUsers.json\").withInputs(union);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 27,
    "bugNodeStartChar": 930,
    "bugNodeLength": 38,
    "fixLineNum": 27,
    "fixNodeStartChar": 930,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionTest.java\nindex d02a292..025db61 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionTest.java\n@@ -25,6 +25,6 @@\n \t\tfinal Sink output = new Sink(\"allUsers.json\").withInputs(union);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 28,
    "bugNodeStartChar": 912,
    "bugNodeLength": 38,
    "fixLineNum": 28,
    "fixNodeStartChar": 912,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "5b13fe70c25812157a1e4432135eb9639e72bc6a",
    "fixCommitParentSHA1": "1429274992a0a98e2bec1b72ecc06a507f39e9a3",
    "bugFilePath": "meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionTest.java",
    "fixPatch": "diff --git a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionTest.java b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionTest.java\nindex d02a292..025db61 100755\n--- a/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionTest.java\n+++ b/meteor/meteor-meteor/src/test/java/eu/stratosphere/meteor/base/UnionTest.java\n@@ -25,6 +25,6 @@\n \t\tfinal Sink output = new Sink(\"allUsers.json\").withInputs(union);\n \t\texpectedPlan.setSinks(output);\n \n-\t\tassertEquals(expectedPlan, actualPlan);\n+\t\tassertPlanEquals(expectedPlan, actualPlan);\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 28,
    "bugNodeStartChar": 912,
    "bugNodeLength": 38,
    "fixLineNum": 28,
    "fixNodeStartChar": 912,
    "fixNodeLength": 42,
    "sourceBeforeFix": "assertEquals(expectedPlan,actualPlan)",
    "sourceAfterFix": "assertPlanEquals(expectedPlan,actualPlan)"
  },
  {
    "bugType": "SWAP_BOOLEAN_LITERAL",
    "fixCommitSHA1": "39aa03261a00c409f29341c286febaec99468d65",
    "fixCommitParentSHA1": "2e7c738cd17220d5e72e6002eda686f9be7f9bfb",
    "bugFilePath": "nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java",
    "fixPatch": "diff --git a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\nindex e9f4a94..295b9f4 100644\n--- a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\n+++ b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\n@@ -126,7 +126,7 @@\n \t *         thrown if the given vertex cannot be connected to <code>vertex</code> in the requested manner\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex) throws JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, null, null, -1, -1, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, null, null, -1, -1, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -145,7 +145,7 @@\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex, final int indexOfOutputGate, final int indexOfInputGate)\n \t\t\tthrows JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, null, null, indexOfOutputGate, indexOfInputGate, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, null, null, indexOfOutputGate, indexOfInputGate, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -162,7 +162,7 @@\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex, final ChannelType channelType,\n \t\t\tfinal CompressionLevel compressionLevel) throws JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -182,7 +182,7 @@\n \tpublic void connectTo(final AbstractJobVertex vertex, final ChannelType channelType,\n \t\t\tfinal CompressionLevel compressionLevel, final DistributionPattern distributionPattern)\n \t\t\tthrows JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, distributionPattern, false);\n+\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, distributionPattern, true);\n \t}\n \n \t/**\n@@ -234,7 +234,7 @@\n \t\t\tfinal CompressionLevel compressionLevel, int indexOfOutputGate, int indexOfInputGate,\n \t\t\tfinal DistributionPattern distributionPattern) throws JobGraphDefinitionException {\n \t\tthis.connectTo(vertex, channelType, compressionLevel, indexOfOutputGate, indexOfInputGate, distributionPattern,\n-\t\t\tfalse);\n+\t\t\ttrue);\n \t}\n \n \t/**\n",
    "projectName": "apache.flink",
    "bugLineNum": 129,
    "bugNodeStartChar": 4034,
    "bugNodeLength": 80,
    "fixLineNum": 129,
    "fixNodeStartChar": 4034,
    "fixNodeLength": 79,
    "sourceBeforeFix": "this.connectTo(vertex,null,null,-1,-1,DistributionPattern.BIPARTITE,false)",
    "sourceAfterFix": "this.connectTo(vertex,null,null,-1,-1,DistributionPattern.BIPARTITE,true)"
  },
  {
    "bugType": "SWAP_BOOLEAN_LITERAL",
    "fixCommitSHA1": "39aa03261a00c409f29341c286febaec99468d65",
    "fixCommitParentSHA1": "2e7c738cd17220d5e72e6002eda686f9be7f9bfb",
    "bugFilePath": "nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java",
    "fixPatch": "diff --git a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\nindex e9f4a94..295b9f4 100644\n--- a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\n+++ b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\n@@ -126,7 +126,7 @@\n \t *         thrown if the given vertex cannot be connected to <code>vertex</code> in the requested manner\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex) throws JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, null, null, -1, -1, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, null, null, -1, -1, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -145,7 +145,7 @@\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex, final int indexOfOutputGate, final int indexOfInputGate)\n \t\t\tthrows JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, null, null, indexOfOutputGate, indexOfInputGate, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, null, null, indexOfOutputGate, indexOfInputGate, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -162,7 +162,7 @@\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex, final ChannelType channelType,\n \t\t\tfinal CompressionLevel compressionLevel) throws JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -182,7 +182,7 @@\n \tpublic void connectTo(final AbstractJobVertex vertex, final ChannelType channelType,\n \t\t\tfinal CompressionLevel compressionLevel, final DistributionPattern distributionPattern)\n \t\t\tthrows JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, distributionPattern, false);\n+\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, distributionPattern, true);\n \t}\n \n \t/**\n@@ -234,7 +234,7 @@\n \t\t\tfinal CompressionLevel compressionLevel, int indexOfOutputGate, int indexOfInputGate,\n \t\t\tfinal DistributionPattern distributionPattern) throws JobGraphDefinitionException {\n \t\tthis.connectTo(vertex, channelType, compressionLevel, indexOfOutputGate, indexOfInputGate, distributionPattern,\n-\t\t\tfalse);\n+\t\t\ttrue);\n \t}\n \n \t/**\n",
    "projectName": "apache.flink",
    "bugLineNum": 148,
    "bugNodeStartChar": 4886,
    "bugNodeLength": 109,
    "fixLineNum": 148,
    "fixNodeStartChar": 4886,
    "fixNodeLength": 108,
    "sourceBeforeFix": "this.connectTo(vertex,null,null,indexOfOutputGate,indexOfInputGate,DistributionPattern.BIPARTITE,false)",
    "sourceAfterFix": "this.connectTo(vertex,null,null,indexOfOutputGate,indexOfInputGate,DistributionPattern.BIPARTITE,true)"
  },
  {
    "bugType": "SWAP_BOOLEAN_LITERAL",
    "fixCommitSHA1": "39aa03261a00c409f29341c286febaec99468d65",
    "fixCommitParentSHA1": "2e7c738cd17220d5e72e6002eda686f9be7f9bfb",
    "bugFilePath": "nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java",
    "fixPatch": "diff --git a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\nindex e9f4a94..295b9f4 100644\n--- a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\n+++ b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\n@@ -126,7 +126,7 @@\n \t *         thrown if the given vertex cannot be connected to <code>vertex</code> in the requested manner\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex) throws JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, null, null, -1, -1, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, null, null, -1, -1, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -145,7 +145,7 @@\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex, final int indexOfOutputGate, final int indexOfInputGate)\n \t\t\tthrows JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, null, null, indexOfOutputGate, indexOfInputGate, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, null, null, indexOfOutputGate, indexOfInputGate, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -162,7 +162,7 @@\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex, final ChannelType channelType,\n \t\t\tfinal CompressionLevel compressionLevel) throws JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -182,7 +182,7 @@\n \tpublic void connectTo(final AbstractJobVertex vertex, final ChannelType channelType,\n \t\t\tfinal CompressionLevel compressionLevel, final DistributionPattern distributionPattern)\n \t\t\tthrows JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, distributionPattern, false);\n+\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, distributionPattern, true);\n \t}\n \n \t/**\n@@ -234,7 +234,7 @@\n \t\t\tfinal CompressionLevel compressionLevel, int indexOfOutputGate, int indexOfInputGate,\n \t\t\tfinal DistributionPattern distributionPattern) throws JobGraphDefinitionException {\n \t\tthis.connectTo(vertex, channelType, compressionLevel, indexOfOutputGate, indexOfInputGate, distributionPattern,\n-\t\t\tfalse);\n+\t\t\ttrue);\n \t}\n \n \t/**\n",
    "projectName": "apache.flink",
    "bugLineNum": 165,
    "bugNodeStartChar": 5670,
    "bugNodeLength": 99,
    "fixLineNum": 165,
    "fixNodeStartChar": 5670,
    "fixNodeLength": 98,
    "sourceBeforeFix": "this.connectTo(vertex,channelType,compressionLevel,-1,-1,DistributionPattern.BIPARTITE,false)",
    "sourceAfterFix": "this.connectTo(vertex,channelType,compressionLevel,-1,-1,DistributionPattern.BIPARTITE,true)"
  },
  {
    "bugType": "SWAP_BOOLEAN_LITERAL",
    "fixCommitSHA1": "39aa03261a00c409f29341c286febaec99468d65",
    "fixCommitParentSHA1": "2e7c738cd17220d5e72e6002eda686f9be7f9bfb",
    "bugFilePath": "nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java",
    "fixPatch": "diff --git a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\nindex e9f4a94..295b9f4 100644\n--- a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\n+++ b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\n@@ -126,7 +126,7 @@\n \t *         thrown if the given vertex cannot be connected to <code>vertex</code> in the requested manner\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex) throws JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, null, null, -1, -1, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, null, null, -1, -1, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -145,7 +145,7 @@\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex, final int indexOfOutputGate, final int indexOfInputGate)\n \t\t\tthrows JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, null, null, indexOfOutputGate, indexOfInputGate, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, null, null, indexOfOutputGate, indexOfInputGate, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -162,7 +162,7 @@\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex, final ChannelType channelType,\n \t\t\tfinal CompressionLevel compressionLevel) throws JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -182,7 +182,7 @@\n \tpublic void connectTo(final AbstractJobVertex vertex, final ChannelType channelType,\n \t\t\tfinal CompressionLevel compressionLevel, final DistributionPattern distributionPattern)\n \t\t\tthrows JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, distributionPattern, false);\n+\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, distributionPattern, true);\n \t}\n \n \t/**\n@@ -234,7 +234,7 @@\n \t\t\tfinal CompressionLevel compressionLevel, int indexOfOutputGate, int indexOfInputGate,\n \t\t\tfinal DistributionPattern distributionPattern) throws JobGraphDefinitionException {\n \t\tthis.connectTo(vertex, channelType, compressionLevel, indexOfOutputGate, indexOfInputGate, distributionPattern,\n-\t\t\tfalse);\n+\t\t\ttrue);\n \t}\n \n \t/**\n",
    "projectName": "apache.flink",
    "bugLineNum": 185,
    "bugNodeStartChar": 6637,
    "bugNodeLength": 89,
    "fixLineNum": 185,
    "fixNodeStartChar": 6637,
    "fixNodeLength": 88,
    "sourceBeforeFix": "this.connectTo(vertex,channelType,compressionLevel,-1,-1,distributionPattern,false)",
    "sourceAfterFix": "this.connectTo(vertex,channelType,compressionLevel,-1,-1,distributionPattern,true)"
  },
  {
    "bugType": "SWAP_BOOLEAN_LITERAL",
    "fixCommitSHA1": "39aa03261a00c409f29341c286febaec99468d65",
    "fixCommitParentSHA1": "2e7c738cd17220d5e72e6002eda686f9be7f9bfb",
    "bugFilePath": "nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java",
    "fixPatch": "diff --git a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\nindex e9f4a94..295b9f4 100644\n--- a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\n+++ b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/AbstractJobVertex.java\n@@ -126,7 +126,7 @@\n \t *         thrown if the given vertex cannot be connected to <code>vertex</code> in the requested manner\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex) throws JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, null, null, -1, -1, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, null, null, -1, -1, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -145,7 +145,7 @@\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex, final int indexOfOutputGate, final int indexOfInputGate)\n \t\t\tthrows JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, null, null, indexOfOutputGate, indexOfInputGate, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, null, null, indexOfOutputGate, indexOfInputGate, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -162,7 +162,7 @@\n \t */\n \tpublic void connectTo(final AbstractJobVertex vertex, final ChannelType channelType,\n \t\t\tfinal CompressionLevel compressionLevel) throws JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, DistributionPattern.BIPARTITE, false);\n+\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, DistributionPattern.BIPARTITE, true);\n \t}\n \n \t/**\n@@ -182,7 +182,7 @@\n \tpublic void connectTo(final AbstractJobVertex vertex, final ChannelType channelType,\n \t\t\tfinal CompressionLevel compressionLevel, final DistributionPattern distributionPattern)\n \t\t\tthrows JobGraphDefinitionException {\n-\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, distributionPattern, false);\n+\t\tthis.connectTo(vertex, channelType, compressionLevel, -1, -1, distributionPattern, true);\n \t}\n \n \t/**\n@@ -234,7 +234,7 @@\n \t\t\tfinal CompressionLevel compressionLevel, int indexOfOutputGate, int indexOfInputGate,\n \t\t\tfinal DistributionPattern distributionPattern) throws JobGraphDefinitionException {\n \t\tthis.connectTo(vertex, channelType, compressionLevel, indexOfOutputGate, indexOfInputGate, distributionPattern,\n-\t\t\tfalse);\n+\t\t\ttrue);\n \t}\n \n \t/**\n",
    "projectName": "apache.flink",
    "bugLineNum": 236,
    "bugNodeStartChar": 9389,
    "bugNodeLength": 121,
    "fixLineNum": 236,
    "fixNodeStartChar": 9389,
    "fixNodeLength": 120,
    "sourceBeforeFix": "this.connectTo(vertex,channelType,compressionLevel,indexOfOutputGate,indexOfInputGate,distributionPattern,false)",
    "sourceAfterFix": "this.connectTo(vertex,channelType,compressionLevel,indexOfOutputGate,indexOfInputGate,distributionPattern,true)"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "92e7da30dab93cf23bcf64084f660426a0d9da85",
    "fixCommitParentSHA1": "e3d3b14391b2bd7742962835ea52b556137d421f",
    "bugFilePath": "nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobGraph.java",
    "fixPatch": "diff --git a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobGraph.java b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobGraph.java\nindex b206e52..16749b6 100644\n--- a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobGraph.java\n+++ b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobGraph.java\n@@ -770,7 +770,7 @@\n \tpublic void read(final Kryo kryo, final Input input) {\n \n \t\t// Read job id\n-\t\tthis.jobID = kryo.readObjectOrNull(input, JobID.class);\n+\t\tthis.jobID = kryo.readObject(input, JobID.class);\n \n \t\t// Read the job name\n \t\tthis.jobName = input.readString();\n",
    "projectName": "apache.flink",
    "bugLineNum": 773,
    "bugNodeStartChar": 21624,
    "bugNodeLength": 41,
    "fixLineNum": 773,
    "fixNodeStartChar": 21624,
    "fixNodeLength": 35,
    "sourceBeforeFix": "kryo.readObjectOrNull(input,JobID.class)",
    "sourceAfterFix": "kryo.readObject(input,JobID.class)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "92e7da30dab93cf23bcf64084f660426a0d9da85",
    "fixCommitParentSHA1": "e3d3b14391b2bd7742962835ea52b556137d421f",
    "bugFilePath": "nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobGraph.java",
    "fixPatch": "diff --git a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobGraph.java b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobGraph.java\nindex b206e52..16749b6 100644\n--- a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobGraph.java\n+++ b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobGraph.java\n@@ -770,7 +770,7 @@\n \tpublic void read(final Kryo kryo, final Input input) {\n \n \t\t// Read job id\n-\t\tthis.jobID = kryo.readObjectOrNull(input, JobID.class);\n+\t\tthis.jobID = kryo.readObject(input, JobID.class);\n \n \t\t// Read the job name\n \t\tthis.jobName = input.readString();\n",
    "projectName": "apache.flink",
    "bugLineNum": 773,
    "bugNodeStartChar": 21624,
    "bugNodeLength": 41,
    "fixLineNum": 773,
    "fixNodeStartChar": 21624,
    "fixNodeLength": 35,
    "sourceBeforeFix": "kryo.readObjectOrNull(input,JobID.class)",
    "sourceAfterFix": "kryo.readObject(input,JobID.class)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "c7f9129cf03d4e6e9089ba518a74efaf6bcaac68",
    "fixCommitParentSHA1": "050930350278b27ab84cb6a79ed78f27adaf7b0d",
    "bugFilePath": "sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/type/ArrayNode.java",
    "fixPatch": "diff --git a/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/type/ArrayNode.java b/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/type/ArrayNode.java\nindex 2402709..47f2dd7 100644\n--- a/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/type/ArrayNode.java\n+++ b/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/type/ArrayNode.java\n@@ -49,12 +49,12 @@\n \t}\n \n \t/**\n-\t * Initializes an ArrayNode which cointains all {@link IJsonNode}s from the given Collection in proper sequence.\n+\t * Initializes an ArrayNode which cointains all {@link IJsonNode}s from the given Iterable in proper sequence.\n \t * \n \t * @param nodes\n \t *        a Collection of nodes that should be added to this ArrayNode\n \t */\n-\tpublic ArrayNode(final Collection<? extends IJsonNode> nodes) {\n+\tpublic ArrayNode(final Iterable<? extends IJsonNode> nodes) {\n \t\tthis();\n \t\tfor (final IJsonNode node : nodes)\n \t\t\tthis.children.add(node);\n",
    "projectName": "apache.flink",
    "bugLineNum": 57,
    "bugNodeStartChar": 1390,
    "bugNodeLength": 31,
    "fixLineNum": 57,
    "fixNodeStartChar": 1390,
    "fixNodeLength": 29,
    "sourceBeforeFix": "Collection<? extends IJsonNode>",
    "sourceAfterFix": "Iterable<? extends IJsonNode>"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "358697a54dd0ac6743f84084a8138f1c8d7f2a21",
    "fixCommitParentSHA1": "8bc461c60a8fb8018d89bf359dd78f57fbb2de35",
    "bugFilePath": "pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/sort/AsynchronousPartialSorter.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/sort/AsynchronousPartialSorter.java b/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/sort/AsynchronousPartialSorter.java\nindex ded99de..138976d 100644\n--- a/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/sort/AsynchronousPartialSorter.java\n+++ b/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/sort/AsynchronousPartialSorter.java\n@@ -39,7 +39,7 @@\n  */\n public class AsynchronousPartialSorter<E> extends UnilateralSortMerger<E>\n {\n-\tprivate static final int MAX_MEM_PER_PARTIAL_SORT = 64 * 1024 * 0124;\n+\tprivate static final int MAX_MEM_PER_PARTIAL_SORT = 512 * 1024 * 1024;\n \t\n \tprivate BufferQueueIterator bufferIterator;\n \t\n",
    "projectName": "apache.flink",
    "bugLineNum": 42,
    "bugNodeStartChar": 1976,
    "bugNodeLength": 16,
    "fixLineNum": 42,
    "fixNodeStartChar": 1976,
    "fixNodeLength": 17,
    "sourceBeforeFix": "64 * 1024 * 0124",
    "sourceAfterFix": "512 * 1024 * 1024"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "358697a54dd0ac6743f84084a8138f1c8d7f2a21",
    "fixCommitParentSHA1": "8bc461c60a8fb8018d89bf359dd78f57fbb2de35",
    "bugFilePath": "pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\nindex cafd24b..ec2fd60 100644\n--- a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\n+++ b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\n@@ -134,7 +134,7 @@\n \t\t\t// merge iterator\n \t\t\tLOG.debug(\"Initializing sortmerger...\");\n \t\t\tSorter<PactRecord> sorter = new AsynchronousPartialSorter<PactRecord>(this.memoryManager, source,\n-\t\t\t\tthis.parentTask, this.serializer, this.comparator, 32 * 1024 * 1024);\n+\t\t\t\tthis.parentTask, this.serializer, this.comparator, 10 * 1024 * 1024);\n \t\n \t\t\trunPartialSorter(sorter, NUM_RECORDS, 2);\n \t\t}\n@@ -157,9 +157,9 @@\n \t\t\t// merge iterator\n \t\t\tLOG.debug(\"Initializing sortmerger...\");\n \t\t\tSorter<PactRecord> sorter = new AsynchronousPartialSorter<PactRecord>(this.memoryManager, source,\n-\t\t\t\tthis.parentTask, this.serializer, this.comparator, 32 * 1024 * 1024);\n+\t\t\t\tthis.parentTask, this.serializer, this.comparator, 10 * 1024 * 1024);\n \t\n-\t\t\trunPartialSorter(sorter, NUM_RECORDS, 28);\n+\t\t\trunPartialSorter(sorter, NUM_RECORDS, 25);\n \t\t}\n \t\tcatch (Exception t) {\n \t\t\tt.printStackTrace();\n",
    "projectName": "apache.flink",
    "bugLineNum": 137,
    "bugNodeStartChar": 5193,
    "bugNodeLength": 16,
    "fixLineNum": 137,
    "fixNodeStartChar": 5193,
    "fixNodeLength": 16,
    "sourceBeforeFix": "32 * 1024 * 1024",
    "sourceAfterFix": "10 * 1024 * 1024"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "358697a54dd0ac6743f84084a8138f1c8d7f2a21",
    "fixCommitParentSHA1": "8bc461c60a8fb8018d89bf359dd78f57fbb2de35",
    "bugFilePath": "pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\nindex cafd24b..ec2fd60 100644\n--- a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\n+++ b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\n@@ -134,7 +134,7 @@\n \t\t\t// merge iterator\n \t\t\tLOG.debug(\"Initializing sortmerger...\");\n \t\t\tSorter<PactRecord> sorter = new AsynchronousPartialSorter<PactRecord>(this.memoryManager, source,\n-\t\t\t\tthis.parentTask, this.serializer, this.comparator, 32 * 1024 * 1024);\n+\t\t\t\tthis.parentTask, this.serializer, this.comparator, 10 * 1024 * 1024);\n \t\n \t\t\trunPartialSorter(sorter, NUM_RECORDS, 2);\n \t\t}\n@@ -157,9 +157,9 @@\n \t\t\t// merge iterator\n \t\t\tLOG.debug(\"Initializing sortmerger...\");\n \t\t\tSorter<PactRecord> sorter = new AsynchronousPartialSorter<PactRecord>(this.memoryManager, source,\n-\t\t\t\tthis.parentTask, this.serializer, this.comparator, 32 * 1024 * 1024);\n+\t\t\t\tthis.parentTask, this.serializer, this.comparator, 10 * 1024 * 1024);\n \t\n-\t\t\trunPartialSorter(sorter, NUM_RECORDS, 28);\n+\t\t\trunPartialSorter(sorter, NUM_RECORDS, 25);\n \t\t}\n \t\tcatch (Exception t) {\n \t\t\tt.printStackTrace();\n",
    "projectName": "apache.flink",
    "bugLineNum": 160,
    "bugNodeStartChar": 6004,
    "bugNodeLength": 16,
    "fixLineNum": 160,
    "fixNodeStartChar": 6004,
    "fixNodeLength": 16,
    "sourceBeforeFix": "32 * 1024 * 1024",
    "sourceAfterFix": "10 * 1024 * 1024"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "358697a54dd0ac6743f84084a8138f1c8d7f2a21",
    "fixCommitParentSHA1": "8bc461c60a8fb8018d89bf359dd78f57fbb2de35",
    "bugFilePath": "pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\nindex cafd24b..ec2fd60 100644\n--- a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\n+++ b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\n@@ -134,7 +134,7 @@\n \t\t\t// merge iterator\n \t\t\tLOG.debug(\"Initializing sortmerger...\");\n \t\t\tSorter<PactRecord> sorter = new AsynchronousPartialSorter<PactRecord>(this.memoryManager, source,\n-\t\t\t\tthis.parentTask, this.serializer, this.comparator, 32 * 1024 * 1024);\n+\t\t\t\tthis.parentTask, this.serializer, this.comparator, 10 * 1024 * 1024);\n \t\n \t\t\trunPartialSorter(sorter, NUM_RECORDS, 2);\n \t\t}\n@@ -157,9 +157,9 @@\n \t\t\t// merge iterator\n \t\t\tLOG.debug(\"Initializing sortmerger...\");\n \t\t\tSorter<PactRecord> sorter = new AsynchronousPartialSorter<PactRecord>(this.memoryManager, source,\n-\t\t\t\tthis.parentTask, this.serializer, this.comparator, 32 * 1024 * 1024);\n+\t\t\t\tthis.parentTask, this.serializer, this.comparator, 10 * 1024 * 1024);\n \t\n-\t\t\trunPartialSorter(sorter, NUM_RECORDS, 28);\n+\t\t\trunPartialSorter(sorter, NUM_RECORDS, 25);\n \t\t}\n \t\tcatch (Exception t) {\n \t\t\tt.printStackTrace();\n",
    "projectName": "apache.flink",
    "bugLineNum": 162,
    "bugNodeStartChar": 6028,
    "bugNodeLength": 41,
    "fixLineNum": 162,
    "fixNodeStartChar": 6028,
    "fixNodeLength": 41,
    "sourceBeforeFix": "runPartialSorter(sorter,NUM_RECORDS,28)",
    "sourceAfterFix": "runPartialSorter(sorter,NUM_RECORDS,25)"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "e9fbd7f493863da1e68776473d8b5d03861daa6f",
    "fixCommitParentSHA1": "1fae50db236e80cfc289c339706ccc34f7c4ff37",
    "bugFilePath": "pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/sort/AsynchronousPartialSorter.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/sort/AsynchronousPartialSorter.java b/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/sort/AsynchronousPartialSorter.java\nindex ded99de..138976d 100644\n--- a/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/sort/AsynchronousPartialSorter.java\n+++ b/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/sort/AsynchronousPartialSorter.java\n@@ -39,7 +39,7 @@\n  */\n public class AsynchronousPartialSorter<E> extends UnilateralSortMerger<E>\n {\n-\tprivate static final int MAX_MEM_PER_PARTIAL_SORT = 64 * 1024 * 0124;\n+\tprivate static final int MAX_MEM_PER_PARTIAL_SORT = 512 * 1024 * 1024;\n \t\n \tprivate BufferQueueIterator bufferIterator;\n \t\n",
    "projectName": "apache.flink",
    "bugLineNum": 42,
    "bugNodeStartChar": 1976,
    "bugNodeLength": 16,
    "fixLineNum": 42,
    "fixNodeStartChar": 1976,
    "fixNodeLength": 17,
    "sourceBeforeFix": "64 * 1024 * 0124",
    "sourceAfterFix": "512 * 1024 * 1024"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "e9fbd7f493863da1e68776473d8b5d03861daa6f",
    "fixCommitParentSHA1": "1fae50db236e80cfc289c339706ccc34f7c4ff37",
    "bugFilePath": "pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\nindex cafd24b..ec2fd60 100644\n--- a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\n+++ b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\n@@ -134,7 +134,7 @@\n \t\t\t// merge iterator\n \t\t\tLOG.debug(\"Initializing sortmerger...\");\n \t\t\tSorter<PactRecord> sorter = new AsynchronousPartialSorter<PactRecord>(this.memoryManager, source,\n-\t\t\t\tthis.parentTask, this.serializer, this.comparator, 32 * 1024 * 1024);\n+\t\t\t\tthis.parentTask, this.serializer, this.comparator, 10 * 1024 * 1024);\n \t\n \t\t\trunPartialSorter(sorter, NUM_RECORDS, 2);\n \t\t}\n@@ -157,9 +157,9 @@\n \t\t\t// merge iterator\n \t\t\tLOG.debug(\"Initializing sortmerger...\");\n \t\t\tSorter<PactRecord> sorter = new AsynchronousPartialSorter<PactRecord>(this.memoryManager, source,\n-\t\t\t\tthis.parentTask, this.serializer, this.comparator, 32 * 1024 * 1024);\n+\t\t\t\tthis.parentTask, this.serializer, this.comparator, 10 * 1024 * 1024);\n \t\n-\t\t\trunPartialSorter(sorter, NUM_RECORDS, 28);\n+\t\t\trunPartialSorter(sorter, NUM_RECORDS, 25);\n \t\t}\n \t\tcatch (Exception t) {\n \t\t\tt.printStackTrace();\n",
    "projectName": "apache.flink",
    "bugLineNum": 137,
    "bugNodeStartChar": 5193,
    "bugNodeLength": 16,
    "fixLineNum": 137,
    "fixNodeStartChar": 5193,
    "fixNodeLength": 16,
    "sourceBeforeFix": "32 * 1024 * 1024",
    "sourceAfterFix": "10 * 1024 * 1024"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "e9fbd7f493863da1e68776473d8b5d03861daa6f",
    "fixCommitParentSHA1": "1fae50db236e80cfc289c339706ccc34f7c4ff37",
    "bugFilePath": "pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\nindex cafd24b..ec2fd60 100644\n--- a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\n+++ b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\n@@ -134,7 +134,7 @@\n \t\t\t// merge iterator\n \t\t\tLOG.debug(\"Initializing sortmerger...\");\n \t\t\tSorter<PactRecord> sorter = new AsynchronousPartialSorter<PactRecord>(this.memoryManager, source,\n-\t\t\t\tthis.parentTask, this.serializer, this.comparator, 32 * 1024 * 1024);\n+\t\t\t\tthis.parentTask, this.serializer, this.comparator, 10 * 1024 * 1024);\n \t\n \t\t\trunPartialSorter(sorter, NUM_RECORDS, 2);\n \t\t}\n@@ -157,9 +157,9 @@\n \t\t\t// merge iterator\n \t\t\tLOG.debug(\"Initializing sortmerger...\");\n \t\t\tSorter<PactRecord> sorter = new AsynchronousPartialSorter<PactRecord>(this.memoryManager, source,\n-\t\t\t\tthis.parentTask, this.serializer, this.comparator, 32 * 1024 * 1024);\n+\t\t\t\tthis.parentTask, this.serializer, this.comparator, 10 * 1024 * 1024);\n \t\n-\t\t\trunPartialSorter(sorter, NUM_RECORDS, 28);\n+\t\t\trunPartialSorter(sorter, NUM_RECORDS, 25);\n \t\t}\n \t\tcatch (Exception t) {\n \t\t\tt.printStackTrace();\n",
    "projectName": "apache.flink",
    "bugLineNum": 160,
    "bugNodeStartChar": 6004,
    "bugNodeLength": 16,
    "fixLineNum": 160,
    "fixNodeStartChar": 6004,
    "fixNodeLength": 16,
    "sourceBeforeFix": "32 * 1024 * 1024",
    "sourceAfterFix": "10 * 1024 * 1024"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "e9fbd7f493863da1e68776473d8b5d03861daa6f",
    "fixCommitParentSHA1": "1fae50db236e80cfc289c339706ccc34f7c4ff37",
    "bugFilePath": "pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\nindex cafd24b..ec2fd60 100644\n--- a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\n+++ b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/AsynchonousPartialSorterITCase.java\n@@ -134,7 +134,7 @@\n \t\t\t// merge iterator\n \t\t\tLOG.debug(\"Initializing sortmerger...\");\n \t\t\tSorter<PactRecord> sorter = new AsynchronousPartialSorter<PactRecord>(this.memoryManager, source,\n-\t\t\t\tthis.parentTask, this.serializer, this.comparator, 32 * 1024 * 1024);\n+\t\t\t\tthis.parentTask, this.serializer, this.comparator, 10 * 1024 * 1024);\n \t\n \t\t\trunPartialSorter(sorter, NUM_RECORDS, 2);\n \t\t}\n@@ -157,9 +157,9 @@\n \t\t\t// merge iterator\n \t\t\tLOG.debug(\"Initializing sortmerger...\");\n \t\t\tSorter<PactRecord> sorter = new AsynchronousPartialSorter<PactRecord>(this.memoryManager, source,\n-\t\t\t\tthis.parentTask, this.serializer, this.comparator, 32 * 1024 * 1024);\n+\t\t\t\tthis.parentTask, this.serializer, this.comparator, 10 * 1024 * 1024);\n \t\n-\t\t\trunPartialSorter(sorter, NUM_RECORDS, 28);\n+\t\t\trunPartialSorter(sorter, NUM_RECORDS, 25);\n \t\t}\n \t\tcatch (Exception t) {\n \t\t\tt.printStackTrace();\n",
    "projectName": "apache.flink",
    "bugLineNum": 162,
    "bugNodeStartChar": 6028,
    "bugNodeLength": 41,
    "fixLineNum": 162,
    "fixNodeStartChar": 6028,
    "fixNodeLength": 41,
    "sourceBeforeFix": "runPartialSorter(sorter,NUM_RECORDS,28)",
    "sourceAfterFix": "runPartialSorter(sorter,NUM_RECORDS,25)"
  },
  {
    "bugType": "CHANGE_OPERATOR",
    "fixCommitSHA1": "144794ec6cbefe37f4e10a94691885643d86a2aa",
    "fixCommitParentSHA1": "51157116981fc3ab0a0cc27c76c99a0d52199d94",
    "bugFilePath": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/executiongraph/ExecutionVertex.java",
    "fixPatch": "diff --git a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/executiongraph/ExecutionVertex.java b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/executiongraph/ExecutionVertex.java\nindex fa02788..5beea18 100644\n--- a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/executiongraph/ExecutionVertex.java\n+++ b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/executiongraph/ExecutionVertex.java\n@@ -408,7 +408,7 @@\n \t\tif (this.cancelRequested.compareAndSet(true, false)) {\n \t\t\tfinal TaskCancelResult tsr = cancelTask();\n \t\t\tif (tsr.getReturnCode() != AbstractTaskResult.ReturnCode.SUCCESS\n-\t\t\t\t|| tsr.getReturnCode() != AbstractTaskResult.ReturnCode.TASK_NOT_FOUND) {\n+\t\t\t\t&& tsr.getReturnCode() != AbstractTaskResult.ReturnCode.TASK_NOT_FOUND) {\n \t\t\t\tLOG.error(\"Unable to cancel vertex \" + this + \": \" + tsr.getReturnCode().toString()\n \t\t\t\t\t+ ((tsr.getDescription() != null) ? (\" (\" + tsr.getDescription() + \")\") : \"\"));\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 410,
    "bugNodeStartChar": 13825,
    "bugNodeLength": 135,
    "fixLineNum": 410,
    "fixNodeStartChar": 13825,
    "fixNodeLength": 135,
    "sourceBeforeFix": "tsr.getReturnCode() != AbstractTaskResult.ReturnCode.SUCCESS || tsr.getReturnCode() != AbstractTaskResult.ReturnCode.TASK_NOT_FOUND",
    "sourceAfterFix": "tsr.getReturnCode() != AbstractTaskResult.ReturnCode.SUCCESS && tsr.getReturnCode() != AbstractTaskResult.ReturnCode.TASK_NOT_FOUND"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "69c1fc12079616a75ac51f90aed6dbfbefd586a2",
    "fixCommitParentSHA1": "a12b62833fe7144390bbc052efbe43d524c683c7",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\nindex 870f1b9..fb2aaef 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\n@@ -230,7 +230,7 @@\n \t{\n \t\t// range check\n \t\tif (fieldNum < 0 || fieldNum >= this.numFields) {\n-\t\t\tthrow new IndexOutOfBoundsException();\n+\t\t\tthrow new IndexOutOfBoundsException(fieldNum+\" for range [0..\"+(this.numFields-1)+\"]\");\n \t\t}\n \t\t\n \t\t// get offset and check for null\n",
    "projectName": "apache.flink",
    "bugLineNum": 233,
    "bugNodeStartChar": 8827,
    "bugNodeLength": 31,
    "fixLineNum": 233,
    "fixNodeStartChar": 8827,
    "fixNodeLength": 80,
    "sourceBeforeFix": "new IndexOutOfBoundsException()",
    "sourceAfterFix": "new IndexOutOfBoundsException(fieldNum + \" for range [0..\" + (this.numFields - 1)+ \"]\")"
  },
  {
    "bugType": "LESS_SPECIFIC_IF",
    "fixCommitSHA1": "69c1fc12079616a75ac51f90aed6dbfbefd586a2",
    "fixCommitParentSHA1": "a12b62833fe7144390bbc052efbe43d524c683c7",
    "bugFilePath": "sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/serialization/ObjectSchema.java",
    "fixPatch": "diff --git a/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/serialization/ObjectSchema.java b/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/serialization/ObjectSchema.java\nindex fe15968..581bf83 100644\n--- a/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/serialization/ObjectSchema.java\n+++ b/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/serialization/ObjectSchema.java\n@@ -94,7 +94,7 @@\n \t@Override\n \tpublic PactRecord jsonToRecord(IJsonNode value, PactRecord target, EvaluationContext context) {\n \t\tIObjectNode others;\n-\t\tif (target == null) {\n+\t\tif (target == null || target.getNumFields() < this.mappings.size() + 1) {\n \t\t\t// the last element is the field \"others\"\n \t\t\ttarget = new PactRecord(this.mappings.size() + 1);\n \t\t\tfor (int i = 0; i < this.mappings.size(); i++)\n",
    "projectName": "apache.flink",
    "bugLineNum": 97,
    "bugNodeStartChar": 2560,
    "bugNodeLength": 14,
    "fixLineNum": 97,
    "fixNodeStartChar": 2560,
    "fixNodeLength": 66,
    "sourceBeforeFix": "target == null",
    "sourceAfterFix": "target == null || target.getNumFields() < this.mappings.size() + 1"
  },
  {
    "bugType": "LESS_SPECIFIC_IF",
    "fixCommitSHA1": "69c1fc12079616a75ac51f90aed6dbfbefd586a2",
    "fixCommitParentSHA1": "a12b62833fe7144390bbc052efbe43d524c683c7",
    "bugFilePath": "sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/serialization/ObjectSchema.java",
    "fixPatch": "diff --git a/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/serialization/ObjectSchema.java b/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/serialization/ObjectSchema.java\nindex fe15968..581bf83 100644\n--- a/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/serialization/ObjectSchema.java\n+++ b/sopremo/sopremo-common/src/main/java/eu/stratosphere/sopremo/serialization/ObjectSchema.java\n@@ -94,7 +94,7 @@\n \t@Override\n \tpublic PactRecord jsonToRecord(IJsonNode value, PactRecord target, EvaluationContext context) {\n \t\tIObjectNode others;\n-\t\tif (target == null) {\n+\t\tif (target == null || target.getNumFields() < this.mappings.size() + 1) {\n \t\t\t// the last element is the field \"others\"\n \t\t\ttarget = new PactRecord(this.mappings.size() + 1);\n \t\t\tfor (int i = 0; i < this.mappings.size(); i++)\n",
    "projectName": "apache.flink",
    "bugLineNum": 97,
    "bugNodeStartChar": 2560,
    "bugNodeLength": 14,
    "fixLineNum": 97,
    "fixNodeStartChar": 2560,
    "fixNodeLength": 66,
    "sourceBeforeFix": "target == null",
    "sourceAfterFix": "target == null || target.getNumFields() < this.mappings.size() + 1"
  },
  {
    "bugType": "DIFFERENT_METHOD_SAME_ARGS",
    "fixCommitSHA1": "ab3d6aa65220d2e024420b850bcc3078ef1cac15",
    "fixCommitParentSHA1": "bbcab8a8328b8d62d24a0ca58a4f425141241266",
    "bugFilePath": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/RemoteReceiver.java",
    "fixPatch": "diff --git a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/RemoteReceiver.java b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/RemoteReceiver.java\nindex 71dce5b..b9b6f43 100644\n--- a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/RemoteReceiver.java\n+++ b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/RemoteReceiver.java\n@@ -132,7 +132,7 @@\n \t\tfinal InetAddress ia = this.connectionAddress.getAddress();\n \t\tout.writeInt(ia.getAddress().length);\n \t\tout.write(ia.getAddress());\n-\t\tout.write(this.connectionAddress.getPort());\n+\t\tout.writeInt(this.connectionAddress.getPort());\n \n \t\tout.writeInt(this.connectionIndex);\n \t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 135,
    "bugNodeStartChar": 3765,
    "bugNodeLength": 43,
    "fixLineNum": 135,
    "fixNodeStartChar": 3765,
    "fixNodeLength": 46,
    "sourceBeforeFix": "out.write(this.connectionAddress.getPort())",
    "sourceAfterFix": "out.writeInt(this.connectionAddress.getPort())"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "ab3d6aa65220d2e024420b850bcc3078ef1cac15",
    "fixCommitParentSHA1": "bbcab8a8328b8d62d24a0ca58a4f425141241266",
    "bugFilePath": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/RemoteReceiver.java",
    "fixPatch": "diff --git a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/RemoteReceiver.java b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/RemoteReceiver.java\nindex 71dce5b..b9b6f43 100644\n--- a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/RemoteReceiver.java\n+++ b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/RemoteReceiver.java\n@@ -132,7 +132,7 @@\n \t\tfinal InetAddress ia = this.connectionAddress.getAddress();\n \t\tout.writeInt(ia.getAddress().length);\n \t\tout.write(ia.getAddress());\n-\t\tout.write(this.connectionAddress.getPort());\n+\t\tout.writeInt(this.connectionAddress.getPort());\n \n \t\tout.writeInt(this.connectionIndex);\n \t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 135,
    "bugNodeStartChar": 3765,
    "bugNodeLength": 43,
    "fixLineNum": 135,
    "fixNodeStartChar": 3765,
    "fixNodeLength": 46,
    "sourceBeforeFix": "out.write(this.connectionAddress.getPort())",
    "sourceAfterFix": "out.writeInt(this.connectionAddress.getPort())"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "9f9bf2d5097433860725e5c7422e9c9e269a691b",
    "fixCommitParentSHA1": "b1c4c7e512cd673f7a66dc2db57926fb8345a0ec",
    "bugFilePath": "sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java",
    "fixPatch": "diff --git a/sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java b/sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java\nindex b1aaf61..2dab078 100644\n--- a/sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java\n+++ b/sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java\n@@ -12,16 +12,16 @@\n import eu.stratosphere.sopremo.type.IArrayNode;\n import eu.stratosphere.sopremo.type.IntNode;\n \n-public class LazyHeadArrayNodeTest extends ArrayNodeBaseTest<LazyTailArrayNode> {\n+public class LazyHeadArrayNodeTest extends ArrayNodeBaseTest<LazyHeadArrayNode> {\n \n \t@Override\n \tpublic void initArrayNode() {\n-\t\tTailArraySchema schema = new TailArraySchema();\n+\t\tHeadArraySchema schema = new HeadArraySchema();\n \t\tschema.setHeadSize(5);\n \t\tPactRecord record = schema.jsonToRecord(\n \t\t\tnew ArrayNode(IntNode.valueOf(0), IntNode.valueOf(1), IntNode.valueOf(2)), null, null);\n \n-\t\tthis.node = new LazyTailArrayNode(record, schema);\n+\t\tthis.node = new LazyHeadArrayNode(record, schema);\n \t}\n \n \t@Test\n",
    "projectName": "apache.flink",
    "bugLineNum": 15,
    "bugNodeStartChar": 497,
    "bugNodeLength": 36,
    "fixLineNum": 15,
    "fixNodeStartChar": 497,
    "fixNodeLength": 36,
    "sourceBeforeFix": "ArrayNodeBaseTest<LazyTailArrayNode>",
    "sourceAfterFix": "ArrayNodeBaseTest<LazyHeadArrayNode>"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "9f9bf2d5097433860725e5c7422e9c9e269a691b",
    "fixCommitParentSHA1": "b1c4c7e512cd673f7a66dc2db57926fb8345a0ec",
    "bugFilePath": "sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java",
    "fixPatch": "diff --git a/sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java b/sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java\nindex b1aaf61..2dab078 100644\n--- a/sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java\n+++ b/sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java\n@@ -12,16 +12,16 @@\n import eu.stratosphere.sopremo.type.IArrayNode;\n import eu.stratosphere.sopremo.type.IntNode;\n \n-public class LazyHeadArrayNodeTest extends ArrayNodeBaseTest<LazyTailArrayNode> {\n+public class LazyHeadArrayNodeTest extends ArrayNodeBaseTest<LazyHeadArrayNode> {\n \n \t@Override\n \tpublic void initArrayNode() {\n-\t\tTailArraySchema schema = new TailArraySchema();\n+\t\tHeadArraySchema schema = new HeadArraySchema();\n \t\tschema.setHeadSize(5);\n \t\tPactRecord record = schema.jsonToRecord(\n \t\t\tnew ArrayNode(IntNode.valueOf(0), IntNode.valueOf(1), IntNode.valueOf(2)), null, null);\n \n-\t\tthis.node = new LazyTailArrayNode(record, schema);\n+\t\tthis.node = new LazyHeadArrayNode(record, schema);\n \t}\n \n \t@Test\n",
    "projectName": "apache.flink",
    "bugLineNum": 19,
    "bugNodeStartChar": 581,
    "bugNodeLength": 47,
    "fixLineNum": 19,
    "fixNodeStartChar": 581,
    "fixNodeLength": 47,
    "sourceBeforeFix": "TailArraySchema schema=new TailArraySchema(); ",
    "sourceAfterFix": "HeadArraySchema schema=new HeadArraySchema(); "
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "9f9bf2d5097433860725e5c7422e9c9e269a691b",
    "fixCommitParentSHA1": "b1c4c7e512cd673f7a66dc2db57926fb8345a0ec",
    "bugFilePath": "sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java",
    "fixPatch": "diff --git a/sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java b/sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java\nindex b1aaf61..2dab078 100644\n--- a/sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java\n+++ b/sopremo/sopremo-common/src/test/java/eu/stratosphere/sopremo/serialization/LazyHeadArrayNodeTest.java\n@@ -12,16 +12,16 @@\n import eu.stratosphere.sopremo.type.IArrayNode;\n import eu.stratosphere.sopremo.type.IntNode;\n \n-public class LazyHeadArrayNodeTest extends ArrayNodeBaseTest<LazyTailArrayNode> {\n+public class LazyHeadArrayNodeTest extends ArrayNodeBaseTest<LazyHeadArrayNode> {\n \n \t@Override\n \tpublic void initArrayNode() {\n-\t\tTailArraySchema schema = new TailArraySchema();\n+\t\tHeadArraySchema schema = new HeadArraySchema();\n \t\tschema.setHeadSize(5);\n \t\tPactRecord record = schema.jsonToRecord(\n \t\t\tnew ArrayNode(IntNode.valueOf(0), IntNode.valueOf(1), IntNode.valueOf(2)), null, null);\n \n-\t\tthis.node = new LazyTailArrayNode(record, schema);\n+\t\tthis.node = new LazyHeadArrayNode(record, schema);\n \t}\n \n \t@Test\n",
    "projectName": "apache.flink",
    "bugLineNum": 24,
    "bugNodeStartChar": 803,
    "bugNodeLength": 37,
    "fixLineNum": 24,
    "fixNodeStartChar": 803,
    "fixNodeLength": 37,
    "sourceBeforeFix": "new LazyTailArrayNode(record,schema)",
    "sourceAfterFix": "new LazyHeadArrayNode(record,schema)"
  },
  {
    "bugType": "SWAP_BOOLEAN_LITERAL",
    "fixCommitSHA1": "2d8ae9afea66a45bf7f3dad61c06ee95d3ef9a46",
    "fixCommitParentSHA1": "858666deddb0eee9bf9d1c675b66b656a65cdac7",
    "bugFilePath": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/checkpointing/ReplayOutputChannelContext.java",
    "fixPatch": "diff --git a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/checkpointing/ReplayOutputChannelContext.java b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/checkpointing/ReplayOutputChannelContext.java\nindex da3b149..6634dc6 100644\n--- a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/checkpointing/ReplayOutputChannelContext.java\n+++ b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/checkpointing/ReplayOutputChannelContext.java\n@@ -31,7 +31,7 @@\n \t@Override\n \tpublic boolean isInputChannel() {\n \n-\t\treturn true;\n+\t\treturn false;\n \t}\n \n \t/**\n",
    "projectName": "apache.flink",
    "bugLineNum": 34,
    "bugNodeStartChar": 1151,
    "bugNodeLength": 12,
    "fixLineNum": 34,
    "fixNodeStartChar": 1151,
    "fixNodeLength": 13,
    "sourceBeforeFix": "return true; ",
    "sourceAfterFix": "return false; "
  },
  {
    "bugType": "SWAP_ARGUMENTS",
    "fixCommitSHA1": "21b9d7485fe3df950479157067bbd1da306592e0",
    "fixCommitParentSHA1": "2ec24808ef09ddf5108dc9cc8e88764e8e83acc3",
    "bugFilePath": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/OptimizerNode.java",
    "fixPatch": "diff --git a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/OptimizerNode.java b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/OptimizerNode.java\nindex 73bcc6d..f38866f 100644\n--- a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/OptimizerNode.java\n+++ b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/OptimizerNode.java\n@@ -1525,7 +1525,7 @@\n \t\n \tpublic boolean keepsUniqueProperty(FieldSet uniqueSet, int input) {\n \t\tfor (Integer uniqueField : uniqueSet) {\n-\t\t\tif (isFieldKept(uniqueField, input) == false) {\n+\t\t\tif (isFieldKept(input, uniqueField) == false) {\n \t\t\t\treturn false;\n \t\t\t}\n \t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 1528,
    "bugNodeStartChar": 50409,
    "bugNodeLength": 31,
    "fixLineNum": 1528,
    "fixNodeStartChar": 50409,
    "fixNodeLength": 31,
    "sourceBeforeFix": "isFieldKept(uniqueField,input)",
    "sourceAfterFix": "isFieldKept(input,uniqueField)"
  },
  {
    "bugType": "CHANGE_UNARY_OPERATOR",
    "fixCommitSHA1": "e1f8f60d8b6743cf1718ae626abf81a554e8354b",
    "fixCommitParentSHA1": "dc1cdb41f9ee89bc807b2a4d49b56d6d3e336d22",
    "bugFilePath": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/checkpointing/ReplayThread.java",
    "fixPatch": "diff --git a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/checkpointing/ReplayThread.java b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/checkpointing/ReplayThread.java\nindex 0f15fb2..0a5158c 100644\n--- a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/checkpointing/ReplayThread.java\n+++ b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/checkpointing/ReplayThread.java\n@@ -110,7 +110,7 @@\n \t\t\tfinal Iterator<ReplayOutputBroker> it = this.outputBrokerMap.values().iterator();\n \t\t\twhile (it.hasNext()) {\n \n-\t\t\t\tif (it.next().hasFinished()) {\n+\t\t\t\tif (!it.next().hasFinished()) {\n \t\t\t\t\tfinished = false;\n \t\t\t\t}\n \t\t\t}\n",
    "projectName": "apache.flink",
    "bugLineNum": 113,
    "bugNodeStartChar": 3465,
    "bugNodeLength": 23,
    "fixLineNum": 113,
    "fixNodeStartChar": 3465,
    "fixNodeLength": 24,
    "sourceBeforeFix": "it.next().hasFinished()",
    "sourceAfterFix": "!it.next().hasFinished()"
  },
  {
    "bugType": "MORE_SPECIFIC_IF",
    "fixCommitSHA1": "e68b9038b64e1b3ad8068541216baf17da23ddde",
    "fixCommitParentSHA1": "aa2fabcfbdb12cfcabdf651f647ec5d18af6696a",
    "bugFilePath": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/costs/CostEstimator.java",
    "fixPatch": "diff --git a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/costs/CostEstimator.java b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/costs/CostEstimator.java\nindex 7cd9e27..20cbdaf 100644\n--- a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/costs/CostEstimator.java\n+++ b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/costs/CostEstimator.java\n@@ -132,7 +132,7 @@\n \t\t}\n \n \t\t// if we have a second input, add its costs\n-\t\tif (secConn != null) {\n+\t\tif (secConn != null && secConn.size() > 0) {\n \t\t\tCosts secCost = new Costs();\n \n \t\t\t// we assume that all connections in the list have the same ship strategy;\n",
    "projectName": "apache.flink",
    "bugLineNum": 135,
    "bugNodeStartChar": 5417,
    "bugNodeLength": 15,
    "fixLineNum": 135,
    "fixNodeStartChar": 5417,
    "fixNodeLength": 37,
    "sourceBeforeFix": "secConn != null",
    "sourceAfterFix": "secConn != null && secConn.size() > 0"
  },
  {
    "bugType": "MORE_SPECIFIC_IF",
    "fixCommitSHA1": "e68b9038b64e1b3ad8068541216baf17da23ddde",
    "fixCommitParentSHA1": "aa2fabcfbdb12cfcabdf651f647ec5d18af6696a",
    "bugFilePath": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/MatchNode.java",
    "fixPatch": "diff --git a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/MatchNode.java b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/MatchNode.java\nindex 4026f62..02d50e2 100644\n--- a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/MatchNode.java\n+++ b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/MatchNode.java\n@@ -804,7 +804,7 @@\n \t\t\tgp1 = new GlobalProperties();\n \t\t}\n \n-\t\tif(preds2.size() == 1) {\n+\t\tif(preds2 != null && preds2.size() == 1) {\n \t\t\tgp2 = PactConnection.getGlobalPropertiesAfterConnection(preds2.get(0), this, ss2);\n \t\t} else {\n \t\t\t// TODO right now we drop all properties in the union case; need to figure out what properties can be kept\n",
    "projectName": "apache.flink",
    "bugLineNum": 807,
    "bugNodeStartChar": 33881,
    "bugNodeLength": 18,
    "fixLineNum": 807,
    "fixNodeStartChar": 33881,
    "fixNodeLength": 36,
    "sourceBeforeFix": "preds2.size() == 1",
    "sourceAfterFix": "preds2 != null && preds2.size() == 1"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "4b141cc461127483f6cc2370655053f473089966",
    "fixCommitParentSHA1": "55d69e56750249d4e4a1e76a6bec55f534f13841",
    "bugFilePath": "pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/test/util/InfiniteInputIterator.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/test/util/InfiniteInputIterator.java b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/test/util/InfiniteInputIterator.java\nindex ec46c3d..4d05afb 100644\n--- a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/test/util/InfiniteInputIterator.java\n+++ b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/test/util/InfiniteInputIterator.java\n@@ -30,7 +30,7 @@\n \t@Override\n \tpublic boolean next(PactRecord target) {\n \t\ttarget.setField(0, val1);\n-\t\ttarget.setField(0, val2);\n+\t\ttarget.setField(1, val2);\n \t\treturn true;\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 33,
    "bugNodeStartChar": 1468,
    "bugNodeLength": 24,
    "fixLineNum": 33,
    "fixNodeStartChar": 1468,
    "fixNodeLength": 24,
    "sourceBeforeFix": "target.setField(0,val2)",
    "sourceAfterFix": "target.setField(1,val2)"
  },
  {
    "bugType": "CHANGE_UNARY_OPERATOR",
    "fixCommitSHA1": "5a97f9067c0764905cdd987f246689fa14f88335",
    "fixCommitParentSHA1": "ba61b9ae3be616011e9acebe02c7f134a41061c6",
    "bugFilePath": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java",
    "fixPatch": "diff --git a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java\nindex 8f8b5b2..643b7bc 100644\n--- a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java\n+++ b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/LocalProperties.java\n@@ -190,7 +190,7 @@\n \t\t}\n \t\t\n \t\t\n-\t\treturn isTrivial();\n+\t\treturn !isTrivial();\n \t\t\n \t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 193,
    "bugNodeStartChar": 5407,
    "bugNodeLength": 11,
    "fixLineNum": 193,
    "fixNodeStartChar": 5407,
    "fixNodeLength": 12,
    "sourceBeforeFix": "isTrivial()",
    "sourceAfterFix": "!isTrivial()"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "d762d973a7e7c33f289aa273d84c2bba5da076fb",
    "fixCommitParentSHA1": "be306c6e69d53a3d4f9d23265c68fa50286b663c",
    "bugFilePath": "sopremo/sopremo-cleansing/src/test/java/eu/stratosphere/sopremo/cleansing/TransitiveClosure/ParallelTransitiveClosureTest.java",
    "fixPatch": "diff --git a/sopremo/sopremo-cleansing/src/test/java/eu/stratosphere/sopremo/cleansing/TransitiveClosure/ParallelTransitiveClosureTest.java b/sopremo/sopremo-cleansing/src/test/java/eu/stratosphere/sopremo/cleansing/TransitiveClosure/ParallelTransitiveClosureTest.java\nindex 90e9824..79c2f5e 100644\n--- a/sopremo/sopremo-cleansing/src/test/java/eu/stratosphere/sopremo/cleansing/TransitiveClosure/ParallelTransitiveClosureTest.java\n+++ b/sopremo/sopremo-cleansing/src/test/java/eu/stratosphere/sopremo/cleansing/TransitiveClosure/ParallelTransitiveClosureTest.java\n@@ -60,7 +60,7 @@\n \tpublic void shouldFindTransitiveClosureInWholeMatrix() {\n \t\tfinal TestTransitiveClosure transitiveClosure = new TestTransitiveClosure();\n \t\ttransitiveClosure.setPhase(3);\n-\t\ttransitiveClosure.setNumberOfPartitions(6);\n+\t\ttransitiveClosure.setNumberOfPartitions(3);\n \t\t\n \t\tfinal SopremoTestPlan sopremoTestPlan = new SopremoTestPlan(transitiveClosure);\n \t\tString nullInput = SopremoTest.getResourcePath(\"null.json\");\n",
    "projectName": "apache.flink",
    "bugLineNum": 63,
    "bugNodeStartChar": 2106,
    "bugNodeLength": 42,
    "fixLineNum": 63,
    "fixNodeStartChar": 2106,
    "fixNodeLength": 42,
    "sourceBeforeFix": "transitiveClosure.setNumberOfPartitions(6)",
    "sourceAfterFix": "transitiveClosure.setNumberOfPartitions(3)"
  },
  {
    "bugType": "CHANGE_OPERATOR",
    "fixCommitSHA1": "594c6ff5541529755a3bef8ca9530d0ac3d5b690",
    "fixCommitParentSHA1": "3775fdcaac370f6cccf6a7a5089bff12b6f28782",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\nindex b83e458..0738200 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\n@@ -643,14 +643,14 @@\n \t\t\t\t// the remainder %8 comes first \n \t\t\t\tint col = numFields - 1;\n \t\t\t\tint mask = 0;\n-\t\t\t\tfor (int i = numFields & 0x7; i >= 0; i--, col--) {\n+\t\t\t\tfor (int i = numFields & 0x7; i > 0; i--, col--) {\n \t\t\t\t\tmask <<= 1;\n \t\t\t\t\tmask |= (offsets[col] != NULL_INDICATOR_OFFSET) ? 0x1 : 0x0;\n \t\t\t\t}\n \t\t\t\tserializer.writeByte(mask);\n \t\t\t\t\n \t\t\t\t// now the eight-bit chunks\n-\t\t\t\tfor (int i = numFields >>> 3; i >= 0; i--) {\n+\t\t\t\tfor (int i = numFields >>> 3; i > 0; i--) {\n \t\t\t\t\tmask = 0;\n \t\t\t\t\tfor (int k = 0; k < 8; k++, col--) {\n \t\t\t\t\t\tmask <<= 1;\n",
    "projectName": "apache.flink",
    "bugLineNum": 646,
    "bugNodeStartChar": 20749,
    "bugNodeLength": 6,
    "fixLineNum": 646,
    "fixNodeStartChar": 20749,
    "fixNodeLength": 5,
    "sourceBeforeFix": "i >= 0",
    "sourceAfterFix": "i > 0"
  },
  {
    "bugType": "CHANGE_OPERATOR",
    "fixCommitSHA1": "594c6ff5541529755a3bef8ca9530d0ac3d5b690",
    "fixCommitParentSHA1": "3775fdcaac370f6cccf6a7a5089bff12b6f28782",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\nindex b83e458..0738200 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\n@@ -643,14 +643,14 @@\n \t\t\t\t// the remainder %8 comes first \n \t\t\t\tint col = numFields - 1;\n \t\t\t\tint mask = 0;\n-\t\t\t\tfor (int i = numFields & 0x7; i >= 0; i--, col--) {\n+\t\t\t\tfor (int i = numFields & 0x7; i > 0; i--, col--) {\n \t\t\t\t\tmask <<= 1;\n \t\t\t\t\tmask |= (offsets[col] != NULL_INDICATOR_OFFSET) ? 0x1 : 0x0;\n \t\t\t\t}\n \t\t\t\tserializer.writeByte(mask);\n \t\t\t\t\n \t\t\t\t// now the eight-bit chunks\n-\t\t\t\tfor (int i = numFields >>> 3; i >= 0; i--) {\n+\t\t\t\tfor (int i = numFields >>> 3; i > 0; i--) {\n \t\t\t\t\tmask = 0;\n \t\t\t\t\tfor (int k = 0; k < 8; k++, col--) {\n \t\t\t\t\t\tmask <<= 1;\n",
    "projectName": "apache.flink",
    "bugLineNum": 653,
    "bugNodeStartChar": 20963,
    "bugNodeLength": 6,
    "fixLineNum": 653,
    "fixNodeStartChar": 20963,
    "fixNodeLength": 5,
    "sourceBeforeFix": "i >= 0",
    "sourceAfterFix": "i > 0"
  },
  {
    "bugType": "CHANGE_OPERATOR",
    "fixCommitSHA1": "641e4509dfb5e9aa7576f8225f37ed035a15894a",
    "fixCommitParentSHA1": "3775fdcaac370f6cccf6a7a5089bff12b6f28782",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\nindex b83e458..0738200 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\n@@ -643,14 +643,14 @@\n \t\t\t\t// the remainder %8 comes first \n \t\t\t\tint col = numFields - 1;\n \t\t\t\tint mask = 0;\n-\t\t\t\tfor (int i = numFields & 0x7; i >= 0; i--, col--) {\n+\t\t\t\tfor (int i = numFields & 0x7; i > 0; i--, col--) {\n \t\t\t\t\tmask <<= 1;\n \t\t\t\t\tmask |= (offsets[col] != NULL_INDICATOR_OFFSET) ? 0x1 : 0x0;\n \t\t\t\t}\n \t\t\t\tserializer.writeByte(mask);\n \t\t\t\t\n \t\t\t\t// now the eight-bit chunks\n-\t\t\t\tfor (int i = numFields >>> 3; i >= 0; i--) {\n+\t\t\t\tfor (int i = numFields >>> 3; i > 0; i--) {\n \t\t\t\t\tmask = 0;\n \t\t\t\t\tfor (int k = 0; k < 8; k++, col--) {\n \t\t\t\t\t\tmask <<= 1;\n",
    "projectName": "apache.flink",
    "bugLineNum": 646,
    "bugNodeStartChar": 20749,
    "bugNodeLength": 6,
    "fixLineNum": 646,
    "fixNodeStartChar": 20749,
    "fixNodeLength": 5,
    "sourceBeforeFix": "i >= 0",
    "sourceAfterFix": "i > 0"
  },
  {
    "bugType": "CHANGE_OPERATOR",
    "fixCommitSHA1": "641e4509dfb5e9aa7576f8225f37ed035a15894a",
    "fixCommitParentSHA1": "3775fdcaac370f6cccf6a7a5089bff12b6f28782",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\nindex b83e458..0738200 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\n@@ -643,14 +643,14 @@\n \t\t\t\t// the remainder %8 comes first \n \t\t\t\tint col = numFields - 1;\n \t\t\t\tint mask = 0;\n-\t\t\t\tfor (int i = numFields & 0x7; i >= 0; i--, col--) {\n+\t\t\t\tfor (int i = numFields & 0x7; i > 0; i--, col--) {\n \t\t\t\t\tmask <<= 1;\n \t\t\t\t\tmask |= (offsets[col] != NULL_INDICATOR_OFFSET) ? 0x1 : 0x0;\n \t\t\t\t}\n \t\t\t\tserializer.writeByte(mask);\n \t\t\t\t\n \t\t\t\t// now the eight-bit chunks\n-\t\t\t\tfor (int i = numFields >>> 3; i >= 0; i--) {\n+\t\t\t\tfor (int i = numFields >>> 3; i > 0; i--) {\n \t\t\t\t\tmask = 0;\n \t\t\t\t\tfor (int k = 0; k < 8; k++, col--) {\n \t\t\t\t\t\tmask <<= 1;\n",
    "projectName": "apache.flink",
    "bugLineNum": 653,
    "bugNodeStartChar": 20963,
    "bugNodeLength": 6,
    "fixLineNum": 653,
    "fixNodeStartChar": 20963,
    "fixNodeLength": 5,
    "sourceBeforeFix": "i >= 0",
    "sourceAfterFix": "i > 0"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "439bd6c127248f312e90722fc4a713d5ac78e32e",
    "fixCommitParentSHA1": "b9ee037c73cf2c183b99afc7ae539af142a73886",
    "bugFilePath": "nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/RecordReader.java",
    "fixPatch": "diff --git a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/RecordReader.java b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/RecordReader.java\nindex 92655d8..2ecdf5f 100644\n--- a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/RecordReader.java\n+++ b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/RecordReader.java\n@@ -30,7 +30,7 @@\n  *        the type of the record that can be read from this record reader\n  */\n \n-public final class RecordReader<T extends Record> extends AbstractRecordReader<T> implements Reader<T> {\n+public class RecordReader<T extends Record> extends AbstractRecordReader<T> implements Reader<T> {\n \n \t/**\n \t * Temporarily stores an exception which may have occurred while reading data from the input gate.\n",
    "projectName": "apache.flink",
    "bugLineNum": 24,
    "bugNodeStartChar": 1099,
    "bugNodeLength": 6996,
    "fixLineNum": 24,
    "fixNodeStartChar": 1099,
    "fixNodeLength": 6990,
    "sourceBeforeFix": "17",
    "sourceAfterFix": "1"
  },
  {
    "bugType": "SWAP_BOOLEAN_LITERAL",
    "fixCommitSHA1": "0ddd20165c20cc368370c29adba7a9957048fe89",
    "fixCommitParentSHA1": "4932dace62490653fd91f5a35a556111d61d9fa3",
    "bugFilePath": "nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/InputGate.java",
    "fixPatch": "diff --git a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/InputGate.java b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/InputGate.java\nindex 9b09257..e60e95c 100644\n--- a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/InputGate.java\n+++ b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/InputGate.java\n@@ -632,7 +632,7 @@\n \t\tif (this.channelToReadFrom == -1) {\n \n \t\t\tif (this.isClosed()) {\n-\t\t\t\treturn false;\n+\t\t\t\treturn true;\n \t\t\t}\n \n \t\t\tsynchronized (this.availableChannels) {\n",
    "projectName": "apache.flink",
    "bugLineNum": 635,
    "bugNodeStartChar": 19182,
    "bugNodeLength": 13,
    "fixLineNum": 635,
    "fixNodeStartChar": 19182,
    "fixNodeLength": 12,
    "sourceBeforeFix": "return false; ",
    "sourceAfterFix": "return true; "
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "fd172432f0b7fcb65b990ece77a85fe10ea42c21",
    "fixCommitParentSHA1": "f79c4eb83cec87ffcdf2ebef264921fe3274eecf",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/io/FileInputFormat.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/io/FileInputFormat.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/io/FileInputFormat.java\nindex 4b47514..e47039e 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/io/FileInputFormat.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/io/FileInputFormat.java\n@@ -478,9 +478,9 @@\n \t\t\n \t\tprivate final long timeout;\n \n-\t\tprivate FSDataInputStream fdis = null;\n+\t\tprivate volatile FSDataInputStream fdis = null;\n \n-\t\tprivate Throwable error = null;\n+\t\tprivate volatile Throwable error = null;\n \n \t\tpublic InputSplitOpenThread(FileInputSplit split, long timeout)\n \t\t{\n@@ -527,7 +527,7 @@\n \t\t\t}\n \t\t\t\n \t\t\t// try to forcefully shut this thread down\n-\t\t\tthrow new IOException(\"OPening request timed out.\");\n+\t\t\tthrow new IOException(\"Opening request timed out.\");\n \t\t}\n \n \t\tpublic FSDataInputStream getFSDataInputStream() {\n",
    "projectName": "apache.flink",
    "bugLineNum": 481,
    "bugNodeStartChar": 16174,
    "bugNodeLength": 38,
    "fixLineNum": 481,
    "fixNodeStartChar": 16174,
    "fixNodeLength": 47,
    "sourceBeforeFix": "2",
    "sourceAfterFix": "66"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "fd172432f0b7fcb65b990ece77a85fe10ea42c21",
    "fixCommitParentSHA1": "f79c4eb83cec87ffcdf2ebef264921fe3274eecf",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/io/FileInputFormat.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/io/FileInputFormat.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/io/FileInputFormat.java\nindex 4b47514..e47039e 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/io/FileInputFormat.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/io/FileInputFormat.java\n@@ -478,9 +478,9 @@\n \t\t\n \t\tprivate final long timeout;\n \n-\t\tprivate FSDataInputStream fdis = null;\n+\t\tprivate volatile FSDataInputStream fdis = null;\n \n-\t\tprivate Throwable error = null;\n+\t\tprivate volatile Throwable error = null;\n \n \t\tpublic InputSplitOpenThread(FileInputSplit split, long timeout)\n \t\t{\n@@ -527,7 +527,7 @@\n \t\t\t}\n \t\t\t\n \t\t\t// try to forcefully shut this thread down\n-\t\t\tthrow new IOException(\"OPening request timed out.\");\n+\t\t\tthrow new IOException(\"Opening request timed out.\");\n \t\t}\n \n \t\tpublic FSDataInputStream getFSDataInputStream() {\n",
    "projectName": "apache.flink",
    "bugLineNum": 483,
    "bugNodeStartChar": 16216,
    "bugNodeLength": 31,
    "fixLineNum": 483,
    "fixNodeStartChar": 16216,
    "fixNodeLength": 40,
    "sourceBeforeFix": "2",
    "sourceAfterFix": "66"
  },
  {
    "bugType": "CHANGE_UNARY_OPERATOR",
    "fixCommitSHA1": "9e159f737f8a42fef886062a3fd5850531281306",
    "fixCommitParentSHA1": "7441a9a2a718f36e9f5b9cd8f4d967f306de4708",
    "bugFilePath": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/PactCompiler.java",
    "fixPatch": "diff --git a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/PactCompiler.java b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/PactCompiler.java\nindex 2b33fb1..3af9a9e 100644\n--- a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/PactCompiler.java\n+++ b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/PactCompiler.java\n@@ -710,7 +710,7 @@\n \t */\n \tpublic static OptimizedPlan createPreOptimizedPlan(Plan pactPlan)\n \t{\n-\t\tGraphCreatingVisitor graphCreator = new GraphCreatingVisitor(null, -1, -1, false);\n+\t\tGraphCreatingVisitor graphCreator = new GraphCreatingVisitor(null, -1, 1, false);\n \t\tpactPlan.accept(graphCreator);\n \t\tOptimizedPlan optPlan = new OptimizedPlan(graphCreator.sources, graphCreator.sinks, graphCreator.con2node.values(),\n \t\t\t\tpactPlan.getJobName());\n",
    "projectName": "apache.flink",
    "bugLineNum": 713,
    "bugNodeStartChar": 32275,
    "bugNodeLength": 2,
    "fixLineNum": 713,
    "fixNodeStartChar": 32275,
    "fixNodeLength": 1,
    "sourceBeforeFix": "-1",
    "sourceAfterFix": "1"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "db0bc68c325530408ffad61660e9f2ccb9ad2a27",
    "fixCommitParentSHA1": "1e9eb92dc929b2e0f41f68c0596861ad2466a6d8",
    "bugFilePath": "pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java",
    "fixPatch": "diff --git a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\nindex 19cb2d4..a1ad345 100644\n--- a/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\n+++ b/pact/pact-common/src/main/java/eu/stratosphere/pact/common/type/PactRecord.java\n@@ -235,7 +235,7 @@\n \t\t}\n \t\t\n \t\t// deserialize\n-\t\tdeserialize(field, offset, limit);\n+\t\tdeserialize(field, offset, limit, fieldNum);\n \t\treturn field;\n \t}\n \t\n@@ -271,7 +271,7 @@\n \t\t}\n \t\t\n \t\tfinal int limit = offset + this.lengths[fieldNum];\n-\t\tdeserialize(target, offset, limit);\n+\t\tdeserialize(target, offset, limit, fieldNum);\n \t\treturn target;\n \t}\n \t\n@@ -303,7 +303,7 @@\n \t\t}\n \t\t\n \t\tfinal int limit = offset + this.lengths[fieldNum];\n-\t\tdeserialize(target, offset, limit);\n+\t\tdeserialize(target, offset, limit, fieldNum);\n \t\treturn true;\n \t}\n \t\n@@ -331,7 +331,7 @@\n \t * @param offset The offset in the binary string.\n \t * @param limit The limit in the binary string.\n \t */\n-\tprivate final <T extends Value> void deserialize(T target, int offset, int limit)\n+\tprivate final <T extends Value> void deserialize(T target, int offset, int limit, int fieldNumber)\n \t{\n \t\tif (this.serializer == null) {\n \t\t\tthis.serializer = new InternalDeSerializer();\n@@ -344,7 +344,7 @@\n \t\t\ttarget.read(serializer);\n \t\t}\n \t\tcatch (Exception e) {\n-\t\t\tthrow new DeserializationException(e);\n+\t\t\tthrow new DeserializationException(\"Error reading field \" + fieldNumber + \" as \" + target.getClass().getName(), e);\n \t\t}\n \t}\n \t\n",
    "projectName": "apache.flink",
    "bugLineNum": 347,
    "bugNodeStartChar": 11889,
    "bugNodeLength": 31,
    "fixLineNum": 347,
    "fixNodeStartChar": 11889,
    "fixNodeLength": 108,
    "sourceBeforeFix": "new DeserializationException(e)",
    "sourceAfterFix": "new DeserializationException(\"Error reading field \" + fieldNumber + \" as \"+ target.getClass().getName(),e)"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "bdd688e74669108b17f7f099ccaa14c3a9b5d333",
    "fixCommitParentSHA1": "28e04e2b56540dc96d83a2d9c5957649cc179701",
    "bugFilePath": "pact/pact-examples/src/main/java/eu/stratosphere/pact/example/graph/EnumTriangles.java",
    "fixPatch": "diff --git a/pact/pact-examples/src/main/java/eu/stratosphere/pact/example/graph/EnumTriangles.java b/pact/pact-examples/src/main/java/eu/stratosphere/pact/example/graph/EnumTriangles.java\nindex 8361b90..c15ae2c 100644\n--- a/pact/pact-examples/src/main/java/eu/stratosphere/pact/example/graph/EnumTriangles.java\n+++ b/pact/pact-examples/src/main/java/eu/stratosphere/pact/example/graph/EnumTriangles.java\n@@ -123,7 +123,7 @@\n \t\t\t\tedge = new Edge(new PactString(rdfObj), new PactString(rdfSubj));\n \t\t\t}\n \n-\t\t\ttarget.setField(1, edge);\n+\t\t\ttarget.setField(0, edge);\n \n \t\t\tLOG.debug(\"Read in: \" + edge);\n \t\t\treturn true;\n@@ -244,7 +244,7 @@\n \t\tprivate static final Log LOG = LogFactory.getLog(CloseTriads.class);\n \n \t\t@Override\n-\t\tpublic void match(PactRecord missingEdge, PactRecord triad, Collector out) throws Exception {\n+\t\tpublic void match(PactRecord triad, PactRecord missingEdge, Collector out) throws Exception {\n \t\t\t\n \t\t\tLOG.debug(\"Emit: \" + missingEdge);\n \t\t\t\n",
    "projectName": "apache.flink",
    "bugLineNum": 126,
    "bugNodeStartChar": 5195,
    "bugNodeLength": 24,
    "fixLineNum": 126,
    "fixNodeStartChar": 5195,
    "fixNodeLength": 24,
    "sourceBeforeFix": "target.setField(1,edge)",
    "sourceAfterFix": "target.setField(0,edge)"
  },
  {
    "bugType": "SWAP_ARGUMENTS",
    "fixCommitSHA1": "6e26eeb283127f1d27ed9c055874236896c5bd10",
    "fixCommitParentSHA1": "7f1e4d0fe54112d093d94b6805270fff5a45995d",
    "bugFilePath": "pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/MatchTask.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/MatchTask.java b/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/MatchTask.java\nindex 2f52c80..cc048e2 100644\n--- a/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/MatchTask.java\n+++ b/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/MatchTask.java\n@@ -131,7 +131,7 @@\n \t\t\tbreak;\n \t\tcase HYBRIDHASH_SECOND:\n \t\t\tthis.matchIterator = new BuildSecondHashMatchIterator(this.inputs[0], this.inputs[1], \n-\t\t\t\tkeyPositions1, keyPositions2, keyClasses, memoryManager, ioManager, this, availableMemory);\n+\t\t\t\tkeyPositions2, keyPositions1, keyClasses, memoryManager, ioManager, this, availableMemory);\n \t\t\tbreak;\n \t\tdefault:\n \t\t\tthrow new Exception(\"Unsupported local strategy for MatchTask: \" + ls.name());\n",
    "projectName": "apache.flink",
    "bugLineNum": 133,
    "bugNodeStartChar": 5368,
    "bugNodeLength": 160,
    "fixLineNum": 133,
    "fixNodeStartChar": 5368,
    "fixNodeLength": 160,
    "sourceBeforeFix": "new BuildSecondHashMatchIterator(this.inputs[0],this.inputs[1],keyPositions1,keyPositions2,keyClasses,memoryManager,ioManager,this,availableMemory)",
    "sourceAfterFix": "new BuildSecondHashMatchIterator(this.inputs[0],this.inputs[1],keyPositions2,keyPositions1,keyClasses,memoryManager,ioManager,this,availableMemory)"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "2976be58f6062932d6c23e63af3d3b4395ac461a",
    "fixCommitParentSHA1": "0c4e37bd41439a7620b3b03974954e41b18e5446",
    "bugFilePath": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/transferenvelope/CheckpointDeserializer.java",
    "fixPatch": "diff --git a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/transferenvelope/CheckpointDeserializer.java b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/transferenvelope/CheckpointDeserializer.java\nindex 15e31b6..ded5c66 100644\n--- a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/transferenvelope/CheckpointDeserializer.java\n+++ b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/transferenvelope/CheckpointDeserializer.java\n@@ -89,7 +89,7 @@\n \t\t}\n \n \t\tfor (int i = 0; i < SIZEOFLONG; ++i) {\n-\t\t\tl |= (byteBuffer.get((SIZEOFLONG - 1) - i) & 0xff) << (i << 3);\n+\t\t\tl |= (byteBuffer.get((SIZEOFLONG - 1) - i) & 0xffL) << (i << 3);\n \t\t}\n \n \t\treturn l;\n",
    "projectName": "apache.flink",
    "bugLineNum": 92,
    "bugNodeStartChar": 3253,
    "bugNodeLength": 43,
    "fixLineNum": 92,
    "fixNodeStartChar": 3253,
    "fixNodeLength": 44,
    "sourceBeforeFix": "byteBuffer.get((SIZEOFLONG - 1) - i) & 0xff",
    "sourceAfterFix": "byteBuffer.get((SIZEOFLONG - 1) - i) & 0xffL"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "2976be58f6062932d6c23e63af3d3b4395ac461a",
    "fixCommitParentSHA1": "0c4e37bd41439a7620b3b03974954e41b18e5446",
    "bugFilePath": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/transferenvelope/CheckpointSerializer.java",
    "fixPatch": "diff --git a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/transferenvelope/CheckpointSerializer.java b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/transferenvelope/CheckpointSerializer.java\nindex 1ad8e2b..ce707b6 100644\n--- a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/transferenvelope/CheckpointSerializer.java\n+++ b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/transferenvelope/CheckpointSerializer.java\n@@ -124,7 +124,7 @@\n \n \t\tfor (int i = 0; i < SIZEOFLONG; ++i) {\n \t\t\tfinal int shift = i << 3; // i * 8\n-\t\t\tbyteBuffer.put((SIZEOFLONG - 1) - i, (byte) ((longToSerialize & (0xff << shift)) >>> shift));\n+\t\t\tbyteBuffer.put((SIZEOFLONG - 1) - i, (byte) ((longToSerialize & (0xffL << shift)) >>> shift));\n \t\t}\n \t}\n }\n",
    "projectName": "apache.flink",
    "bugLineNum": 127,
    "bugNodeStartChar": 4098,
    "bugNodeLength": 13,
    "fixLineNum": 127,
    "fixNodeStartChar": 4098,
    "fixNodeLength": 14,
    "sourceBeforeFix": "0xff << shift",
    "sourceAfterFix": "0xffL << shift"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "cf43da88e745ca49d617ad4ebb5c5d8036dc4ed3",
    "fixCommitParentSHA1": "9f43ae7a3cae06587c35ad694a112fec0f8da8ab",
    "bugFilePath": "sopremo/sopremo-cleansing/src/main/java/eu/stratosphere/usecase/cleansing/GovWild.java",
    "fixPatch": "diff --git a/sopremo/sopremo-cleansing/src/main/java/eu/stratosphere/usecase/cleansing/GovWild.java b/sopremo/sopremo-cleansing/src/main/java/eu/stratosphere/usecase/cleansing/GovWild.java\nindex a625707..ef69d83 100644\n--- a/sopremo/sopremo-cleansing/src/main/java/eu/stratosphere/usecase/cleansing/GovWild.java\n+++ b/sopremo/sopremo-cleansing/src/main/java/eu/stratosphere/usecase/cleansing/GovWild.java\n@@ -248,12 +248,15 @@\n \t\t// ObjectAccess(\"lastName\"), new ConstantExpression(0), new ConstantExpression(2)));\n \t\tDisjunctPartitioning partitioning = new DisjunctPartitioning(\n \t\t\tnew TernaryExpression(\n-\t\t\t\tnew AndExpression(new ObjectAccess(\"addresses\"), new PathExpression(new ObjectAccess(\"addresses\"),\n-\t\t\t\t\tnew ArrayAccess(0), new ObjectAccess(\"zipCode\"))),\n+\t\t\t\tnew AndExpression(new ObjectAccess(\"addresses\"), \n+\t\t\t\t\tnew ComparativeExpression(new ConstantExpression(2), BinaryOperator.LESS_EQUAL, \n+\t\t\t\t\t\tnew FunctionCall(\"length\", new PathExpression(new ObjectAccess(\"addresses\"),\n+\t\t\t\t\t\t\tnew ArrayAccess(0), new ObjectAccess(\"zipCode\"))))),\n \t\t\t\tnew FunctionCall(\"substring\", new PathExpression(new ObjectAccess(\"addresses\"), new ArrayAccess(0),\n-\t\t\t\t\tnew ObjectAccess(\"zipCode\"))),\n+\t\t\t\t\tnew ObjectAccess(\"zipCode\")), new ConstantExpression(0), new ConstantExpression(2)),\n \t\t\t\tnew ConstantExpression(\"\")),\n-\t\t\t\tnew FunctionCall(\"extract\", new ConstantExpression(\"([^ ])*\"), new ObjectAccess(\"name\")));\n+\t\t\t\tnew FunctionCall(\"extract\", new ConstantExpression(\"([^ ])*\"), \n+\t\t\t\t\tnew PathExpression(new ObjectAccess(\"names\"), new ArrayAccess(0))));\n \t\tInterSourceRecordLinkage recordLinkage = new InterSourceRecordLinkage(partitioning, simmFunction, 0.8,\n \t\t\tthis.fused[EARMARK][LEGAL_ENTITY], this.inputs[SPENDING][LEGAL_ENTITY]);\n \t\trecordLinkage.setLinkageMode(LinkageMode.ALL_CLUSTERS_FLAT);\n@@ -310,10 +313,10 @@\n \n \t\tObjectCreation merge = new ObjectCreation();\n \t\tmerge.addMapping(new ObjectCreation.CopyFields(new InputSelection(1)));\n-\t\tmerge.addMapping(\"names\", new FunctionCall(\"distict\", new FunctionCall(\"unionAll\",\n+\t\tmerge.addMapping(\"names\", new FunctionCall(\"distinct\", new FunctionCall(\"unionAll\",\n \t\t\tnew PathExpression(new InputSelection(0), new ObjectAccess(\"names\")),\n \t\t\tnew PathExpression(new InputSelection(1), new ObjectAccess(\"names\")))));\n-\t\tmerge.addMapping(\"addresses\", new FunctionCall(\"distict\", new FunctionCall(\"unionAll\",\n+\t\tmerge.addMapping(\"addresses\", new FunctionCall(\"distinct\", new FunctionCall(\"unionAll\",\n \t\t\tnew PathExpression(new InputSelection(0), new ObjectAccess(\"addresses\")),\n \t\t\tnew PathExpression(new InputSelection(1), new ObjectAccess(\"addresses\")))));\n \t\t// merge.addMapping(\"funds\", new PathExpression(new ArrayAccess(1), new ObjectAccess(\"enactedFunds\")));\n@@ -345,10 +348,10 @@\n \t\t\n \t\tObjectCreation merge = new ObjectCreation();\n \t\tmerge.addMapping(new ObjectCreation.CopyFields(new InputSelection(1)));\n-\t\tmerge.addMapping(\"names\", new FunctionCall(\"distict\", new FunctionCall(\"unionAll\",\n+\t\tmerge.addMapping(\"names\", new FunctionCall(\"distinct\", new FunctionCall(\"unionAll\",\n \t\t\tnew PathExpression(new InputSelection(0), new ObjectAccess(\"names\")),\n \t\t\tnew PathExpression(new InputSelection(1), new ObjectAccess(\"names\")))));\n-\t\tmerge.addMapping(\"addresses\", new FunctionCall(\"distict\", new FunctionCall(\"unionAll\",\n+\t\tmerge.addMapping(\"addresses\", new FunctionCall(\"distinct\", new FunctionCall(\"unionAll\",\n \t\t\tnew PathExpression(new InputSelection(0), new ObjectAccess(\"addresses\")),\n \t\t\tnew PathExpression(new InputSelection(1), new ObjectAccess(\"addresses\")))));\n \t\t// merge.addMapping(\"funds\", new PathExpression(new ArrayAccess(1), new ObjectAccess(\"enactedFunds\")));\n",
    "projectName": "apache.flink",
    "bugLineNum": 253,
    "bugNodeStartChar": 12284,
    "bugNodeLength": 134,
    "fixLineNum": 253,
    "fixNodeStartChar": 12284,
    "fixNodeLength": 188,
    "sourceBeforeFix": "new FunctionCall(\"substring\",new PathExpression(new ObjectAccess(\"addresses\"),new ArrayAccess(0),new ObjectAccess(\"zipCode\")))",
    "sourceAfterFix": "new FunctionCall(\"substring\",new PathExpression(new ObjectAccess(\"addresses\"),new ArrayAccess(0),new ObjectAccess(\"zipCode\")),new ConstantExpression(0),new ConstantExpression(2))"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "0cae1dd2419de0832f496dad480d8a11a9d6074f",
    "fixCommitParentSHA1": "1b7a8767f3019d121e0698f5dcd66b6412eb0bd9",
    "bugFilePath": "pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/PartitionTask.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/PartitionTask.java b/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/PartitionTask.java\nindex cf3d9cc..1bd7e40 100644\n--- a/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/PartitionTask.java\n+++ b/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/PartitionTask.java\n@@ -262,7 +262,7 @@\n \t\t\n \t\t// determine distribution pattern for reader from input ship strategy\n \t\tDistributionPattern dpReader = null;\n-\t\tswitch (config.getInputShipStrategy(0)) {\n+\t\tswitch (config.getInputShipStrategy(1)) {\n \t\tcase FORWARD:\n \t\t\t// forward requires Pointwise DP\n \t\t\tdpReader = new PointwiseDistributionPattern();\n",
    "projectName": "apache.flink",
    "bugLineNum": 265,
    "bugNodeStartChar": 9431,
    "bugNodeLength": 30,
    "fixLineNum": 265,
    "fixNodeStartChar": 9431,
    "fixNodeLength": 30,
    "sourceBeforeFix": "config.getInputShipStrategy(0)",
    "sourceAfterFix": "config.getInputShipStrategy(1)"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "56713d36c65344d448b58f697bb109c882f44dc0",
    "fixCommitParentSHA1": "b298b97893ac3ec02c35bc83e4969369f33f3f60",
    "bugFilePath": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java",
    "fixPatch": "diff --git a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java\nindex b5815dd..d2c7063 100644\n--- a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java\n+++ b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java\n@@ -50,11 +50,11 @@\n \n \tprivate static final Log LOG = LogFactory.getLog(ByteBufferedChannelManager.class);\n \n-\tprivate static final int DEFAULT_NUMBER_OF_READ_BUFFERS = 128;\n+\tprivate static final int DEFAULT_NUMBER_OF_READ_BUFFERS = 256;\n \n-\tprivate static final int DEFAULT_NUMBER_OF_WRITE_BUFFERS = 128;\n+\tprivate static final int DEFAULT_NUMBER_OF_WRITE_BUFFERS = 256;\n \n-\tprivate static final int DEFAULT_BUFFER_SIZE_IN_BYTES = 128 * 1024; // 128k\n+\tprivate static final int DEFAULT_BUFFER_SIZE_IN_BYTES = 64 * 1024; // 64k\n \n \tprivate static final boolean DEFAULT_ALLOW_SPILLING = true;\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 53,
    "bugNodeStartChar": 2437,
    "bugNodeLength": 36,
    "fixLineNum": 53,
    "fixNodeStartChar": 2437,
    "fixNodeLength": 36,
    "sourceBeforeFix": "DEFAULT_NUMBER_OF_READ_BUFFERS=128",
    "sourceAfterFix": "DEFAULT_NUMBER_OF_READ_BUFFERS=256"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "56713d36c65344d448b58f697bb109c882f44dc0",
    "fixCommitParentSHA1": "b298b97893ac3ec02c35bc83e4969369f33f3f60",
    "bugFilePath": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java",
    "fixPatch": "diff --git a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java\nindex b5815dd..d2c7063 100644\n--- a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java\n+++ b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java\n@@ -50,11 +50,11 @@\n \n \tprivate static final Log LOG = LogFactory.getLog(ByteBufferedChannelManager.class);\n \n-\tprivate static final int DEFAULT_NUMBER_OF_READ_BUFFERS = 128;\n+\tprivate static final int DEFAULT_NUMBER_OF_READ_BUFFERS = 256;\n \n-\tprivate static final int DEFAULT_NUMBER_OF_WRITE_BUFFERS = 128;\n+\tprivate static final int DEFAULT_NUMBER_OF_WRITE_BUFFERS = 256;\n \n-\tprivate static final int DEFAULT_BUFFER_SIZE_IN_BYTES = 128 * 1024; // 128k\n+\tprivate static final int DEFAULT_BUFFER_SIZE_IN_BYTES = 64 * 1024; // 64k\n \n \tprivate static final boolean DEFAULT_ALLOW_SPILLING = true;\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 55,
    "bugNodeStartChar": 2502,
    "bugNodeLength": 37,
    "fixLineNum": 55,
    "fixNodeStartChar": 2502,
    "fixNodeLength": 37,
    "sourceBeforeFix": "DEFAULT_NUMBER_OF_WRITE_BUFFERS=128",
    "sourceAfterFix": "DEFAULT_NUMBER_OF_WRITE_BUFFERS=256"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "56713d36c65344d448b58f697bb109c882f44dc0",
    "fixCommitParentSHA1": "b298b97893ac3ec02c35bc83e4969369f33f3f60",
    "bugFilePath": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java",
    "fixPatch": "diff --git a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java\nindex b5815dd..d2c7063 100644\n--- a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java\n+++ b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java\n@@ -50,11 +50,11 @@\n \n \tprivate static final Log LOG = LogFactory.getLog(ByteBufferedChannelManager.class);\n \n-\tprivate static final int DEFAULT_NUMBER_OF_READ_BUFFERS = 128;\n+\tprivate static final int DEFAULT_NUMBER_OF_READ_BUFFERS = 256;\n \n-\tprivate static final int DEFAULT_NUMBER_OF_WRITE_BUFFERS = 128;\n+\tprivate static final int DEFAULT_NUMBER_OF_WRITE_BUFFERS = 256;\n \n-\tprivate static final int DEFAULT_BUFFER_SIZE_IN_BYTES = 128 * 1024; // 128k\n+\tprivate static final int DEFAULT_BUFFER_SIZE_IN_BYTES = 64 * 1024; // 64k\n \n \tprivate static final boolean DEFAULT_ALLOW_SPILLING = true;\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 57,
    "bugNodeStartChar": 2599,
    "bugNodeLength": 10,
    "fixLineNum": 57,
    "fixNodeStartChar": 2599,
    "fixNodeLength": 9,
    "sourceBeforeFix": "128 * 1024",
    "sourceAfterFix": "64 * 1024"
  },
  {
    "bugType": "CHANGE_CALLER_IN_FUNCTION_CALL",
    "fixCommitSHA1": "3a45f84e3ea25e96bd89d48b8e5ddb145e4a3815",
    "fixCommitParentSHA1": "9c64037812a2bdd7de70f6f99c1d93baa93340c3",
    "bugFilePath": "pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/task/CombineTaskExternalITCase.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/task/CombineTaskExternalITCase.java b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/task/CombineTaskExternalITCase.java\nindex 901f4ca..52d2d81 100644\n--- a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/task/CombineTaskExternalITCase.java\n+++ b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/task/CombineTaskExternalITCase.java\n@@ -135,7 +135,7 @@\n \t\t\t}\n \t\t}\n \t\t\n-\t\tAssert.assertTrue(\"Resultset size was \"+aggMap.size()+\". Expected was \"+keyCnt, outList.size() == keyCnt);\n+\t\tAssert.assertTrue(\"Resultset size was \"+aggMap.size()+\". Expected was \"+keyCnt, aggMap.size() == keyCnt);\n \t\t\n \t\tfor (PactInteger integer : aggMap.values()) {\n \t\t\tAssert.assertTrue(\"Incorrect result\", integer.getValue() == expSum);\n",
    "projectName": "apache.flink",
    "bugLineNum": 138,
    "bugNodeStartChar": 4773,
    "bugNodeLength": 14,
    "fixLineNum": 138,
    "fixNodeStartChar": 4773,
    "fixNodeLength": 13,
    "sourceBeforeFix": "outList.size()",
    "sourceAfterFix": "aggMap.size()"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "3a45f84e3ea25e96bd89d48b8e5ddb145e4a3815",
    "fixCommitParentSHA1": "9c64037812a2bdd7de70f6f99c1d93baa93340c3",
    "bugFilePath": "pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/task/CombineTaskExternalITCase.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/task/CombineTaskExternalITCase.java b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/task/CombineTaskExternalITCase.java\nindex 901f4ca..52d2d81 100644\n--- a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/task/CombineTaskExternalITCase.java\n+++ b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/task/CombineTaskExternalITCase.java\n@@ -135,7 +135,7 @@\n \t\t\t}\n \t\t}\n \t\t\n-\t\tAssert.assertTrue(\"Resultset size was \"+aggMap.size()+\". Expected was \"+keyCnt, outList.size() == keyCnt);\n+\t\tAssert.assertTrue(\"Resultset size was \"+aggMap.size()+\". Expected was \"+keyCnt, aggMap.size() == keyCnt);\n \t\t\n \t\tfor (PactInteger integer : aggMap.values()) {\n \t\t\tAssert.assertTrue(\"Incorrect result\", integer.getValue() == expSum);\n",
    "projectName": "apache.flink",
    "bugLineNum": 138,
    "bugNodeStartChar": 4773,
    "bugNodeLength": 14,
    "fixLineNum": 138,
    "fixNodeStartChar": 4773,
    "fixNodeLength": 13,
    "sourceBeforeFix": "outList.size()",
    "sourceAfterFix": "aggMap.size()"
  },
  {
    "bugType": "OVERLOAD_METHOD_MORE_ARGS",
    "fixCommitSHA1": "5dc0df588da9ff37e41685edfea041fd29371aed",
    "fixCommitParentSHA1": "8ca29830e25900fc66734c7008cedc56f0186397",
    "bugFilePath": "pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/UnilateralSortMergerITCase.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/UnilateralSortMergerITCase.java b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/UnilateralSortMergerITCase.java\nindex d0f47f0..547d5ea 100644\n--- a/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/UnilateralSortMergerITCase.java\n+++ b/pact/pact-runtime/src/test/java/eu/stratosphere/pact/runtime/sort/UnilateralSortMergerITCase.java\n@@ -159,7 +159,7 @@\n \t\tLOG.debug(\"initializing sortmerger\");\n \t\tSortMerger<TestData.Key, TestData.Value> merger = new UnilateralSortMerger<TestData.Key, TestData.Value>(\n \t\t\tmemoryManager, ioManager, 40 * 1024 * 1024, 1024 * 1024 * 1, 10, 2, keySerialization, valSerialization,\n-\t\t\tkeyComparator, reader, parentTask);\n+\t\t\tkeyComparator, reader, parentTask, 0.7f);\n \n \t\t// emit data\n \t\tLOG.debug(\"emitting data\");\n",
    "projectName": "apache.flink",
    "bugLineNum": 160,
    "bugNodeStartChar": 5703,
    "bugNodeLength": 200,
    "fixLineNum": 160,
    "fixNodeStartChar": 5703,
    "fixNodeLength": 206,
    "sourceBeforeFix": "new UnilateralSortMerger<TestData.Key,TestData.Value>(memoryManager,ioManager,40 * 1024 * 1024,1024 * 1024 * 1,10,2,keySerialization,valSerialization,keyComparator,reader,parentTask)",
    "sourceAfterFix": "new UnilateralSortMerger<TestData.Key,TestData.Value>(memoryManager,ioManager,40 * 1024 * 1024,1024 * 1024 * 1,10,2,keySerialization,valSerialization,keyComparator,reader,parentTask,0.7f)"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "1682d9037d8e9f49bbccdfe682553203abfea665",
    "fixCommitParentSHA1": "1f421be2133108bcd2d851811deb2157f887be03",
    "bugFilePath": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/io/channels/FileBuffer.java",
    "fixPatch": "diff --git a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/io/channels/FileBuffer.java b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/io/channels/FileBuffer.java\nindex f5b7404..ee8d456 100644\n--- a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/io/channels/FileBuffer.java\n+++ b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/io/channels/FileBuffer.java\n@@ -183,7 +183,7 @@\n \t\t\t}\n \n \t\t\tbytesWritten += b;\n-\t\t\tthis.totalBytesWritten += bytesWritten;\n+\t\t\tthis.totalBytesWritten += b;\n \t\t\tdiff = this.bufferSize - this.totalBytesWritten;\n \t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 186,
    "bugNodeStartChar": 5316,
    "bugNodeLength": 38,
    "fixLineNum": 186,
    "fixNodeStartChar": 5316,
    "fixNodeLength": 27,
    "sourceBeforeFix": "this.totalBytesWritten+=bytesWritten",
    "sourceAfterFix": "this.totalBytesWritten+=b"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "283b039b2b8ce9bdc4beae8c7df3442d0eb4175a",
    "fixCommitParentSHA1": "d8d4c61fa241b59da1df3287ca2349d7e6f4edab",
    "bugFilePath": "pact/pact-clients/src/main/java/eu/stratosphere/pact/client/CliFrontend.java",
    "fixPatch": "diff --git a/pact/pact-clients/src/main/java/eu/stratosphere/pact/client/CliFrontend.java b/pact/pact-clients/src/main/java/eu/stratosphere/pact/client/CliFrontend.java\nindex 3adabf6..47de3bc 100644\n--- a/pact/pact-clients/src/main/java/eu/stratosphere/pact/client/CliFrontend.java\n+++ b/pact/pact-clients/src/main/java/eu/stratosphere/pact/client/CliFrontend.java\n@@ -490,7 +490,7 @@\n \t\t\t\tif(scheduledJobs.size() == 0) {\n \t\t\t\t\tSystem.out.println(\"No scheduled jobs.\");\n \t\t\t\t} else {\n-\t\t\t\t\tCollections.sort(runningJobs, njec);\n+\t\t\t\t\tCollections.sort(scheduledJobs, njec);\n \t\t\t\t\t\n \t\t\t\t\tSystem.out.println(\"----------------------- Scheduled Jobs -----------------------\");\n \t\t\t\t\tfor(RecentJobEvent je : scheduledJobs) {\n",
    "projectName": "apache.flink",
    "bugLineNum": 493,
    "bugNodeStartChar": 14575,
    "bugNodeLength": 35,
    "fixLineNum": 493,
    "fixNodeStartChar": 14575,
    "fixNodeLength": 37,
    "sourceBeforeFix": "Collections.sort(runningJobs,njec)",
    "sourceAfterFix": "Collections.sort(scheduledJobs,njec)"
  },
  {
    "bugType": "CHANGE_MODIFIER",
    "fixCommitSHA1": "4f49fc2f6f1761e84ee8c6572b14e1d9adc16abc",
    "fixCommitParentSHA1": "59f9981a2502b1a2eaffe88c6e39e2f8bbbf6672",
    "bugFilePath": "nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/channels/Buffer.java",
    "fixPatch": "diff --git a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/channels/Buffer.java b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/channels/Buffer.java\nindex e470adc..fcc6afe 100644\n--- a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/channels/Buffer.java\n+++ b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/channels/Buffer.java\n@@ -34,7 +34,7 @@\n  * \n  * @author warneke\n  */\n-public final class Buffer implements ReadableByteChannel, WritableByteChannel {\n+public class Buffer implements ReadableByteChannel, WritableByteChannel {\n \n \t/**\n \t * The concrete buffer implementation to which all method calls on\n",
    "projectName": "apache.flink",
    "bugLineNum": 24,
    "bugNodeStartChar": 1118,
    "bugNodeLength": 6724,
    "fixLineNum": 24,
    "fixNodeStartChar": 1118,
    "fixNodeLength": 6718,
    "sourceBeforeFix": "17",
    "sourceAfterFix": "1"
  },
  {
    "bugType": "CHANGE_NUMERAL",
    "fixCommitSHA1": "e75b06a12cc6de7b6ffaa998c4c8540230ef5514",
    "fixCommitParentSHA1": "cc1a2ee025849b3d7ca58d42e780be310a1f6130",
    "bugFilePath": "pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/MatchNode.java",
    "fixPatch": "diff --git a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/MatchNode.java b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/MatchNode.java\nindex 073164ca..78c9a0f 100644\n--- a/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/MatchNode.java\n+++ b/pact/pact-compiler/src/main/java/eu/stratosphere/pact/compiler/plan/MatchNode.java\n@@ -142,7 +142,7 @@\n \t\t\tcase SORT_BOTH_MERGE:      return 2;\n \t\t\tcase SORT_FIRST_MERGE:     return 1;\n \t\t\tcase SORT_SECOND_MERGE:    return 1;\n-\t\t\tcase MERGE:                return 0;\n+\t\t\tcase MERGE:                return 1;\n \t\t\tcase HYBRIDHASH_FIRST:     return 1;\n \t\t\tcase HYBRIDHASH_SECOND:    return 1;\n \t\t\tcase MMHASH_FIRST:         return 1;\n",
    "projectName": "apache.flink",
    "bugLineNum": 145,
    "bugNodeStartChar": 6057,
    "bugNodeLength": 9,
    "fixLineNum": 145,
    "fixNodeStartChar": 6057,
    "fixNodeLength": 9,
    "sourceBeforeFix": "return 0; ",
    "sourceAfterFix": "return 1; "
  },
  {
    "bugType": "SWAP_BOOLEAN_LITERAL",
    "fixCommitSHA1": "2417f343536cdac38986221bd8dfab36ff0bf8bb",
    "fixCommitParentSHA1": "215cd50949e9baba47a93d5dabb7387ac16cd994",
    "bugFilePath": "nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java",
    "fixPatch": "diff --git a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java\nindex aeba66d..fb7a6fb 100644\n--- a/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java\n+++ b/nephele/nephele-server/src/main/java/eu/stratosphere/nephele/taskmanager/bytebuffered/ByteBufferedChannelManager.java\n@@ -62,7 +62,7 @@\n \n \tprivate static final int DEFAULT_BUFFER_SIZE_IN_BYTES = 128 * 1024; // 128k\n \n-\tprivate static final boolean DEFAULT_ALLOW_SPILLING = false;\n+\tprivate static final boolean DEFAULT_ALLOW_SPILLING = true;\n \n \tprivate static final int DEFAULT_NUMBER_OF_OUTGOING_CONNECTION_THREADS = 1;\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 65,
    "bugNodeStartChar": 2860,
    "bugNodeLength": 30,
    "fixLineNum": 65,
    "fixNodeStartChar": 2860,
    "fixNodeLength": 29,
    "sourceBeforeFix": "DEFAULT_ALLOW_SPILLING=false",
    "sourceAfterFix": "DEFAULT_ALLOW_SPILLING=true"
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "544285e04c0f376fe11c65e897ffa096d8049709",
    "fixCommitParentSHA1": "a4eabdb6677fcc58f38920dce7cc33987590d66f",
    "bugFilePath": "pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/util/OutputEmitter.java",
    "fixPatch": "diff --git a/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/util/OutputEmitter.java b/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/util/OutputEmitter.java\nindex 664f316..da045ea 100644\n--- a/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/util/OutputEmitter.java\n+++ b/pact/pact-runtime/src/main/java/eu/stratosphere/pact/runtime/task/util/OutputEmitter.java\n@@ -85,7 +85,7 @@\n \t */\n \tpublic OutputEmitter(ShipStrategy strategy, byte[] salt) {\n \t\tif (strategy != ShipStrategy.BROADCAST && strategy != ShipStrategy.PARTITION_HASH\n-\t\t\t\t&& strategy != ShipStrategy.NONE)\n+\t\t\t\t&& strategy != ShipStrategy.FORWARD)\n \t\t{\n \t\t\tthrow new UnsupportedOperationException(\"Unsupported distribution strategy: \" + strategy.name());\n \t\t}\n@@ -108,7 +108,7 @@\n \t\t\treturn broadcast(numberOfChannels);\n \t\tcase PARTITION_HASH:\n \t\t\treturn partition(pair, numberOfChannels);\n-\t\tcase NONE:\n+\t\tcase FORWARD:\n \t\t\treturn robin(numberOfChannels);\n \t\tdefault:\n \t\t\tthrow new UnsupportedOperationException();\n",
    "projectName": "apache.flink",
    "bugLineNum": 111,
    "bugNodeStartChar": 4028,
    "bugNodeLength": 10,
    "fixLineNum": 111,
    "fixNodeStartChar": 4028,
    "fixNodeLength": 13,
    "sourceBeforeFix": "case NONE: ",
    "sourceAfterFix": "case FORWARD: "
  },
  {
    "bugType": "CHANGE_IDENTIFIER",
    "fixCommitSHA1": "a33c7226df1ebd6d0deab62bead9438d9a1ff2ac",
    "fixCommitParentSHA1": "88695264999bd49d16c23e2bbb772ff4b5112b4d",
    "bugFilePath": "nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/ID.java",
    "fixPatch": "diff --git a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/ID.java b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/ID.java\nindex 92e951c..8de82d5 100644\n--- a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/ID.java\n+++ b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/io/ID.java\n@@ -75,7 +75,7 @@\n \t\t\treturn;\n \t\t}\n \t\t\n-\t\tSystem.arraycopy(bytes, 0, this.bytes, 0, SIZE);\n+\t\tSystem.arraycopy(src, 0, this.bytes, 0, SIZE);\n \t}\n \n \t/**\n",
    "projectName": "apache.flink",
    "bugLineNum": 78,
    "bugNodeStartChar": 2120,
    "bugNodeLength": 47,
    "fixLineNum": 78,
    "fixNodeStartChar": 2120,
    "fixNodeLength": 45,
    "sourceBeforeFix": "System.arraycopy(bytes,0,this.bytes,0,SIZE)",
    "sourceAfterFix": "System.arraycopy(src,0,this.bytes,0,SIZE)"
  },
  {
    "bugType": "CHANGE_UNARY_OPERATOR",
    "fixCommitSHA1": "d9a2914d0c3fc866e16a2cc4b67f83198a71a16c",
    "fixCommitParentSHA1": "f850e77d4ba974eab8ec51636ac58a90ab472d40",
    "bugFilePath": "nephele/nephele-management/src/main/java/eu/stratosphere/nephele/profiling/types/ThreadProfilingEvent.java",
    "fixPatch": "diff --git a/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/profiling/types/ThreadProfilingEvent.java b/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/profiling/types/ThreadProfilingEvent.java\nindex e57d87f..15a6a5c 100644\n--- a/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/profiling/types/ThreadProfilingEvent.java\n+++ b/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/profiling/types/ThreadProfilingEvent.java\n@@ -124,7 +124,7 @@\n \t@Override\n \tpublic boolean equals(Object obj) {\n \n-\t\tif (super.equals(obj)) {\n+\t\tif (!super.equals(obj)) {\n \t\t\treturn false;\n \t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 127,
    "bugNodeStartChar": 3579,
    "bugNodeLength": 17,
    "fixLineNum": 127,
    "fixNodeStartChar": 3579,
    "fixNodeLength": 18,
    "sourceBeforeFix": "super.equals(obj)",
    "sourceAfterFix": "!super.equals(obj)"
  },
  {
    "bugType": "CHANGE_UNARY_OPERATOR",
    "fixCommitSHA1": "f850e77d4ba974eab8ec51636ac58a90ab472d40",
    "fixCommitParentSHA1": "33252df8fd56096055225328b9721485438356ce",
    "bugFilePath": "nephele/nephele-management/src/main/java/eu/stratosphere/nephele/profiling/types/SingleInstanceProfilingEvent.java",
    "fixPatch": "diff --git a/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/profiling/types/SingleInstanceProfilingEvent.java b/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/profiling/types/SingleInstanceProfilingEvent.java\nindex e7545fa..f51658a 100644\n--- a/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/profiling/types/SingleInstanceProfilingEvent.java\n+++ b/nephele/nephele-management/src/main/java/eu/stratosphere/nephele/profiling/types/SingleInstanceProfilingEvent.java\n@@ -86,7 +86,7 @@\n \n \t\tfinal SingleInstanceProfilingEvent singleInstanceProfilingEvent = (SingleInstanceProfilingEvent) obj;\n \n-\t\tif (this.instanceName.equals(singleInstanceProfilingEvent.getInstanceName())) {\n+\t\tif (!this.instanceName.equals(singleInstanceProfilingEvent.getInstanceName())) {\n \t\t\treturn false;\n \t\t}\n \n",
    "projectName": "apache.flink",
    "bugLineNum": 89,
    "bugNodeStartChar": 2736,
    "bugNodeLength": 72,
    "fixLineNum": 89,
    "fixNodeStartChar": 2736,
    "fixNodeLength": 73,
    "sourceBeforeFix": "this.instanceName.equals(singleInstanceProfilingEvent.getInstanceName())",
    "sourceAfterFix": "!this.instanceName.equals(singleInstanceProfilingEvent.getInstanceName())"
  },
  {
    "bugType": "OVERLOAD_METHOD_DELETED_ARGS",
    "fixCommitSHA1": "78cee55f0fc0c43885fc2254e2b14939e17224ee",
    "fixCommitParentSHA1": "8e8644d0a5ec5f6be2756f7176979c643e89652d",
    "bugFilePath": "nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobFileInputVertex.java",
    "fixPatch": "diff --git a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobFileInputVertex.java b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobFileInputVertex.java\nindex 909a910..0224cc9 100644\n--- a/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobFileInputVertex.java\n+++ b/nephele/nephele-common/src/main/java/eu/stratosphere/nephele/jobgraph/JobFileInputVertex.java\n@@ -275,7 +275,7 @@\n \t\t}\n \n \t\tfinal int numSubtasks = getNumberOfSubtasks();\n-\t\tfinal List<FileInputSplit> inputSplits = new ArrayList<FileInputSplit>(numSubtasks);\n+\t\tfinal List<FileInputSplit> inputSplits = new ArrayList<FileInputSplit>();\n \n \t\t// get all the files that are involved in the splits\n \t\tList<FileStatus> files = new ArrayList<FileStatus>();\n@@ -301,7 +301,7 @@\n \t\t\t}\n \n \t\t\tfinal long minSplitSize = 1;\n-\t\t\tfinal long maxSplitSize = totalLength / numSubtasks + (totalLength % numSubtasks == 0 ? 0 : 1);\n+\t\t\tfinal long maxSplitSize = (numSubtasks < 1) ? Long.MAX_VALUE : (totalLength / numSubtasks + (totalLength % numSubtasks == 0 ? 0 : 1));\n \n \t\t\t// now that we have the files, generate the splits\n \t\t\tfor (FileStatus file : files) {\n",
    "projectName": "apache.flink",
    "bugLineNum": 278,
    "bugNodeStartChar": 7956,
    "bugNodeLength": 42,
    "fixLineNum": 278,
    "fixNodeStartChar": 7956,
    "fixNodeLength": 31,
    "sourceBeforeFix": "new ArrayList<FileInputSplit>(numSubtasks)",
    "sourceAfterFix": "new ArrayList<FileInputSplit>()"
  }
]