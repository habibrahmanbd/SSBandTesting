[{"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "1ee280f9971b12ca1a2089f59e41a1d94f885637", "fixCommitParentSHA1": "29b91727840b5d8e5c2e8c756c74fcb2bd4225bd", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/TokenizerBenchmarkTestCase.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/TokenizerBenchmarkTestCase.java b/itest/src/edu/stanford/nlp/pipeline/TokenizerBenchmarkTestCase.java\nindex b78e7ac..49d7656 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/TokenizerBenchmarkTestCase.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/TokenizerBenchmarkTestCase.java\n@@ -150,9 +150,9 @@\n             placeholderToken.setBeginPosition(beginPosition);\n             placeholderToken.setEndPosition(beginPosition + placeholderToken.word().length());\n             placeholderToken.set(TokenizerBenchmarkTestCase.MWTTokenCharacterOffsetBeginAnnotation.class,\n-                    containedToken.get(TokenizerBenchmarkTestCase.MWTTokenCharacterOffsetBeginAnnotation.class));\n+                    containedToken.beginPosition());\n             placeholderToken.set(TokenizerBenchmarkTestCase.MWTTokenCharacterOffsetEndAnnotation.class,\n-                    containedToken.get(TokenizerBenchmarkTestCase.MWTTokenCharacterOffsetEndAnnotation.class));\n+                    containedToken.endPosition());\n             placeholderToken.setIsMWT(true);\n             return placeholderToken;\n         }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 153, "bugNodeStartChar": 6327, "bugNodeLength": 91, "fixLineNum": 153, "fixNodeStartChar": 6327, "fixNodeLength": 30, "sourceBeforeFix": "containedToken.get(TokenizerBenchmarkTestCase.MWTTokenCharacterOffsetBeginAnnotation.class)", "sourceAfterFix": "containedToken.beginPosition()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "1ee280f9971b12ca1a2089f59e41a1d94f885637", "fixCommitParentSHA1": "29b91727840b5d8e5c2e8c756c74fcb2bd4225bd", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/TokenizerBenchmarkTestCase.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/TokenizerBenchmarkTestCase.java b/itest/src/edu/stanford/nlp/pipeline/TokenizerBenchmarkTestCase.java\nindex b78e7ac..49d7656 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/TokenizerBenchmarkTestCase.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/TokenizerBenchmarkTestCase.java\n@@ -150,9 +150,9 @@\n             placeholderToken.setBeginPosition(beginPosition);\n             placeholderToken.setEndPosition(beginPosition + placeholderToken.word().length());\n             placeholderToken.set(TokenizerBenchmarkTestCase.MWTTokenCharacterOffsetBeginAnnotation.class,\n-                    containedToken.get(TokenizerBenchmarkTestCase.MWTTokenCharacterOffsetBeginAnnotation.class));\n+                    containedToken.beginPosition());\n             placeholderToken.set(TokenizerBenchmarkTestCase.MWTTokenCharacterOffsetEndAnnotation.class,\n-                    containedToken.get(TokenizerBenchmarkTestCase.MWTTokenCharacterOffsetEndAnnotation.class));\n+                    containedToken.endPosition());\n             placeholderToken.setIsMWT(true);\n             return placeholderToken;\n         }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 155, "bugNodeStartChar": 6545, "bugNodeLength": 89, "fixLineNum": 155, "fixNodeStartChar": 6545, "fixNodeLength": 28, "sourceBeforeFix": "containedToken.get(TokenizerBenchmarkTestCase.MWTTokenCharacterOffsetEndAnnotation.class)", "sourceAfterFix": "containedToken.endPosition()"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "82b69a773252bdecdba84ff3ef8e0314e7d3ac4f", "fixCommitParentSHA1": "c07db3d7e249efb402e5cff26713eaeef4b2949c", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/KBPAnnotatorEnglishBenchmarkSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/KBPAnnotatorEnglishBenchmarkSlowITest.java b/itest/src/edu/stanford/nlp/pipeline/KBPAnnotatorEnglishBenchmarkSlowITest.java\nindex c4c7d01..3f0d2ed 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/KBPAnnotatorEnglishBenchmarkSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/KBPAnnotatorEnglishBenchmarkSlowITest.java\n@@ -12,7 +12,7 @@\n     // set the English specific settings\n     KBP_DOCS_DIR = \"/u/scr/nlp/data/kbp-benchmark/kbp-docs\";\n     GOLD_RELATIONS_PATH = \"/u/scr/nlp/data/kbp-benchmark/kbp-gold-relations.txt\";\n-    KBP_MINIMUM_SCORE = .445;\n+    KBP_MINIMUM_SCORE = .456;\n     docIDToText = new HashMap<String,String>();\n     docIDToRelations = new HashMap<String,Set<String>>();\n     // load the gold relations from gold relations file\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 15, "bugNodeStartChar": 423, "bugNodeLength": 24, "fixLineNum": 15, "fixNodeStartChar": 423, "fixNodeLength": 24, "sourceBeforeFix": "KBP_MINIMUM_SCORE=.445", "sourceAfterFix": "KBP_MINIMUM_SCORE=.456"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "6eb2326aa30e1308247e6e40ff77b6b4a532e62d", "fixCommitParentSHA1": "f2b3b4439a53e9c0c6afe02887fb451e2d7042a1", "bugFilePath": "src/edu/stanford/nlp/pipeline/StanfordCoreNLPServer.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/StanfordCoreNLPServer.java b/src/edu/stanford/nlp/pipeline/StanfordCoreNLPServer.java\nindex f57ea4e..d94afd5 100644\n--- a/src/edu/stanford/nlp/pipeline/StanfordCoreNLPServer.java\n+++ b/src/edu/stanford/nlp/pipeline/StanfordCoreNLPServer.java\n@@ -423,7 +423,7 @@\n     if (language != null && !\"default\".equals(language)) {\n       String languagePropertiesFile = LanguageInfo.getLanguagePropertiesFile(language);\n       if (languagePropertiesFile != null) {\n-        try (InputStream is = IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(languagePropertiesFile)){\n+        try (BufferedReader is = IOUtils.readerFromString(languagePropertiesFile)){\n           Properties languageSpecificProperties = new Properties();\n           languageSpecificProperties.load(is);\n           PropertiesUtils.overWriteProperties(props,languageSpecificProperties);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 426, "bugNodeStartChar": 20041, "bugNodeLength": 93, "fixLineNum": 426, "fixNodeStartChar": 20041, "fixNodeLength": 68, "sourceBeforeFix": "InputStream is=IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(languagePropertiesFile)", "sourceAfterFix": "BufferedReader is=IOUtils.readerFromString(languagePropertiesFile)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "0adf369fda5c2d4231881d66e3bc0bd12fb86c9a", "fixCommitParentSHA1": "efbd23012372ae763f661549a45985ace88488a5", "bugFilePath": "src/edu/stanford/nlp/examples/BasicPipelineExample.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/examples/BasicPipelineExample.java b/src/edu/stanford/nlp/examples/BasicPipelineExample.java\nindex f590e02..d1a87b4 100644\n--- a/src/edu/stanford/nlp/examples/BasicPipelineExample.java\n+++ b/src/edu/stanford/nlp/examples/BasicPipelineExample.java\n@@ -34,7 +34,7 @@\n     // examples\n \n     // 10th token of the document\n-    CoreLabel token = document.tokens().get(10);\n+    CoreLabel token = document.tokens().get(9);\n     System.out.println(\"Example: token\");\n     System.out.println(token);\n     System.out.println();\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 37, "bugNodeStartChar": 1462, "bugNodeLength": 25, "fixLineNum": 37, "fixNodeStartChar": 1462, "fixNodeLength": 24, "sourceBeforeFix": "document.tokens().get(10)", "sourceAfterFix": "document.tokens().get(9)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "92f329a43ce75e77dbfcd6387a67aa2a987a1aee", "fixCommitParentSHA1": "789c9653a9605fd28f1e4509f0f1610d20ddc5fe", "bugFilePath": "itest/src/edu/stanford/nlp/dcoref/DcorefBenchmarkSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/dcoref/DcorefBenchmarkSlowITest.java b/itest/src/edu/stanford/nlp/dcoref/DcorefBenchmarkSlowITest.java\nindex 5ef2084..c2fff3e 100644\n--- a/itest/src/edu/stanford/nlp/dcoref/DcorefBenchmarkSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/dcoref/DcorefBenchmarkSlowITest.java\n@@ -149,7 +149,7 @@\n     setLowHighExpected(lowResults, highResults, expectedResults, MUC_TP, 6245, 6255, 6250);\n     setLowHighExpected(lowResults, highResults, expectedResults, MUC_F1, 60.65, 60.7, 60.66);\n \n-    setLowHighExpected(lowResults, highResults, expectedResults, BCUBED_TP, 12440, 12450, 12451.87);\n+    setLowHighExpected(lowResults, highResults, expectedResults, BCUBED_TP, 12440, 12451.87, 12451.87);\n     setLowHighExpected(lowResults, highResults, expectedResults, BCUBED_F1, 70.75, 70.85, 70.80);\n \n     setLowHighExpected(lowResults, highResults, expectedResults, CEAFM_TP, 10915, 10930, 10920);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 152, "bugNodeStartChar": 6295, "bugNodeLength": 95, "fixLineNum": 152, "fixNodeStartChar": 6295, "fixNodeLength": 98, "sourceBeforeFix": "setLowHighExpected(lowResults,highResults,expectedResults,BCUBED_TP,12440,12450,12451.87)", "sourceAfterFix": "setLowHighExpected(lowResults,highResults,expectedResults,BCUBED_TP,12440,12451.87,12451.87)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "a5f1f8199f76349cbcb39ba969e51dadc65f2c63", "fixCommitParentSHA1": "851549fd18eacce994d68cf6a6b52607937463bc", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/POSTaggerBenchmarkITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/POSTaggerBenchmarkITest.java b/itest/src/edu/stanford/nlp/pipeline/POSTaggerBenchmarkITest.java\nindex ce508cb..9bbcaa9 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/POSTaggerBenchmarkITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/POSTaggerBenchmarkITest.java\n@@ -82,7 +82,7 @@\n     String englishPOSTestPath = \"/u/nlp/data/pos-tagger/english/test-wsj-22-24\";\n     List<String> sentences = readInPOSData(englishPOSTestPath);\n     double ENGLISH_BIDIRECTIONAL_TOKEN_ACCURACY = .972;\n-    double ENGLISH_BIDIRECTIONAL_SENTENCE_ACCURACY = .564;\n+    double ENGLISH_BIDIRECTIONAL_SENTENCE_ACCURACY = .563;\n     runPOSTest(sentences, \"_\", englishBiDirectionalPipeline,\n         ENGLISH_BIDIRECTIONAL_TOKEN_ACCURACY, ENGLISH_BIDIRECTIONAL_SENTENCE_ACCURACY,\n         \"English BiDirectional\", false);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 85, "bugNodeStartChar": 3608, "bugNodeLength": 46, "fixLineNum": 85, "fixNodeStartChar": 3608, "fixNodeLength": 46, "sourceBeforeFix": "ENGLISH_BIDIRECTIONAL_SENTENCE_ACCURACY=.564", "sourceAfterFix": "ENGLISH_BIDIRECTIONAL_SENTENCE_ACCURACY=.563"}, {"bugType": "MORE_SPECIFIC_IF", "fixCommitSHA1": "f65ac8922236a90efb7bacb2e20239f54b53ecc1", "fixCommitParentSHA1": "8fcc588d6271e070c85180806b8f0edf4e53349e", "bugFilePath": "src/edu/stanford/nlp/pipeline/EntityMentionsAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/EntityMentionsAnnotator.java b/src/edu/stanford/nlp/pipeline/EntityMentionsAnnotator.java\nindex ced9ab5..c67b645 100644\n--- a/src/edu/stanford/nlp/pipeline/EntityMentionsAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/EntityMentionsAnnotator.java\n@@ -233,7 +233,7 @@\n       Map<String,Double> labelProbsForToken =\n           token.get(CoreAnnotations.NamedEntityTagProbsAnnotation.class);\n       for (String label : labelProbsForToken.keySet()) {\n-        if (labelProbsForToken.get(label) < entityLabelProbVals.get(label))\n+        if (entityLabelProbVals.containsKey(label) && labelProbsForToken.get(label) < entityLabelProbVals.get(label))\n           entityLabelProbVals.put(label, labelProbsForToken.get(label));\n       }\n     }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 236, "bugNodeStartChar": 10205, "bugNodeLength": 62, "fixLineNum": 236, "fixNodeStartChar": 10205, "fixNodeLength": 104, "sourceBeforeFix": "labelProbsForToken.get(label) < entityLabelProbVals.get(label)", "sourceAfterFix": "entityLabelProbVals.containsKey(label) && labelProbsForToken.get(label) < entityLabelProbVals.get(label)"}, {"bugType": "MORE_SPECIFIC_IF", "fixCommitSHA1": "f9e5e4396a481051abc07a5fd6a456310c89d980", "fixCommitParentSHA1": "3da8b2db909e9fb6831549855399cf01c04a0377", "bugFilePath": "src/edu/stanford/nlp/pipeline/CoreDocument.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/CoreDocument.java b/src/edu/stanford/nlp/pipeline/CoreDocument.java\nindex 036155d..ac5727b 100644\n--- a/src/edu/stanford/nlp/pipeline/CoreDocument.java\n+++ b/src/edu/stanford/nlp/pipeline/CoreDocument.java\n@@ -34,7 +34,7 @@\n     if (this.annotationDocument.get(CoreAnnotations.SentencesAnnotation.class) != null) {\n       wrapSentences();\n       // if there are entity mentions, build a document wide list\n-      if (sentences.get(0).entityMentions() != null) {\n+      if (sentences.size() > 0 && sentences.get(0).entityMentions() != null) {\n         buildDocumentEntityMentionsList();\n       }\n       // if there are quotes, build a document wide list\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 37, "bugNodeStartChar": 1072, "bugNodeLength": 41, "fixLineNum": 37, "fixNodeStartChar": 1072, "fixNodeLength": 65, "sourceBeforeFix": "sentences.get(0).entityMentions() != null", "sourceAfterFix": "sentences.size() > 0 && sentences.get(0).entityMentions() != null"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "e4c5baf756708048420bb50e91747609394bbd45", "fixCommitParentSHA1": "a522019050c3b418ba6846cf084c744c7fcfd7db", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/TokenBeginEndAnnotationITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/TokenBeginEndAnnotationITest.java b/itest/src/edu/stanford/nlp/pipeline/TokenBeginEndAnnotationITest.java\nindex 16e9864..5f450c5 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/TokenBeginEndAnnotationITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/TokenBeginEndAnnotationITest.java\n@@ -138,7 +138,7 @@\n     assertTrue(!tokenIndexTwoNinetyThree.isNewline());\n     CoreLabel tokenIndexFiveFortyTwo = xmlDocAnnotation.get(CoreAnnotations.TokensAnnotation.class).get(542);\n     assertEquals(\"location\", tokenIndexFiveFortyTwo.originalText());\n-    CoreLabel tokenIndexFiveFiftyFour = xmlDocAnnotation.get(CoreAnnotations.TokensAnnotation.class).get(554);\n+    CoreLabel tokenIndexFiveFiftyFour = xmlDocAnnotation.get(CoreAnnotations.TokensAnnotation.class).get(543);\n     assertEquals(\".\", tokenIndexFiveFiftyFour.originalText());\n   }\n \n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 141, "bugNodeStartChar": 7674, "bugNodeLength": 69, "fixLineNum": 141, "fixNodeStartChar": 7674, "fixNodeLength": 69, "sourceBeforeFix": "xmlDocAnnotation.get(CoreAnnotations.TokensAnnotation.class).get(554)", "sourceAfterFix": "xmlDocAnnotation.get(CoreAnnotations.TokensAnnotation.class).get(543)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "a13c4373a5e57d322b143516b493e84f14909fec", "fixCommitParentSHA1": "8a9bf436f692efc084f4cf43b3531cef81324696", "bugFilePath": "test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java", "fixPatch": "diff --git a/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java b/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java\nindex 3b0848a..b11f7ab 100644\n--- a/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java\n+++ b/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java\n@@ -92,7 +92,7 @@\n     // make sure that there are the correct # of tokens\n     // (does NOT contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(15, tokens.size());\n+    assertEquals(13, tokens.size());\n   }\n \n   @Test\n@@ -128,7 +128,7 @@\n \n     // make sure that there are the correct # of tokens (does contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(12, tokens.size());\n+    assertEquals(9, tokens.size());\n   }\n \n   @Test\n@@ -149,7 +149,7 @@\n \n     // make sure that there are the correct # of tokens (does contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(12, tokens.size());\n+    assertEquals(9, tokens.size());\n \n     List<CoreLabel> sentenceTwoTokens = sentences.get(1).get(CoreAnnotations.TokensAnnotation.class);\n     String sentenceTwo = SentenceUtils.listToString(sentenceTwoTokens);\n@@ -173,7 +173,7 @@\n \n     // make sure that there are the correct # of tokens (count does contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(12, tokens.size());\n+    assertEquals(9, tokens.size());\n \n     for (int i = 0; i < Math.min(sents.length, sentences.size()); i++) {\n       CoreMap sentence = sentences.get(i);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 95, "bugNodeStartChar": 3496, "bugNodeLength": 31, "fixLineNum": 95, "fixNodeStartChar": 3496, "fixNodeLength": 31, "sourceBeforeFix": "assertEquals(15,tokens.size())", "sourceAfterFix": "assertEquals(13,tokens.size())"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "a13c4373a5e57d322b143516b493e84f14909fec", "fixCommitParentSHA1": "8a9bf436f692efc084f4cf43b3531cef81324696", "bugFilePath": "test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java", "fixPatch": "diff --git a/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java b/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java\nindex 3b0848a..b11f7ab 100644\n--- a/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java\n+++ b/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java\n@@ -92,7 +92,7 @@\n     // make sure that there are the correct # of tokens\n     // (does NOT contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(15, tokens.size());\n+    assertEquals(13, tokens.size());\n   }\n \n   @Test\n@@ -128,7 +128,7 @@\n \n     // make sure that there are the correct # of tokens (does contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(12, tokens.size());\n+    assertEquals(9, tokens.size());\n   }\n \n   @Test\n@@ -149,7 +149,7 @@\n \n     // make sure that there are the correct # of tokens (does contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(12, tokens.size());\n+    assertEquals(9, tokens.size());\n \n     List<CoreLabel> sentenceTwoTokens = sentences.get(1).get(CoreAnnotations.TokensAnnotation.class);\n     String sentenceTwo = SentenceUtils.listToString(sentenceTwoTokens);\n@@ -173,7 +173,7 @@\n \n     // make sure that there are the correct # of tokens (count does contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(12, tokens.size());\n+    assertEquals(9, tokens.size());\n \n     for (int i = 0; i < Math.min(sents.length, sentences.size()); i++) {\n       CoreMap sentence = sentences.get(i);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 131, "bugNodeStartChar": 5003, "bugNodeLength": 31, "fixLineNum": 131, "fixNodeStartChar": 5003, "fixNodeLength": 30, "sourceBeforeFix": "assertEquals(12,tokens.size())", "sourceAfterFix": "assertEquals(9,tokens.size())"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "a13c4373a5e57d322b143516b493e84f14909fec", "fixCommitParentSHA1": "8a9bf436f692efc084f4cf43b3531cef81324696", "bugFilePath": "test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java", "fixPatch": "diff --git a/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java b/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java\nindex 3b0848a..b11f7ab 100644\n--- a/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java\n+++ b/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java\n@@ -92,7 +92,7 @@\n     // make sure that there are the correct # of tokens\n     // (does NOT contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(15, tokens.size());\n+    assertEquals(13, tokens.size());\n   }\n \n   @Test\n@@ -128,7 +128,7 @@\n \n     // make sure that there are the correct # of tokens (does contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(12, tokens.size());\n+    assertEquals(9, tokens.size());\n   }\n \n   @Test\n@@ -149,7 +149,7 @@\n \n     // make sure that there are the correct # of tokens (does contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(12, tokens.size());\n+    assertEquals(9, tokens.size());\n \n     List<CoreLabel> sentenceTwoTokens = sentences.get(1).get(CoreAnnotations.TokensAnnotation.class);\n     String sentenceTwo = SentenceUtils.listToString(sentenceTwoTokens);\n@@ -173,7 +173,7 @@\n \n     // make sure that there are the correct # of tokens (count does contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(12, tokens.size());\n+    assertEquals(9, tokens.size());\n \n     for (int i = 0; i < Math.min(sents.length, sentences.size()); i++) {\n       CoreMap sentence = sentences.get(i);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 152, "bugNodeStartChar": 5888, "bugNodeLength": 31, "fixLineNum": 152, "fixNodeStartChar": 5888, "fixNodeLength": 30, "sourceBeforeFix": "assertEquals(12,tokens.size())", "sourceAfterFix": "assertEquals(9,tokens.size())"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "a13c4373a5e57d322b143516b493e84f14909fec", "fixCommitParentSHA1": "8a9bf436f692efc084f4cf43b3531cef81324696", "bugFilePath": "test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java", "fixPatch": "diff --git a/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java b/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java\nindex 3b0848a..b11f7ab 100644\n--- a/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java\n+++ b/test/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotatorTest.java\n@@ -92,7 +92,7 @@\n     // make sure that there are the correct # of tokens\n     // (does NOT contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(15, tokens.size());\n+    assertEquals(13, tokens.size());\n   }\n \n   @Test\n@@ -128,7 +128,7 @@\n \n     // make sure that there are the correct # of tokens (does contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(12, tokens.size());\n+    assertEquals(9, tokens.size());\n   }\n \n   @Test\n@@ -149,7 +149,7 @@\n \n     // make sure that there are the correct # of tokens (does contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(12, tokens.size());\n+    assertEquals(9, tokens.size());\n \n     List<CoreLabel> sentenceTwoTokens = sentences.get(1).get(CoreAnnotations.TokensAnnotation.class);\n     String sentenceTwo = SentenceUtils.listToString(sentenceTwoTokens);\n@@ -173,7 +173,7 @@\n \n     // make sure that there are the correct # of tokens (count does contain NL tokens)\n     List<CoreLabel> tokens = document1.get(CoreAnnotations.TokensAnnotation.class);\n-    assertEquals(12, tokens.size());\n+    assertEquals(9, tokens.size());\n \n     for (int i = 0; i < Math.min(sents.length, sentences.size()); i++) {\n       CoreMap sentence = sentences.get(i);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 176, "bugNodeStartChar": 7001, "bugNodeLength": 31, "fixLineNum": 176, "fixNodeStartChar": 7001, "fixNodeLength": 30, "sourceBeforeFix": "assertEquals(12,tokens.size())", "sourceAfterFix": "assertEquals(9,tokens.size())"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "2a9e6b4fb3f30995b96623b7ab10d55de79153df", "fixCommitParentSHA1": "8f1884f992069a6f353107ad34ca1266b0a75ff3", "bugFilePath": "src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotator.java b/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotator.java\nindex f8e0c1d..a0d4c51 100644\n--- a/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotator.java\n@@ -358,7 +358,10 @@\n         CoreAnnotations.TokensAnnotation.class,\n         CoreAnnotations.CharacterOffsetBeginAnnotation.class,\n         CoreAnnotations.CharacterOffsetEndAnnotation.class,\n-        CoreAnnotations.IsNewlineAnnotation.class\n+        CoreAnnotations.IsNewlineAnnotation.class,\n+        CoreAnnotations.TokenBeginAnnotation.class,\n+        CoreAnnotations.TokenEndAnnotation.class,\n+        CoreAnnotations.OriginalTextAnnotation.class\n     )));\n   }\n \n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 356, "bugNodeStartChar": 16420, "bugNodeLength": 286, "fixLineNum": 356, "fixNodeStartChar": 16420, "fixNodeLength": 442, "sourceBeforeFix": "Arrays.asList(CoreAnnotations.TextAnnotation.class,CoreAnnotations.TokensAnnotation.class,CoreAnnotations.CharacterOffsetBeginAnnotation.class,CoreAnnotations.CharacterOffsetEndAnnotation.class,CoreAnnotations.IsNewlineAnnotation.class)", "sourceAfterFix": "Arrays.asList(CoreAnnotations.TextAnnotation.class,CoreAnnotations.TokensAnnotation.class,CoreAnnotations.CharacterOffsetBeginAnnotation.class,CoreAnnotations.CharacterOffsetEndAnnotation.class,CoreAnnotations.IsNewlineAnnotation.class,CoreAnnotations.TokenBeginAnnotation.class,CoreAnnotations.TokenEndAnnotation.class,CoreAnnotations.OriginalTextAnnotation.class)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "8783e22c53a444ba096e8e7e814f7f509b1edcf4", "fixCommitParentSHA1": "8ef406c94e97ea01f9b68711db679e784d46aab7", "bugFilePath": "src/edu/stanford/nlp/pipeline/NERCombinerAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/NERCombinerAnnotator.java b/src/edu/stanford/nlp/pipeline/NERCombinerAnnotator.java\nindex f524de8..7e4053e 100644\n--- a/src/edu/stanford/nlp/pipeline/NERCombinerAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/NERCombinerAnnotator.java\n@@ -363,7 +363,8 @@\n           CoreAnnotations.IndexAnnotation.class,\n           CoreAnnotations.OriginalTextAnnotation.class,\n           CoreAnnotations.SentenceIndexAnnotation.class,\n-          CoreAnnotations.IsNewlineAnnotation.class\n+          CoreAnnotations.IsNewlineAnnotation.class,\n+          CoreAnnotations.TokenIndexAnnotation.class\n       )));\n     } else {\n       return Collections.unmodifiableSet(new HashSet<>(Arrays.asList(\n@@ -379,7 +380,8 @@\n           CoreAnnotations.IndexAnnotation.class,\n           CoreAnnotations.OriginalTextAnnotation.class,\n           CoreAnnotations.SentenceIndexAnnotation.class,\n-          CoreAnnotations.IsNewlineAnnotation.class\n+          CoreAnnotations.IsNewlineAnnotation.class,\n+          CoreAnnotations.TokenIndexAnnotation.class\n       )));\n     }\n   }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 351, "bugNodeStartChar": 14204, "bugNodeLength": 823, "fixLineNum": 351, "fixNodeStartChar": 14204, "fixNodeLength": 877, "sourceBeforeFix": "Arrays.asList(CoreAnnotations.TextAnnotation.class,CoreAnnotations.TokensAnnotation.class,CoreAnnotations.SentencesAnnotation.class,CoreAnnotations.CharacterOffsetBeginAnnotation.class,CoreAnnotations.CharacterOffsetEndAnnotation.class,CoreAnnotations.PartOfSpeechAnnotation.class,CoreAnnotations.LemmaAnnotation.class,CoreAnnotations.BeforeAnnotation.class,CoreAnnotations.AfterAnnotation.class,CoreAnnotations.TokenBeginAnnotation.class,CoreAnnotations.TokenEndAnnotation.class,CoreAnnotations.IndexAnnotation.class,CoreAnnotations.OriginalTextAnnotation.class,CoreAnnotations.SentenceIndexAnnotation.class,CoreAnnotations.IsNewlineAnnotation.class)", "sourceAfterFix": "Arrays.asList(CoreAnnotations.TextAnnotation.class,CoreAnnotations.TokensAnnotation.class,CoreAnnotations.SentencesAnnotation.class,CoreAnnotations.CharacterOffsetBeginAnnotation.class,CoreAnnotations.CharacterOffsetEndAnnotation.class,CoreAnnotations.PartOfSpeechAnnotation.class,CoreAnnotations.LemmaAnnotation.class,CoreAnnotations.BeforeAnnotation.class,CoreAnnotations.AfterAnnotation.class,CoreAnnotations.TokenBeginAnnotation.class,CoreAnnotations.TokenEndAnnotation.class,CoreAnnotations.IndexAnnotation.class,CoreAnnotations.OriginalTextAnnotation.class,CoreAnnotations.SentenceIndexAnnotation.class,CoreAnnotations.IsNewlineAnnotation.class,CoreAnnotations.TokenIndexAnnotation.class)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "8783e22c53a444ba096e8e7e814f7f509b1edcf4", "fixCommitParentSHA1": "8ef406c94e97ea01f9b68711db679e784d46aab7", "bugFilePath": "src/edu/stanford/nlp/pipeline/NERCombinerAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/NERCombinerAnnotator.java b/src/edu/stanford/nlp/pipeline/NERCombinerAnnotator.java\nindex f524de8..7e4053e 100644\n--- a/src/edu/stanford/nlp/pipeline/NERCombinerAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/NERCombinerAnnotator.java\n@@ -363,7 +363,8 @@\n           CoreAnnotations.IndexAnnotation.class,\n           CoreAnnotations.OriginalTextAnnotation.class,\n           CoreAnnotations.SentenceIndexAnnotation.class,\n-          CoreAnnotations.IsNewlineAnnotation.class\n+          CoreAnnotations.IsNewlineAnnotation.class,\n+          CoreAnnotations.TokenIndexAnnotation.class\n       )));\n     } else {\n       return Collections.unmodifiableSet(new HashSet<>(Arrays.asList(\n@@ -379,7 +380,8 @@\n           CoreAnnotations.IndexAnnotation.class,\n           CoreAnnotations.OriginalTextAnnotation.class,\n           CoreAnnotations.SentenceIndexAnnotation.class,\n-          CoreAnnotations.IsNewlineAnnotation.class\n+          CoreAnnotations.IsNewlineAnnotation.class,\n+          CoreAnnotations.TokenIndexAnnotation.class\n       )));\n     }\n   }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 369, "bugNodeStartChar": 15099, "bugNodeLength": 718, "fixLineNum": 369, "fixNodeStartChar": 15099, "fixNodeLength": 772, "sourceBeforeFix": "Arrays.asList(CoreAnnotations.TextAnnotation.class,CoreAnnotations.TokensAnnotation.class,CoreAnnotations.SentencesAnnotation.class,CoreAnnotations.CharacterOffsetBeginAnnotation.class,CoreAnnotations.CharacterOffsetEndAnnotation.class,CoreAnnotations.BeforeAnnotation.class,CoreAnnotations.AfterAnnotation.class,CoreAnnotations.TokenBeginAnnotation.class,CoreAnnotations.TokenEndAnnotation.class,CoreAnnotations.IndexAnnotation.class,CoreAnnotations.OriginalTextAnnotation.class,CoreAnnotations.SentenceIndexAnnotation.class,CoreAnnotations.IsNewlineAnnotation.class)", "sourceAfterFix": "Arrays.asList(CoreAnnotations.TextAnnotation.class,CoreAnnotations.TokensAnnotation.class,CoreAnnotations.SentencesAnnotation.class,CoreAnnotations.CharacterOffsetBeginAnnotation.class,CoreAnnotations.CharacterOffsetEndAnnotation.class,CoreAnnotations.BeforeAnnotation.class,CoreAnnotations.AfterAnnotation.class,CoreAnnotations.TokenBeginAnnotation.class,CoreAnnotations.TokenEndAnnotation.class,CoreAnnotations.IndexAnnotation.class,CoreAnnotations.OriginalTextAnnotation.class,CoreAnnotations.SentenceIndexAnnotation.class,CoreAnnotations.IsNewlineAnnotation.class,CoreAnnotations.TokenIndexAnnotation.class)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "8783e22c53a444ba096e8e7e814f7f509b1edcf4", "fixCommitParentSHA1": "8ef406c94e97ea01f9b68711db679e784d46aab7", "bugFilePath": "src/edu/stanford/nlp/pipeline/TokenizerAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/TokenizerAnnotator.java b/src/edu/stanford/nlp/pipeline/TokenizerAnnotator.java\nindex 281fb2b..da4b931 100644\n--- a/src/edu/stanford/nlp/pipeline/TokenizerAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/TokenizerAnnotator.java\n@@ -370,7 +370,8 @@\n         CoreAnnotations.IndexAnnotation.class,\n         CoreAnnotations.OriginalTextAnnotation.class,\n         CoreAnnotations.ValueAnnotation.class,\n-        CoreAnnotations.IsNewlineAnnotation.class\n+        CoreAnnotations.IsNewlineAnnotation.class,\n+        CoreAnnotations.TokenIndexAnnotation.class\n     ));\n   }\n \n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 360, "bugNodeStartChar": 13070, "bugNodeLength": 681, "fixLineNum": 360, "fixNodeStartChar": 13070, "fixNodeLength": 733, "sourceBeforeFix": "Arrays.asList(CoreAnnotations.TextAnnotation.class,CoreAnnotations.TokensAnnotation.class,CoreAnnotations.CharacterOffsetBeginAnnotation.class,CoreAnnotations.CharacterOffsetEndAnnotation.class,CoreAnnotations.BeforeAnnotation.class,CoreAnnotations.AfterAnnotation.class,CoreAnnotations.TokenBeginAnnotation.class,CoreAnnotations.TokenEndAnnotation.class,CoreAnnotations.PositionAnnotation.class,CoreAnnotations.IndexAnnotation.class,CoreAnnotations.OriginalTextAnnotation.class,CoreAnnotations.ValueAnnotation.class,CoreAnnotations.IsNewlineAnnotation.class)", "sourceAfterFix": "Arrays.asList(CoreAnnotations.TextAnnotation.class,CoreAnnotations.TokensAnnotation.class,CoreAnnotations.CharacterOffsetBeginAnnotation.class,CoreAnnotations.CharacterOffsetEndAnnotation.class,CoreAnnotations.BeforeAnnotation.class,CoreAnnotations.AfterAnnotation.class,CoreAnnotations.TokenBeginAnnotation.class,CoreAnnotations.TokenEndAnnotation.class,CoreAnnotations.PositionAnnotation.class,CoreAnnotations.IndexAnnotation.class,CoreAnnotations.OriginalTextAnnotation.class,CoreAnnotations.ValueAnnotation.class,CoreAnnotations.IsNewlineAnnotation.class,CoreAnnotations.TokenIndexAnnotation.class)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "a599db297dce998b49821c76fef2e711b8962cab", "fixCommitParentSHA1": "b66b43ec8d74ef6bebb45300c53fcf9832268462", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\nindex d93bb67..d0d0350 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n@@ -330,7 +330,7 @@\n \n     // Check size\n     assertTrue(\"\" + compressedProto.length, compressedProto.length < 395000);\n-    assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 2560000);\n+    assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 2570000);\n   }\n \n   @Test\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 333, "bugNodeStartChar": 45755, "bugNodeLength": 34, "fixLineNum": 333, "fixNodeStartChar": 45755, "fixNodeLength": 34, "sourceBeforeFix": "uncompressedProto.length < 2560000", "sourceAfterFix": "uncompressedProto.length < 2570000"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "377e56534d9879079209b2ca99a28506f55d52e8", "fixCommitParentSHA1": "846d9d0495802867239870f13f05fdcec7ceabe1", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\nindex 20f5424..4e31f30 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n@@ -330,7 +330,7 @@\n \n     // Check size\n     assertTrue(\"\" + compressedProto.length, compressedProto.length < 392000);\n-    assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 2550000);\n+    assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 2560000);\n   }\n \n   @Test\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 333, "bugNodeStartChar": 45755, "bugNodeLength": 34, "fixLineNum": 333, "fixNodeStartChar": 45755, "fixNodeLength": 34, "sourceBeforeFix": "uncompressedProto.length < 2550000", "sourceAfterFix": "uncompressedProto.length < 2560000"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "a833198dd3994b8496f9a42bc1034a961399e596", "fixCommitParentSHA1": "f5e4bf88e49a8b7b4a5c4e633dd4c159e9c7ec4d", "bugFilePath": "src/edu/stanford/nlp/pipeline/Annotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/Annotator.java b/src/edu/stanford/nlp/pipeline/Annotator.java\nindex 1f216f3..65d08f1 100644\n--- a/src/edu/stanford/nlp/pipeline/Annotator.java\n+++ b/src/edu/stanford/nlp/pipeline/Annotator.java\n@@ -140,7 +140,7 @@\n     put(STANFORD_DEPENDENCIES,             new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS)));\n     put(STANFORD_NATLOG,                   new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_DEPENDENCIES)));\n     put(STANFORD_OPENIE,                   new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_DEPENDENCIES, STANFORD_NATLOG)));\n-    put(STANFORD_QUOTE,                    new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT)));\n+    put(STANFORD_QUOTE,                    new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_NER)));\n     put(STANFORD_QUOTE_ATTRIBUTION,        new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_NER, STANFORD_COREF_MENTION, STANFORD_DEPENDENCIES, STANFORD_QUOTE)));\n     put(STANFORD_UD_FEATURES,              new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_DEPENDENCIES)));\n     put(STANFORD_LINK,                     new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_DEPENDENCIES, STANFORD_LEMMA, STANFORD_NER, STANFORD_ENTITY_MENTIONS)));\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 143, "bugNodeStartChar": 8136, "bugNodeLength": 49, "fixLineNum": 143, "fixNodeStartChar": 8136, "fixNodeLength": 93, "sourceBeforeFix": "Arrays.asList(STANFORD_TOKENIZE,STANFORD_SSPLIT)", "sourceAfterFix": "Arrays.asList(STANFORD_TOKENIZE,STANFORD_SSPLIT,STANFORD_POS,STANFORD_LEMMA,STANFORD_NER)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "a388f50557e5c372fd55896f2eaebc1404ee499c", "fixCommitParentSHA1": "1504cc52bc7ebb582a51de0951314d9741a29f85", "bugFilePath": "src/edu/stanford/nlp/pipeline/Annotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/Annotator.java b/src/edu/stanford/nlp/pipeline/Annotator.java\nindex 91c72f6..0975f1c 100644\n--- a/src/edu/stanford/nlp/pipeline/Annotator.java\n+++ b/src/edu/stanford/nlp/pipeline/Annotator.java\n@@ -132,7 +132,7 @@\n     put(STANFORD_TRUECASE,                 new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT)));\n     put(STANFORD_PARSE,                    new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS)));\n     put(STANFORD_DETERMINISTIC_COREF,      new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_NER, STANFORD_PARSE)));\n-    put(STANFORD_COREF,                    new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_NER)));\n+    put(STANFORD_COREF,                    new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_NER, STANFORD_DEPENDENCIES)));\n     put(STANFORD_COREF_MENTION,                  new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_NER, STANFORD_DEPENDENCIES)));\n     put(STANFORD_RELATION,                 new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_NER, STANFORD_PARSE, STANFORD_DEPENDENCIES)));\n     put(STANFORD_SENTIMENT,                new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_PARSE)));\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 135, "bugNodeStartChar": 6851, "bugNodeLength": 93, "fixLineNum": 135, "fixNodeStartChar": 6851, "fixNodeLength": 116, "sourceBeforeFix": "Arrays.asList(STANFORD_TOKENIZE,STANFORD_SSPLIT,STANFORD_POS,STANFORD_LEMMA,STANFORD_NER)", "sourceAfterFix": "Arrays.asList(STANFORD_TOKENIZE,STANFORD_SSPLIT,STANFORD_POS,STANFORD_LEMMA,STANFORD_NER,STANFORD_DEPENDENCIES)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "08119404453c2caafcff29de60ea1e523bc60616", "fixCommitParentSHA1": "9d85ae2d9245632eea8a63a4a5e77b9e0081a2c6", "bugFilePath": "itest/src/edu/stanford/nlp/ie/crf/TrainCRFClassifierSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/ie/crf/TrainCRFClassifierSlowITest.java b/itest/src/edu/stanford/nlp/ie/crf/TrainCRFClassifierSlowITest.java\nindex f6f1fcd..45c1d08 100644\n--- a/itest/src/edu/stanford/nlp/ie/crf/TrainCRFClassifierSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/ie/crf/TrainCRFClassifierSlowITest.java\n@@ -42,7 +42,7 @@\n     double p = scanner.nextDouble();\n     Assert.assertEquals(\"Precision outside target range\", 0.8364, p, 0.001);\n     double r = scanner.nextDouble();\n-    Assert.assertEquals(\"Recall outside target range\", 0.6924, r, 0.001);\n+    Assert.assertEquals(\"Recall outside target range\", 0.691, r, 0.001);\n     double f1 = scanner.nextDouble();\n     Assert.assertEquals(\"Precision outside target range\", 0.7576, f1, 0.001);\n   }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 45, "bugNodeStartChar": 1821, "bugNodeLength": 68, "fixLineNum": 45, "fixNodeStartChar": 1821, "fixNodeLength": 67, "sourceBeforeFix": "Assert.assertEquals(\"Recall outside target range\",0.6924,r,0.001)", "sourceAfterFix": "Assert.assertEquals(\"Recall outside target range\",0.691,r,0.001)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "9d85ae2d9245632eea8a63a4a5e77b9e0081a2c6", "fixCommitParentSHA1": "ba56fb8ada30fc4dc0f5d5429cf2ba8781768e0f", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\nindex 0c960e3..b541e60 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n@@ -330,7 +330,7 @@\n \n     // Check size\n     assertTrue(\"\" + compressedProto.length, compressedProto.length < 391000);\n-    assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 2500000);\n+    assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 2550000);\n   }\n \n   @Test\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 333, "bugNodeStartChar": 45714, "bugNodeLength": 34, "fixLineNum": 333, "fixNodeStartChar": 45714, "fixNodeLength": 34, "sourceBeforeFix": "uncompressedProto.length < 2500000", "sourceAfterFix": "uncompressedProto.length < 2550000"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "f521ac79d4dfc08f8ef542e06ce0bbc3e9ac833b", "fixCommitParentSHA1": "a3f9d7faa5e102d8f11c5ffaacdfee1dea2714ba", "bugFilePath": "src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java b/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\nindex 8523a93..871739a 100644\n--- a/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\n+++ b/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\n@@ -83,7 +83,7 @@\n   }\n   public double[] initial(boolean useRandomSeed) {\n     Random randToUse = useRandomSeed ? new Random() : rand;\n-    return initial(rand);\n+    return initial(randToUse);\n   }\n \n   public double[] initial(Random randGen) {\n@@ -223,7 +223,7 @@\n       featureVal3DArr = featureVal[docIndex];\n     }\n     // make a clique tree for this document\n-    CRFCliqueTree cliqueTree = CRFCliqueTree.getCalibratedCliqueTree(docData, labelIndices, numClasses, classIndex, backgroundSymbol, cliquePotentialFunc, featureVal3DArr);\n+    CRFCliqueTree<String> cliqueTree = CRFCliqueTree.getCalibratedCliqueTree(docData, labelIndices, numClasses, classIndex, backgroundSymbol, cliquePotentialFunc, featureVal3DArr);\n \n     double prob = 0.0;\n     if (doValueCalc) {\n@@ -238,7 +238,7 @@\n   }\n \n   /** Compute the expected counts for this document, which we will need to compute the derivative. */\n-  protected void documentExpectedCounts(double[][] E, int[][][] docData, double[][][] featureVal3DArr, CRFCliqueTree cliqueTree) {\n+  protected void documentExpectedCounts(double[][] E, int[][][] docData, double[][][] featureVal3DArr, CRFCliqueTree<String> cliqueTree) {\n     // iterate over the positions in this document\n     for (int i = 0; i < docData.length; i++) {\n       // for each possible clique at this position\n@@ -261,7 +261,7 @@\n   }\n \n   /** Compute the log probability of the document given the model with the parameters x. */\n-  private double documentLogProbability(int[][][] docData, int docIndex, CRFCliqueTree cliqueTree) {\n+  private double documentLogProbability(int[][][] docData, int docIndex, CRFCliqueTree<String> cliqueTree) {\n     int[] docLabels = labels[docIndex];\n     int[] given = new int[window - 1];\n     Arrays.fill(given, classIndex.indexOf(backgroundSymbol));\n@@ -671,7 +671,7 @@\n   }\n \n \n-  protected Pair<double[][][], double[][][]> getCondProbs(CRFCliqueTree cTree, int[][][] docData) {\n+  protected Pair<double[][][], double[][][]> getCondProbs(CRFCliqueTree<String> cTree, int[][][] docData) {\n     // first index position is curr index, second index curr-class, third index prev-class\n     // e.g. [1][2][3] means curr is at position 1 with class 2, prev is at position 0 with class 3\n     double[][][] prevGivenCurr = new double[docData.length][][];\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 86, "bugNodeStartChar": 3384, "bugNodeLength": 13, "fixLineNum": 86, "fixNodeStartChar": 3384, "fixNodeLength": 18, "sourceBeforeFix": "initial(rand)", "sourceAfterFix": "initial(randToUse)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "ad75cdf582309f67d7e5d5bcb71035c67a018b6f", "fixCommitParentSHA1": "5bc2200d0f8ff81f585fbb71d95c3266a2789f5b", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\nindex 81fe42a..e26b25e 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n@@ -322,7 +322,7 @@\n \n     // Check size\n     assertTrue(\"\" + compressedProto.length, compressedProto.length < 391000);\n-    assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 2100000);\n+    assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 2500000);\n   }\n \n   @Test\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 325, "bugNodeStartChar": 45503, "bugNodeLength": 34, "fixLineNum": 325, "fixNodeStartChar": 45503, "fixNodeLength": 34, "sourceBeforeFix": "uncompressedProto.length < 2100000", "sourceAfterFix": "uncompressedProto.length < 2500000"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "5b669bd30cef473ddf2c0c0318652a938d8d3428", "fixCommitParentSHA1": "bb7650a3d71534f3d6ceec85fdabd395417aeffb", "bugFilePath": "src/edu/stanford/nlp/pipeline/StanfordCoreNLPClient.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/StanfordCoreNLPClient.java b/src/edu/stanford/nlp/pipeline/StanfordCoreNLPClient.java\nindex 923242d..d6ee60a 100644\n--- a/src/edu/stanford/nlp/pipeline/StanfordCoreNLPClient.java\n+++ b/src/edu/stanford/nlp/pipeline/StanfordCoreNLPClient.java\n@@ -35,11 +35,12 @@\n   private static final Redwood.RedwoodChannels log = Redwood.channels(StanfordCoreNLPClient.class);\n \n   /** A simple URL spec, for parsing backend URLs */\n-  private static final Pattern URL_PATTERN = Pattern.compile(\"(?:(https?)://)?([^:]+):([0-9]+)?\");\n+  private static final Pattern URL_PATTERN = Pattern.compile(\"(?:(https?)://)?([^:]+)(?::([0-9]+))?\");\n \n   /**\n    * Information on how to connect to a backend.\n    * The semantics of one of these objects is as follows:\n+   *\n    * <ul>\n    *   <li>It should define a hostname and port to connect to.</li>\n    *   <li>This represents ONE thread on the remote server. The client should\n@@ -453,7 +454,7 @@\n         //    2. It must not throw an exception\n         doAnnotation(annotation, backend, serverURL, message, 0);\n       } catch (Throwable t) {\n-        log.warn(\"Could not annotate via server! Trying to annotate locally...\", t);\n+        log.err(\"Could not annotate via server! Trying to annotate locally...\", t);\n         StanfordCoreNLP corenlp = new StanfordCoreNLP(properties);\n         corenlp.annotate(annotation);\n       } finally {\n@@ -673,15 +674,16 @@\n \n \n   /**\n-   * This can be used just for testing or for command-line text processing.\n+   * Client that runs data through a StanfordCoreNLPServer either just for testing or for command-line text processing.\n    * This runs the pipeline you specify on the\n-   * text in the file that you specify and sends some results to stdout.\n+   * text in the file(s) that you specify (with -file or -filelist) and sends some results to stdout.\n    * The current code in this main method assumes that each line of the file\n    * is to be processed separately as a single sentence.\n-   * <p>\n+   * A site must be specified with a protocol like \"https:\" in front of it.\n+   *\n    * Example usage:<br>\n-   * java -mx6g edu.stanford.nlp.pipeline.StanfordCoreNLP -props properties -backends site1:port1,site2,port2 <br>\n-   *    or just -host name -port number\n+   * java -mx6g edu.stanford.nlp.pipeline.StanfordCoreNLP -props properties -backends site1:port1,site2:port2 <br>\n+   *    or just -host https://foo.bar.com [-port 9000]\n    *\n    * @param args List of required properties\n    * @throws java.io.IOException If IO problem\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 456, "bugNodeStartChar": 16426, "bugNodeLength": 75, "fixLineNum": 456, "fixNodeStartChar": 16426, "fixNodeLength": 74, "sourceBeforeFix": "log.warn(\"Could not annotate via server! Trying to annotate locally...\",t)", "sourceAfterFix": "log.err(\"Could not annotate via server! Trying to annotate locally...\",t)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "5b669bd30cef473ddf2c0c0318652a938d8d3428", "fixCommitParentSHA1": "bb7650a3d71534f3d6ceec85fdabd395417aeffb", "bugFilePath": "src/edu/stanford/nlp/pipeline/StanfordCoreNLPClient.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/StanfordCoreNLPClient.java b/src/edu/stanford/nlp/pipeline/StanfordCoreNLPClient.java\nindex 923242d..d6ee60a 100644\n--- a/src/edu/stanford/nlp/pipeline/StanfordCoreNLPClient.java\n+++ b/src/edu/stanford/nlp/pipeline/StanfordCoreNLPClient.java\n@@ -35,11 +35,12 @@\n   private static final Redwood.RedwoodChannels log = Redwood.channels(StanfordCoreNLPClient.class);\n \n   /** A simple URL spec, for parsing backend URLs */\n-  private static final Pattern URL_PATTERN = Pattern.compile(\"(?:(https?)://)?([^:]+):([0-9]+)?\");\n+  private static final Pattern URL_PATTERN = Pattern.compile(\"(?:(https?)://)?([^:]+)(?::([0-9]+))?\");\n \n   /**\n    * Information on how to connect to a backend.\n    * The semantics of one of these objects is as follows:\n+   *\n    * <ul>\n    *   <li>It should define a hostname and port to connect to.</li>\n    *   <li>This represents ONE thread on the remote server. The client should\n@@ -453,7 +454,7 @@\n         //    2. It must not throw an exception\n         doAnnotation(annotation, backend, serverURL, message, 0);\n       } catch (Throwable t) {\n-        log.warn(\"Could not annotate via server! Trying to annotate locally...\", t);\n+        log.err(\"Could not annotate via server! Trying to annotate locally...\", t);\n         StanfordCoreNLP corenlp = new StanfordCoreNLP(properties);\n         corenlp.annotate(annotation);\n       } finally {\n@@ -673,15 +674,16 @@\n \n \n   /**\n-   * This can be used just for testing or for command-line text processing.\n+   * Client that runs data through a StanfordCoreNLPServer either just for testing or for command-line text processing.\n    * This runs the pipeline you specify on the\n-   * text in the file that you specify and sends some results to stdout.\n+   * text in the file(s) that you specify (with -file or -filelist) and sends some results to stdout.\n    * The current code in this main method assumes that each line of the file\n    * is to be processed separately as a single sentence.\n-   * <p>\n+   * A site must be specified with a protocol like \"https:\" in front of it.\n+   *\n    * Example usage:<br>\n-   * java -mx6g edu.stanford.nlp.pipeline.StanfordCoreNLP -props properties -backends site1:port1,site2,port2 <br>\n-   *    or just -host name -port number\n+   * java -mx6g edu.stanford.nlp.pipeline.StanfordCoreNLP -props properties -backends site1:port1,site2:port2 <br>\n+   *    or just -host https://foo.bar.com [-port 9000]\n    *\n    * @param args List of required properties\n    * @throws java.io.IOException If IO problem\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 456, "bugNodeStartChar": 16426, "bugNodeLength": 75, "fixLineNum": 456, "fixNodeStartChar": 16426, "fixNodeLength": 74, "sourceBeforeFix": "log.warn(\"Could not annotate via server! Trying to annotate locally...\",t)", "sourceAfterFix": "log.err(\"Could not annotate via server! Trying to annotate locally...\",t)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "71b163969a0a1a15179ddf03f44f39fefbd06d7f", "fixCommitParentSHA1": "d6c22c89755434fc16174175c08d28755b8356ec", "bugFilePath": "src/edu/stanford/nlp/pipeline/TokenizerAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/TokenizerAnnotator.java b/src/edu/stanford/nlp/pipeline/TokenizerAnnotator.java\nindex 6ad98a7..b904da5 100644\n--- a/src/edu/stanford/nlp/pipeline/TokenizerAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/TokenizerAnnotator.java\n@@ -358,7 +358,8 @@\n         CoreAnnotations.PositionAnnotation.class,\n         CoreAnnotations.IndexAnnotation.class,\n         CoreAnnotations.OriginalTextAnnotation.class,\n-        CoreAnnotations.ValueAnnotation.class\n+        CoreAnnotations.ValueAnnotation.class,\n+        CoreAnnotations.IsNewlineAnnotation.class\n     ));\n   }\n \n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 349, "bugNodeStartChar": 12744, "bugNodeLength": 630, "fixLineNum": 349, "fixNodeStartChar": 12744, "fixNodeLength": 681, "sourceBeforeFix": "Arrays.asList(CoreAnnotations.TextAnnotation.class,CoreAnnotations.TokensAnnotation.class,CoreAnnotations.CharacterOffsetBeginAnnotation.class,CoreAnnotations.CharacterOffsetEndAnnotation.class,CoreAnnotations.BeforeAnnotation.class,CoreAnnotations.AfterAnnotation.class,CoreAnnotations.TokenBeginAnnotation.class,CoreAnnotations.TokenEndAnnotation.class,CoreAnnotations.PositionAnnotation.class,CoreAnnotations.IndexAnnotation.class,CoreAnnotations.OriginalTextAnnotation.class,CoreAnnotations.ValueAnnotation.class)", "sourceAfterFix": "Arrays.asList(CoreAnnotations.TextAnnotation.class,CoreAnnotations.TokensAnnotation.class,CoreAnnotations.CharacterOffsetBeginAnnotation.class,CoreAnnotations.CharacterOffsetEndAnnotation.class,CoreAnnotations.BeforeAnnotation.class,CoreAnnotations.AfterAnnotation.class,CoreAnnotations.TokenBeginAnnotation.class,CoreAnnotations.TokenEndAnnotation.class,CoreAnnotations.PositionAnnotation.class,CoreAnnotations.IndexAnnotation.class,CoreAnnotations.OriginalTextAnnotation.class,CoreAnnotations.ValueAnnotation.class,CoreAnnotations.IsNewlineAnnotation.class)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "71b163969a0a1a15179ddf03f44f39fefbd06d7f", "fixCommitParentSHA1": "d6c22c89755434fc16174175c08d28755b8356ec", "bugFilePath": "src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotator.java b/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotator.java\nindex b5123e5..65b62a4 100644\n--- a/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/WordsToSentencesAnnotator.java\n@@ -332,7 +332,8 @@\n         CoreAnnotations.TextAnnotation.class,\n         CoreAnnotations.TokensAnnotation.class,\n         CoreAnnotations.CharacterOffsetBeginAnnotation.class,\n-        CoreAnnotations.CharacterOffsetEndAnnotation.class\n+        CoreAnnotations.CharacterOffsetEndAnnotation.class,\n+        CoreAnnotations.IsNewlineAnnotation.class\n     )));\n   }\n \n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 331, "bugNodeStartChar": 15056, "bugNodeLength": 235, "fixLineNum": 331, "fixNodeStartChar": 15056, "fixNodeLength": 286, "sourceBeforeFix": "Arrays.asList(CoreAnnotations.TextAnnotation.class,CoreAnnotations.TokensAnnotation.class,CoreAnnotations.CharacterOffsetBeginAnnotation.class,CoreAnnotations.CharacterOffsetEndAnnotation.class)", "sourceAfterFix": "Arrays.asList(CoreAnnotations.TextAnnotation.class,CoreAnnotations.TokensAnnotation.class,CoreAnnotations.CharacterOffsetBeginAnnotation.class,CoreAnnotations.CharacterOffsetEndAnnotation.class,CoreAnnotations.IsNewlineAnnotation.class)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "b15bc79473746786930f2d3ba5b8e6904654a117", "fixCommitParentSHA1": "44f120e83467a6ede572ff8762747d8b9979914e", "bugFilePath": "src/edu/stanford/nlp/pipeline/JSONOutputter.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/JSONOutputter.java b/src/edu/stanford/nlp/pipeline/JSONOutputter.java\nindex b66d3ad..6ba7b41 100644\n--- a/src/edu/stanford/nlp/pipeline/JSONOutputter.java\n+++ b/src/edu/stanford/nlp/pipeline/JSONOutputter.java\n@@ -94,7 +94,7 @@\n           Tree sentimentTree = sentence.get(SentimentCoreAnnotations.SentimentAnnotatedTree.class);\n           if (sentimentTree != null) {\n             int sentiment = RNNCoreAnnotations.getPredictedClass(sentimentTree);\n-            List<String> sentimentPredictions =\n+            List<Double> sentimentPredictions =\n                 RNNCoreAnnotations.getPredictionsAsStringList(sentimentTree);\n             String sentimentClass = sentence.get(SentimentCoreAnnotations.SentimentClass.class);\n             l2.set(\"sentimentValue\", Integer.toString(sentiment));\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 97, "bugNodeStartChar": 4779, "bugNodeLength": 12, "fixLineNum": 97, "fixNodeStartChar": 4779, "fixNodeLength": 12, "sourceBeforeFix": "List<String>", "sourceAfterFix": "List<Double>"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "c4975cb807eb1816657aff3487ef070ef78ef7cc", "fixCommitParentSHA1": "a9e7250eabd8ec9f86c3a400cf0dcefe87bccf11", "bugFilePath": "src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java b/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java\nindex 890ba0e..2846620 100644\n--- a/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java\n@@ -699,7 +699,7 @@\n             currSectionCoreMap.set(CoreAnnotations.AuthorAnnotation.class, foundAuthor);\n             // get author mention info\n             if (foundAuthor != null) {\n-              Pattern p = Pattern.compile(foundAuthor);\n+              Pattern p = Pattern.compile(foundAuthor, Pattern.LITERAL);\n               Matcher matcher = p.matcher(sectionStartTagToken.word());\n               if (matcher.find()) {\n                 int authorMentionStart = matcher.start() + sectionStartTagCharBegin;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 702, "bugNodeStartChar": 28655, "bugNodeLength": 28, "fixLineNum": 702, "fixNodeStartChar": 28655, "fixNodeLength": 45, "sourceBeforeFix": "Pattern.compile(foundAuthor)", "sourceAfterFix": "Pattern.compile(foundAuthor,Pattern.LITERAL)"}, {"bugType": "CHANGE_CALLER_IN_FUNCTION_CALL", "fixCommitSHA1": "dbb575b5861a30bd4ab660603c671441abb3f2cf", "fixCommitParentSHA1": "532a11ab5914c8b4b5b9da62273ac76f7f944531", "bugFilePath": "src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java b/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java\nindex 44fddeb..1c954bd 100644\n--- a/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java\n@@ -699,7 +699,7 @@\n             currSectionCoreMap.set(CoreAnnotations.AuthorAnnotation.class, foundAuthor);\n             // get author mention info\n             Pattern p = Pattern.compile(foundAuthor);\n-            Matcher matcher = p.matcher(sectionStartToken.word());\n+            Matcher matcher = p.matcher(sectionStartTagToken.word());\n             if (matcher.find()) {\n               int authorMentionStart = matcher.start() + sectionStartTagCharBegin;\n               int authorMentionEnd = matcher.end() + sectionStartTagCharBegin;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 702, "bugNodeStartChar": 28684, "bugNodeLength": 24, "fixLineNum": 702, "fixNodeStartChar": 28684, "fixNodeLength": 27, "sourceBeforeFix": "sectionStartToken.word()", "sourceAfterFix": "sectionStartTagToken.word()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "dbb575b5861a30bd4ab660603c671441abb3f2cf", "fixCommitParentSHA1": "532a11ab5914c8b4b5b9da62273ac76f7f944531", "bugFilePath": "src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java b/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java\nindex 44fddeb..1c954bd 100644\n--- a/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/CleanXmlAnnotator.java\n@@ -699,7 +699,7 @@\n             currSectionCoreMap.set(CoreAnnotations.AuthorAnnotation.class, foundAuthor);\n             // get author mention info\n             Pattern p = Pattern.compile(foundAuthor);\n-            Matcher matcher = p.matcher(sectionStartToken.word());\n+            Matcher matcher = p.matcher(sectionStartTagToken.word());\n             if (matcher.find()) {\n               int authorMentionStart = matcher.start() + sectionStartTagCharBegin;\n               int authorMentionEnd = matcher.end() + sectionStartTagCharBegin;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 702, "bugNodeStartChar": 28684, "bugNodeLength": 24, "fixLineNum": 702, "fixNodeStartChar": 28684, "fixNodeLength": 27, "sourceBeforeFix": "sectionStartToken.word()", "sourceAfterFix": "sectionStartTagToken.word()"}, {"bugType": "CHANGE_OPERATOR", "fixCommitSHA1": "3065978a391204e4dacea84430ad1d5fa410534c", "fixCommitParentSHA1": "3cf46c3c9e446e04529d15bbebdfe0f1df383508", "bugFilePath": "src/edu/stanford/nlp/parser/tools/ParseAndSetLabels.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/tools/ParseAndSetLabels.java b/src/edu/stanford/nlp/parser/tools/ParseAndSetLabels.java\nindex ded5110..08bc8b8 100644\n--- a/src/edu/stanford/nlp/parser/tools/ParseAndSetLabels.java\n+++ b/src/edu/stanford/nlp/parser/tools/ParseAndSetLabels.java\n@@ -259,7 +259,7 @@\n     Map<String, String> labelMap = readLabelMap(labelsFile, separator, remapLabels);\n \n     List<String> sentences;\n-    if (sentencesFile == null) {\n+    if (sentencesFile != null) {\n       sentences = readSentences(sentencesFile);\n     } else {\n       sentences = new ArrayList<String>(labelMap.keySet());\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 262, "bugNodeStartChar": 9251, "bugNodeLength": 21, "fixLineNum": 262, "fixNodeStartChar": 9251, "fixNodeLength": 21, "sourceBeforeFix": "sentencesFile == null", "sourceAfterFix": "sentencesFile != null"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "2a522f666712e6932f3baed27a412478c321decf", "fixCommitParentSHA1": "04790a26b7bbdfb2ef055fbb55c7a5207963ead1", "bugFilePath": "src/edu/stanford/nlp/pipeline/Annotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/Annotator.java b/src/edu/stanford/nlp/pipeline/Annotator.java\nindex 5204baa..b5e9161 100644\n--- a/src/edu/stanford/nlp/pipeline/Annotator.java\n+++ b/src/edu/stanford/nlp/pipeline/Annotator.java\n@@ -130,7 +130,7 @@\n     put(STANFORD_ENTITY_MENTIONS,          new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_NER)));\n     put(STANFORD_GENDER,                   new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_NER)));\n     put(STANFORD_TRUECASE,                 new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT)));\n-    put(STANFORD_PARSE,                    new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT)));\n+    put(STANFORD_PARSE,                    new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS)));\n     put(STANFORD_DETERMINISTIC_COREF,      new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_NER, STANFORD_MENTION, STANFORD_PARSE)));\n     put(STANFORD_COREF,                    new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_NER, STANFORD_MENTION)));\n     put(STANFORD_MENTION,                  new LinkedHashSet<>(Arrays.asList(STANFORD_TOKENIZE, STANFORD_SSPLIT, STANFORD_POS, STANFORD_LEMMA, STANFORD_NER, STANFORD_DEPENDENCIES)));\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 133, "bugNodeStartChar": 6538, "bugNodeLength": 49, "fixLineNum": 133, "fixNodeStartChar": 6538, "fixNodeLength": 63, "sourceBeforeFix": "Arrays.asList(STANFORD_TOKENIZE,STANFORD_SSPLIT)", "sourceAfterFix": "Arrays.asList(STANFORD_TOKENIZE,STANFORD_SSPLIT,STANFORD_POS)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "b9c41e9ff3fc13529a73bd488d9e9d86b33d0871", "fixCommitParentSHA1": "54893fd676fd13c708103ee2c032e0bc3597b32c", "bugFilePath": "src/edu/stanford/nlp/pipeline/StanfordCoreNLP.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/StanfordCoreNLP.java b/src/edu/stanford/nlp/pipeline/StanfordCoreNLP.java\nindex c0cd4a8..3faee85 100644\n--- a/src/edu/stanford/nlp/pipeline/StanfordCoreNLP.java\n+++ b/src/edu/stanford/nlp/pipeline/StanfordCoreNLP.java\n@@ -556,7 +556,7 @@\n             property.substring(CUSTOM_ANNOTATOR_PREFIX.length());\n         final String customClassName = inputProps.getProperty(property);\n         logger.info(\"Registering annotator \" + customName + \" with class \" + customClassName);\n-        pool.register(customName, inputProps, Lazy.of(() -> annotatorImplementation.custom(inputProps, customName)));\n+        pool.register(customName, inputProps, Lazy.of(() -> annotatorImplementation.custom(inputProps, property)));\n       }\n     }\n   }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 559, "bugNodeStartChar": 22794, "bugNodeLength": 54, "fixLineNum": 559, "fixNodeStartChar": 22794, "fixNodeLength": 52, "sourceBeforeFix": "annotatorImplementation.custom(inputProps,customName)", "sourceAfterFix": "annotatorImplementation.custom(inputProps,property)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "c2cfe6ff21e76d8c7f1ac185ba7538ef993f2733", "fixCommitParentSHA1": "233b4275d73c8f480017b709100acb58caa51e2a", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/KBPAnnotatorBenchmarkSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/KBPAnnotatorBenchmarkSlowITest.java b/itest/src/edu/stanford/nlp/pipeline/KBPAnnotatorBenchmarkSlowITest.java\nindex 4972ad0..6d62c9b 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/KBPAnnotatorBenchmarkSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/KBPAnnotatorBenchmarkSlowITest.java\n@@ -18,7 +18,7 @@\n \n   public String KBP_DOCS_DIR = \"/scr/nlp/data/kbp-benchmark//kbp-docs\";\n   public String GOLD_RELATIONS_PATH = \"/scr/nlp/data/kbp-benchmark/kbp-gold-relations.txt\";\n-  public double KBP_MINIMUM_SCORE = 45.30;\n+  public double KBP_MINIMUM_SCORE = .453;\n \n   private String convertRelationName(String relationName) {\n     /*if (relationName.equals(\"org:top_members/employees\")) {\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 21, "bugNodeStartChar": 645, "bugNodeLength": 25, "fixLineNum": 21, "fixNodeStartChar": 645, "fixNodeLength": 24, "sourceBeforeFix": "KBP_MINIMUM_SCORE=45.30", "sourceAfterFix": "KBP_MINIMUM_SCORE=.453"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "71457e50581dd7f16b18cf8faaec3f0aa21cd880", "fixCommitParentSHA1": "6e5badc4ef35c1391775b1f542d53ef030a9df16", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\nindex 859a456..57afc37 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n@@ -313,7 +313,7 @@\n     assertNotNull(compressedProto);\n \n     // Check size\n-    assertTrue(\"\" + compressedProto.length, compressedProto.length < 390200);\n+    assertTrue(\"\" + compressedProto.length, compressedProto.length < 391000);\n     assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 2100000);\n   }\n \n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 316, "bugNodeStartChar": 44947, "bugNodeLength": 31, "fixLineNum": 316, "fixNodeStartChar": 44947, "fixNodeLength": 31, "sourceBeforeFix": "compressedProto.length < 390200", "sourceAfterFix": "compressedProto.length < 391000"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "3066e39a4024e88b26e3346a93bd5b9b56dcefed", "fixCommitParentSHA1": "71c955dccb934d8c459398728a7dbb2b169e4e03", "bugFilePath": "itest/src/edu/stanford/nlp/coref/hybrid/ChineseCorefBenchmarkSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/coref/hybrid/ChineseCorefBenchmarkSlowITest.java b/itest/src/edu/stanford/nlp/coref/hybrid/ChineseCorefBenchmarkSlowITest.java\nindex 444fbc7..3586a08 100644\n--- a/itest/src/edu/stanford/nlp/coref/hybrid/ChineseCorefBenchmarkSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/coref/hybrid/ChineseCorefBenchmarkSlowITest.java\n@@ -124,7 +124,7 @@\n \n     setLowHighExpected(lowResults, highResults, expectedResults, BLANC_F1, 46.00, 47.25, 46.68); // In 2015 was: 46.19\n \n-    setLowHighExpected(lowResults, highResults, expectedResults, CONLL_SCORE, 53.75, 54.00, 54.01); // In 2015 was: 53.19\n+    setLowHighExpected(lowResults, highResults, expectedResults, CONLL_SCORE, 53.75, 54.10, 54.01); // In 2015 was: 53.19\n \n     BenchmarkingHelper.benchmarkResults(results, lowResults, highResults, expectedResults);\n   }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 127, "bugNodeStartChar": 5971, "bugNodeLength": 94, "fixLineNum": 127, "fixNodeStartChar": 5971, "fixNodeLength": 94, "sourceBeforeFix": "setLowHighExpected(lowResults,highResults,expectedResults,CONLL_SCORE,53.75,54.00,54.01)", "sourceAfterFix": "setLowHighExpected(lowResults,highResults,expectedResults,CONLL_SCORE,53.75,54.10,54.01)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "71c955dccb934d8c459398728a7dbb2b169e4e03", "fixCommitParentSHA1": "bea6849f515408303d87a0cb7ef05f30c4a8858b", "bugFilePath": "itest/src/edu/stanford/nlp/coref/hybrid/ChineseCorefBenchmarkSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/coref/hybrid/ChineseCorefBenchmarkSlowITest.java b/itest/src/edu/stanford/nlp/coref/hybrid/ChineseCorefBenchmarkSlowITest.java\nindex 36f215c..444fbc7 100644\n--- a/itest/src/edu/stanford/nlp/coref/hybrid/ChineseCorefBenchmarkSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/coref/hybrid/ChineseCorefBenchmarkSlowITest.java\n@@ -114,7 +114,7 @@\n     setLowHighExpected(lowResults, highResults, expectedResults, MUC_F1, 58.30, 58.80, 58.52); // In 2015 was: 57.87\n \n     setLowHighExpected(lowResults, highResults, expectedResults, BCUBED_TP, 6990, 7110.00, 7026.39); // In 2015 was: 6936.32\n-    setLowHighExpected(lowResults, highResults, expectedResults, BCUBED_F1, 51.60, 52.10, 52.08); // In 2015 was: 51.07\n+    setLowHighExpected(lowResults, highResults, expectedResults, BCUBED_F1, 51.60, 52.20, 52.11); // In 2015 was: 51.07\n \n     setLowHighExpected(lowResults, highResults, expectedResults, CEAFM_TP, 8220, 8260, 8224); // In 2015 was: 8074\n     setLowHighExpected(lowResults, highResults, expectedResults, CEAFM_F1, 55.40, 56.00, 55.43); // In 2015 was: 55.10\n@@ -124,7 +124,7 @@\n \n     setLowHighExpected(lowResults, highResults, expectedResults, BLANC_F1, 46.00, 47.25, 46.68); // In 2015 was: 46.19\n \n-    setLowHighExpected(lowResults, highResults, expectedResults, CONLL_SCORE, 53.75, 54.00, 53.98); // In 2015 was: 53.19\n+    setLowHighExpected(lowResults, highResults, expectedResults, CONLL_SCORE, 53.75, 54.00, 54.01); // In 2015 was: 53.19\n \n     BenchmarkingHelper.benchmarkResults(results, lowResults, highResults, expectedResults);\n   }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 127, "bugNodeStartChar": 5971, "bugNodeLength": 94, "fixLineNum": 127, "fixNodeStartChar": 5971, "fixNodeLength": 94, "sourceBeforeFix": "setLowHighExpected(lowResults,highResults,expectedResults,CONLL_SCORE,53.75,54.00,53.98)", "sourceAfterFix": "setLowHighExpected(lowResults,highResults,expectedResults,CONLL_SCORE,53.75,54.00,54.01)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "f5677191fdcbb8d526e1f87ed48b8e9df7a6b973", "fixCommitParentSHA1": "57836aea1f0c8d7e261fcf7acdadf3586282679d", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\nindex 87c4ae9..859a456 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n@@ -313,7 +313,7 @@\n     assertNotNull(compressedProto);\n \n     // Check size\n-    assertTrue(\"\" + compressedProto.length, compressedProto.length < 390000);\n+    assertTrue(\"\" + compressedProto.length, compressedProto.length < 390200);\n     assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 2100000);\n   }\n \n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 316, "bugNodeStartChar": 44947, "bugNodeLength": 31, "fixLineNum": 316, "fixNodeStartChar": 44947, "fixNodeLength": 31, "sourceBeforeFix": "compressedProto.length < 390000", "sourceAfterFix": "compressedProto.length < 390200"}, {"bugType": "OVERLOAD_METHOD_DELETED_ARGS", "fixCommitSHA1": "2bac95c9764d5cc853dd00ccfc354b410b32251b", "fixCommitParentSHA1": "7dc4e4c75072173cb87d97110afade4b98819a9f", "bugFilePath": "src/edu/stanford/nlp/trees/international/pennchinese/UniversalChineseGrammaticalRelations.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/trees/international/pennchinese/UniversalChineseGrammaticalRelations.java b/src/edu/stanford/nlp/trees/international/pennchinese/UniversalChineseGrammaticalRelations.java\nindex 3f788f9..eefeb18 100644\n--- a/src/edu/stanford/nlp/trees/international/pennchinese/UniversalChineseGrammaticalRelations.java\n+++ b/src/edu/stanford/nlp/trees/international/pennchinese/UniversalChineseGrammaticalRelations.java\n@@ -339,8 +339,7 @@\n     new GrammaticalRelation(Language.UniversalChinese, \"nummod\", \"numeric modifier\",\n                             MODIFIER,\n                             \"QP|NP|DP\", tregexCompiler,\n-            \"NP|QP < ( QP  =target << M $++ NN|NP|QP)\",\n-            \"NP|QP < ( DNP=target < (QP < CD !< OD) !< JJ|ADJP $++ NP|QP )\"\n+            \"NP|QP < ( QP  =target << M $++ NN|NP|QP)\"\n             // the following rule is merged into mark:clf\n             //\"DP < ( DT $+ CLP=target )\"\n             );\n@@ -516,7 +515,7 @@\n             \"NP|CLP|QP < (ADJP=target $++ NP|CLP|QP ) \",\n             \"NP  $++ (CP=target << VA !<< VV) > NP \",\n             \"NP  < ( CP=target $++ NP << VA !<< VV)\",\n-            \"NP|QP < ( DNP=target < JJ|ADJP !< NP|QP $++ NP|QP )\");\n+            \"NP|QP < ( DNP=target < JJ|ADJP|QP !< NP $++ NP|QP )\");\n \n   /**\n    * The \"ordinal modifier\" (ordmod) grammatical relation.\n@@ -525,8 +524,7 @@\n           new GrammaticalRelation(Language.UniversalChinese, \"amod:ordmod\", \"ordinal numeric modifier\",\n                   ADJECTIVAL_MODIFIER,\n                   \"NP|QP\", tregexCompiler,\n-                  \"NP < (QP=target < OD !< CLP)\",\n-                  \"NP|QP < ( DNP=target < (QP < OD !< CD) !< JJ|ADJP $++ NP|QP )\"\n+                  \"NP < (QP=target < OD !< CLP)\"\n                   // the following rule is merged into mark:clf\n                   //\"QP < (OD=target $+ CLP)\"\n                   );\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 339, "bugNodeStartChar": 11721, "bugNodeLength": 420, "fixLineNum": 339, "fixNodeStartChar": 11721, "fixNodeLength": 343, "sourceBeforeFix": "new GrammaticalRelation(Language.UniversalChinese,\"nummod\",\"numeric modifier\",MODIFIER,\"QP|NP|DP\",tregexCompiler,\"NP|QP < ( QP  =target << M $++ NN|NP|QP)\",\"NP|QP < ( DNP=target < (QP < CD !< OD) !< JJ|ADJP $++ NP|QP )\")", "sourceAfterFix": "new GrammaticalRelation(Language.UniversalChinese,\"nummod\",\"numeric modifier\",MODIFIER,\"QP|NP|DP\",tregexCompiler,\"NP|QP < ( QP  =target << M $++ NN|NP|QP)\")"}, {"bugType": "OVERLOAD_METHOD_DELETED_ARGS", "fixCommitSHA1": "2bac95c9764d5cc853dd00ccfc354b410b32251b", "fixCommitParentSHA1": "7dc4e4c75072173cb87d97110afade4b98819a9f", "bugFilePath": "src/edu/stanford/nlp/trees/international/pennchinese/UniversalChineseGrammaticalRelations.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/trees/international/pennchinese/UniversalChineseGrammaticalRelations.java b/src/edu/stanford/nlp/trees/international/pennchinese/UniversalChineseGrammaticalRelations.java\nindex 3f788f9..eefeb18 100644\n--- a/src/edu/stanford/nlp/trees/international/pennchinese/UniversalChineseGrammaticalRelations.java\n+++ b/src/edu/stanford/nlp/trees/international/pennchinese/UniversalChineseGrammaticalRelations.java\n@@ -339,8 +339,7 @@\n     new GrammaticalRelation(Language.UniversalChinese, \"nummod\", \"numeric modifier\",\n                             MODIFIER,\n                             \"QP|NP|DP\", tregexCompiler,\n-            \"NP|QP < ( QP  =target << M $++ NN|NP|QP)\",\n-            \"NP|QP < ( DNP=target < (QP < CD !< OD) !< JJ|ADJP $++ NP|QP )\"\n+            \"NP|QP < ( QP  =target << M $++ NN|NP|QP)\"\n             // the following rule is merged into mark:clf\n             //\"DP < ( DT $+ CLP=target )\"\n             );\n@@ -516,7 +515,7 @@\n             \"NP|CLP|QP < (ADJP=target $++ NP|CLP|QP ) \",\n             \"NP  $++ (CP=target << VA !<< VV) > NP \",\n             \"NP  < ( CP=target $++ NP << VA !<< VV)\",\n-            \"NP|QP < ( DNP=target < JJ|ADJP !< NP|QP $++ NP|QP )\");\n+            \"NP|QP < ( DNP=target < JJ|ADJP|QP !< NP $++ NP|QP )\");\n \n   /**\n    * The \"ordinal modifier\" (ordmod) grammatical relation.\n@@ -525,8 +524,7 @@\n           new GrammaticalRelation(Language.UniversalChinese, \"amod:ordmod\", \"ordinal numeric modifier\",\n                   ADJECTIVAL_MODIFIER,\n                   \"NP|QP\", tregexCompiler,\n-                  \"NP < (QP=target < OD !< CLP)\",\n-                  \"NP|QP < ( DNP=target < (QP < OD !< CD) !< JJ|ADJP $++ NP|QP )\"\n+                  \"NP < (QP=target < OD !< CLP)\"\n                   // the following rule is merged into mark:clf\n                   //\"QP < (OD=target $+ CLP)\"\n                   );\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 525, "bugNodeStartChar": 18289, "bugNodeLength": 437, "fixLineNum": 525, "fixNodeStartChar": 18289, "fixNodeLength": 354, "sourceBeforeFix": "new GrammaticalRelation(Language.UniversalChinese,\"amod:ordmod\",\"ordinal numeric modifier\",ADJECTIVAL_MODIFIER,\"NP|QP\",tregexCompiler,\"NP < (QP=target < OD !< CLP)\",\"NP|QP < ( DNP=target < (QP < OD !< CD) !< JJ|ADJP $++ NP|QP )\")", "sourceAfterFix": "new GrammaticalRelation(Language.UniversalChinese,\"amod:ordmod\",\"ordinal numeric modifier\",ADJECTIVAL_MODIFIER,\"NP|QP\",tregexCompiler,\"NP < (QP=target < OD !< CLP)\")"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "2b4a0c1d9d2f2b27558951c1bf7776d96bd495e1", "fixCommitParentSHA1": "b4ae93e67eba4d3b291da618c8841621bfd57e66", "bugFilePath": "src/edu/stanford/nlp/pipeline/ChineseSegmenterAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/ChineseSegmenterAnnotator.java b/src/edu/stanford/nlp/pipeline/ChineseSegmenterAnnotator.java\nindex 163ec3f..ccef653 100644\n--- a/src/edu/stanford/nlp/pipeline/ChineseSegmenterAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/ChineseSegmenterAnnotator.java\n@@ -138,7 +138,7 @@\n       String wordString = new String(ca);\n \n       // if this word is a whitespace or a control character, set 'seg' to true for next word, and break\n-      if (Character.isSpaceChar(origText.charAt(i)) || Character.isISOControl(origText.charAt(i))) {\n+      if (Character.isWhitespace(origText.charAt(i)) || Character.isISOControl(origText.charAt(i))) {\n         seg = true;\n       } else {\n         // if this word is a word, put it as a feature label and set seg to false for next word\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 141, "bugNodeStartChar": 4872, "bugNodeLength": 41, "fixLineNum": 141, "fixNodeStartChar": 4872, "fixNodeLength": 42, "sourceBeforeFix": "Character.isSpaceChar(origText.charAt(i))", "sourceAfterFix": "Character.isWhitespace(origText.charAt(i))"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "2b4a0c1d9d2f2b27558951c1bf7776d96bd495e1", "fixCommitParentSHA1": "b4ae93e67eba4d3b291da618c8841621bfd57e66", "bugFilePath": "src/edu/stanford/nlp/pipeline/ChineseSegmenterAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/ChineseSegmenterAnnotator.java b/src/edu/stanford/nlp/pipeline/ChineseSegmenterAnnotator.java\nindex 163ec3f..ccef653 100644\n--- a/src/edu/stanford/nlp/pipeline/ChineseSegmenterAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/ChineseSegmenterAnnotator.java\n@@ -138,7 +138,7 @@\n       String wordString = new String(ca);\n \n       // if this word is a whitespace or a control character, set 'seg' to true for next word, and break\n-      if (Character.isSpaceChar(origText.charAt(i)) || Character.isISOControl(origText.charAt(i))) {\n+      if (Character.isWhitespace(origText.charAt(i)) || Character.isISOControl(origText.charAt(i))) {\n         seg = true;\n       } else {\n         // if this word is a word, put it as a feature label and set seg to false for next word\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 141, "bugNodeStartChar": 4872, "bugNodeLength": 41, "fixLineNum": 141, "fixNodeStartChar": 4872, "fixNodeLength": 42, "sourceBeforeFix": "Character.isSpaceChar(origText.charAt(i))", "sourceAfterFix": "Character.isWhitespace(origText.charAt(i))"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "a829d431df342200f7ce9030c9a6045ebe13deed", "fixCommitParentSHA1": "90a2d97e9309c056408717cd3af39570485efc03", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\nindex 81ebc62..ce43530 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n@@ -306,7 +306,7 @@\n \n     // Check size\n     assertTrue(\"\" + compressedProto.length, compressedProto.length < 380000);\n-    assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 1700000);\n+    assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 1800000);\n   }\n \n   @Test\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 309, "bugNodeStartChar": 44649, "bugNodeLength": 34, "fixLineNum": 309, "fixNodeStartChar": 44649, "fixNodeLength": 34, "sourceBeforeFix": "uncompressedProto.length < 1700000", "sourceAfterFix": "uncompressedProto.length < 1800000"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "90a2d97e9309c056408717cd3af39570485efc03", "fixCommitParentSHA1": "564cb2fa83f92a26564ea4e5f2443e5da749a86a", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\nindex 9fe3ac0..81ebc62 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n@@ -305,7 +305,7 @@\n     assertNotNull(compressedProto);\n \n     // Check size\n-    assertTrue(\"\" + compressedProto.length, compressedProto.length < 340000);\n+    assertTrue(\"\" + compressedProto.length, compressedProto.length < 380000);\n     assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 1700000);\n   }\n \n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 308, "bugNodeStartChar": 44569, "bugNodeLength": 31, "fixLineNum": 308, "fixNodeStartChar": 44569, "fixNodeLength": 31, "sourceBeforeFix": "compressedProto.length < 340000", "sourceAfterFix": "compressedProto.length < 380000"}, {"bugType": "SWAP_ARGUMENTS", "fixCommitSHA1": "ae8511699a039b9a0b207b0699b32c2e7de72b83", "fixCommitParentSHA1": "4519fcc62fe227d1089e496e152545861d69e6da", "bugFilePath": "itest/src/edu/stanford/nlp/hcoref/DcorefChineseBenchmarkSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/hcoref/DcorefChineseBenchmarkSlowITest.java b/itest/src/edu/stanford/nlp/hcoref/DcorefChineseBenchmarkSlowITest.java\nindex 9650222..9bc7b17 100644\n--- a/itest/src/edu/stanford/nlp/hcoref/DcorefChineseBenchmarkSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/hcoref/DcorefChineseBenchmarkSlowITest.java\n@@ -105,23 +105,23 @@\n \n \n     setAll(lowResults, highResults, expectedResults, MENTION_TP, 12370);\n-    setLowHighExpected(lowResults, highResults, expectedResults, MENTION_F1, 55.5, 55.6, 55.7);\n+    setLowHighExpected(lowResults, highResults, expectedResults, MENTION_F1, 55.5, 55.7, 55.6);\n \n     setLowHighExpected(lowResults, highResults, expectedResults, MUC_TP, 5957, 5970, 5965);\n-    setAll(lowResults,highResults,expectedResults,MUC_F1,57.87);\n+    setLowHighExpected(lowResults, highResults, expectedResults, MUC_F1, 57.85, 57.90, 57.87);\n \n     setLowHighExpected(lowResults, highResults, expectedResults, BCUBED_TP, 6868.8, 6936.32, 6868.81);\n-    setAll(lowResults, highResults, expectedResults, BCUBED_F1,51.07);\n+    setLowHighExpected(lowResults, highResults, expectedResults, BCUBED_F1, 51.05, 51.10, 51.07);\n \n     setAll(lowResults, highResults, expectedResults, CEAFM_TP,8074);\n     setLowHighExpected(lowResults, highResults, expectedResults, CEAFM_F1, 54.75, 55.2, 54.79);\n \n     setAll(lowResults, highResults, expectedResults, CEAFE_TP, 2205.69);\n-    setLowHighExpected(lowResults, highResults, expectedResults, CEAFE_F1, 50.45, 50.61, 50.47);\n+    setLowHighExpected(lowResults, highResults, expectedResults, CEAFE_F1, 50.50, 50.65, 50.61);\n \n-    setAll(lowResults,highResults,expectedResults,BLANC_F1,46.19);\n+    setLowHighExpected(lowResults, highResults, expectedResults, BLANC_F1, 46.15, 46.25, 46.19);\n \n-    setAll(lowResults,highResults,expectedResults,CONLL_SCORE,53.19);\n+    setLowHighExpected(lowResults, highResults, expectedResults, CONLL_SCORE, 53.18, 53.20, 53.19);\n \n \n     Counter<String> results = new ClassicCounter<String>();\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 108, "bugNodeStartChar": 4631, "bugNodeLength": 90, "fixLineNum": 108, "fixNodeStartChar": 4631, "fixNodeLength": 90, "sourceBeforeFix": "setLowHighExpected(lowResults,highResults,expectedResults,MENTION_F1,55.5,55.6,55.7)", "sourceAfterFix": "setLowHighExpected(lowResults,highResults,expectedResults,MENTION_F1,55.5,55.7,55.6)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "5362cfa0c0ec238299e695c3ed143bff1a9e48ff", "fixCommitParentSHA1": "c63b166a61cc7b88a01d7ba5f88a475208d07e03", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\nindex 2247c7a..d3c5ce2 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n@@ -304,7 +304,7 @@\n     assertNotNull(compressedProto);\n \n     // Check size\n-    assertTrue(\"\" + compressedProto.length, compressedProto.length < 330000);\n+    assertTrue(\"\" + compressedProto.length, compressedProto.length < 340000);\n     assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 1700000);\n   }\n \n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 307, "bugNodeStartChar": 44524, "bugNodeLength": 31, "fixLineNum": 307, "fixNodeStartChar": 44524, "fixNodeLength": 31, "sourceBeforeFix": "compressedProto.length < 330000", "sourceAfterFix": "compressedProto.length < 340000"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "1b48948453abfca68f40756572ca9ee744242d5c", "fixCommitParentSHA1": "2fb1bb709d3b36c9dd2d11aacb0d10afcfe54927", "bugFilePath": "itest/src/edu/stanford/nlp/parser/nndep/DependencyParserITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/parser/nndep/DependencyParserITest.java b/itest/src/edu/stanford/nlp/parser/nndep/DependencyParserITest.java\nindex 6b02600..6e2eeda 100644\n--- a/itest/src/edu/stanford/nlp/parser/nndep/DependencyParserITest.java\n+++ b/itest/src/edu/stanford/nlp/parser/nndep/DependencyParserITest.java\n@@ -46,7 +46,7 @@\n   }\n \n   // Lower because we're evaluating on PTB + extraDevTest, not just PTB\n-  private static final double EnglishUdLas = 84.9873;\n+  private static final double EnglishUdLas = 88.72648417258083;\n \n   /**\n    * Test that the NN dependency parser performance doesn't change.\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 49, "bugNodeStartChar": 1770, "bugNodeLength": 22, "fixLineNum": 49, "fixNodeStartChar": 1770, "fixNodeLength": 32, "sourceBeforeFix": "EnglishUdLas=84.9873", "sourceAfterFix": "EnglishUdLas=88.72648417258083"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "bc675a98c050975d97bbaac4c135e9558d167b3b", "fixCommitParentSHA1": "e33c5dd67170e0faf0e854921e9dcb0c9d6ec057", "bugFilePath": "itest/src/edu/stanford/nlp/hcoref/DcorefChineseBenchmarkSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/hcoref/DcorefChineseBenchmarkSlowITest.java b/itest/src/edu/stanford/nlp/hcoref/DcorefChineseBenchmarkSlowITest.java\nindex 2b110d2..b8b5a1f 100644\n--- a/itest/src/edu/stanford/nlp/hcoref/DcorefChineseBenchmarkSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/hcoref/DcorefChineseBenchmarkSlowITest.java\n@@ -107,7 +107,7 @@\n     setAll(lowResults,highResults,expectedResults,MENTION_TP,12370);\n     setAll(lowResults,highResults,expectedResults,MENTION_F1,55.6);\n \n-    setAll(lowResults,highResults,expectedResults,MUC_TP,5965);\n+    setLowHighExpected(lowResults, highResults, expectedResults, MUC_TP, 5965, 5970, 5965);\n     setAll(lowResults,highResults,expectedResults,MUC_F1,57.93);\n \n     setAll(lowResults,highResults,expectedResults,BCUBED_TP,6868.81);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 110, "bugNodeStartChar": 4696, "bugNodeLength": 58, "fixLineNum": 110, "fixNodeStartChar": 4696, "fixNodeLength": 86, "sourceBeforeFix": "setAll(lowResults,highResults,expectedResults,MUC_TP,5965)", "sourceAfterFix": "setLowHighExpected(lowResults,highResults,expectedResults,MUC_TP,5965,5970,5965)"}, {"bugType": "CHANGE_MODIFIER", "fixCommitSHA1": "cade2271924f42b35da72efa4ad6b2eebc9875cd", "fixCommitParentSHA1": "d45561c1b3bf0e4a44a768407131a2fb2093efe9", "bugFilePath": "src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java b/src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java\nindex 59df89a..72aa237 100644\n--- a/src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java\n+++ b/src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java\n@@ -73,7 +73,7 @@\n    *\n    * @param dictPath the path to the dictionary file\n    */\n-  private HashMap<String, String> setupDictionary(String dictPath) {\n+  private static HashMap<String, String> setupDictionary(String dictPath) {\n     HashMap<String, String> dictionary = new HashMap<>();\n     BufferedReader br = null;\n     try {\n@@ -83,7 +83,7 @@\n         if (words.length < 3) {\n           System.err.printf(\"SpanishVerbStripper: adding words to dict, missing fields, ignoring line: %s%n\", line);\n         } else {\n-          dict.put(words[0], words[2]);\n+          dictionary.put(words[0], words[2]);\n         }\n       }\n     } catch (UnsupportedEncodingException e) {\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 69, "bugNodeStartChar": 2172, "bugNodeLength": 1076, "fixLineNum": 69, "fixNodeStartChar": 2172, "fixNodeLength": 1083, "sourceBeforeFix": "2", "sourceAfterFix": "10"}, {"bugType": "CHANGE_CALLER_IN_FUNCTION_CALL", "fixCommitSHA1": "cade2271924f42b35da72efa4ad6b2eebc9875cd", "fixCommitParentSHA1": "d45561c1b3bf0e4a44a768407131a2fb2093efe9", "bugFilePath": "src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java b/src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java\nindex 59df89a..72aa237 100644\n--- a/src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java\n+++ b/src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java\n@@ -73,7 +73,7 @@\n    *\n    * @param dictPath the path to the dictionary file\n    */\n-  private HashMap<String, String> setupDictionary(String dictPath) {\n+  private static HashMap<String, String> setupDictionary(String dictPath) {\n     HashMap<String, String> dictionary = new HashMap<>();\n     BufferedReader br = null;\n     try {\n@@ -83,7 +83,7 @@\n         if (words.length < 3) {\n           System.err.printf(\"SpanishVerbStripper: adding words to dict, missing fields, ignoring line: %s%n\", line);\n         } else {\n-          dict.put(words[0], words[2]);\n+          dictionary.put(words[0], words[2]);\n         }\n       }\n     } catch (UnsupportedEncodingException e) {\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 86, "bugNodeStartChar": 2931, "bugNodeLength": 28, "fixLineNum": 86, "fixNodeStartChar": 2931, "fixNodeLength": 34, "sourceBeforeFix": "dict.put(words[0],words[2])", "sourceAfterFix": "dictionary.put(words[0],words[2])"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "cade2271924f42b35da72efa4ad6b2eebc9875cd", "fixCommitParentSHA1": "d45561c1b3bf0e4a44a768407131a2fb2093efe9", "bugFilePath": "src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java b/src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java\nindex 59df89a..72aa237 100644\n--- a/src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java\n+++ b/src/edu/stanford/nlp/international/spanish/SpanishVerbStripper.java\n@@ -73,7 +73,7 @@\n    *\n    * @param dictPath the path to the dictionary file\n    */\n-  private HashMap<String, String> setupDictionary(String dictPath) {\n+  private static HashMap<String, String> setupDictionary(String dictPath) {\n     HashMap<String, String> dictionary = new HashMap<>();\n     BufferedReader br = null;\n     try {\n@@ -83,7 +83,7 @@\n         if (words.length < 3) {\n           System.err.printf(\"SpanishVerbStripper: adding words to dict, missing fields, ignoring line: %s%n\", line);\n         } else {\n-          dict.put(words[0], words[2]);\n+          dictionary.put(words[0], words[2]);\n         }\n       }\n     } catch (UnsupportedEncodingException e) {\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 86, "bugNodeStartChar": 2931, "bugNodeLength": 28, "fixLineNum": 86, "fixNodeStartChar": 2931, "fixNodeLength": 34, "sourceBeforeFix": "dict.put(words[0],words[2])", "sourceAfterFix": "dictionary.put(words[0],words[2])"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "526022e101f5d10253272f1b0bd9e64861ed66e1", "fixCommitParentSHA1": "b748fc61a4602aeff60a60d765254faef130b3e9", "bugFilePath": "src/edu/stanford/nlp/simple/SentenceAlgorithms.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/simple/SentenceAlgorithms.java b/src/edu/stanford/nlp/simple/SentenceAlgorithms.java\nindex 2d9418a..479b92d 100644\n--- a/src/edu/stanford/nlp/simple/SentenceAlgorithms.java\n+++ b/src/edu/stanford/nlp/simple/SentenceAlgorithms.java\n@@ -354,7 +354,7 @@\n     int endAncestor = end;\n     seenVertices.clear();\n     while (governors.get(endAncestor).isPresent() && governors.get(endAncestor).get() >= 0) {\n-      if (seenVertices.contains(startAncestor)) {\n+      if (seenVertices.contains(endAncestor)) {\n         return Collections.EMPTY_LIST;\n       }\n       seenVertices.add(startAncestor);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 357, "bugNodeStartChar": 13645, "bugNodeLength": 36, "fixLineNum": 357, "fixNodeStartChar": 13645, "fixNodeLength": 34, "sourceBeforeFix": "seenVertices.contains(startAncestor)", "sourceAfterFix": "seenVertices.contains(endAncestor)"}, {"bugType": "OVERLOAD_METHOD_DELETED_ARGS", "fixCommitSHA1": "ab68841ecd0f9ce9c299ef8dc54052af996a7465", "fixCommitParentSHA1": "a4a7007467daaf231bcd410db569c8f9e46682fa", "bugFilePath": "src/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.java b/src/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.java\nindex 01bccce..28f39a5 100644\n--- a/src/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.java\n+++ b/src/edu/stanford/nlp/semgraph/semgrex/SemgrexPattern.java\n@@ -410,7 +410,7 @@\n         treebank.loadPath(treeFile);\n         for (Tree tree : treebank) {\n           // TODO: allow other languages... this defaults to English\n-          SemanticGraph graph = SemanticGraphFactory.makeFromTree(tree, mode, useExtras ? GrammaticalStructure.Extras.MAXIMAL : GrammaticalStructure.Extras.NONE, true, null);\n+          SemanticGraph graph = SemanticGraphFactory.makeFromTree(tree, mode, useExtras ? GrammaticalStructure.Extras.MAXIMAL : GrammaticalStructure.Extras.NONE, true);\n           graphs.add(graph);\n         }\n       }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 413, "bugNodeStartChar": 16252, "bugNodeLength": 141, "fixLineNum": 413, "fixNodeStartChar": 16252, "fixNodeLength": 135, "sourceBeforeFix": "SemanticGraphFactory.makeFromTree(tree,mode,useExtras ? GrammaticalStructure.Extras.MAXIMAL : GrammaticalStructure.Extras.NONE,true,null)", "sourceAfterFix": "SemanticGraphFactory.makeFromTree(tree,mode,useExtras ? GrammaticalStructure.Extras.MAXIMAL : GrammaticalStructure.Extras.NONE,true)"}, {"bugType": "SWAP_BOOLEAN_LITERAL", "fixCommitSHA1": "bce9b34b81eec6bce73b01bf6ff062b386095875", "fixCommitParentSHA1": "835fb8501e8a26a7180086d96a511a9b43674bcd", "bugFilePath": "src/edu/stanford/nlp/ie/util/OntonotesXMLtoColumn.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/ie/util/OntonotesXMLtoColumn.java b/src/edu/stanford/nlp/ie/util/OntonotesXMLtoColumn.java\nindex 9a34e2f..6c61b1f 100644\n--- a/src/edu/stanford/nlp/ie/util/OntonotesXMLtoColumn.java\n+++ b/src/edu/stanford/nlp/ie/util/OntonotesXMLtoColumn.java\n@@ -32,7 +32,7 @@\n  */\n public class OntonotesXMLtoColumn {\n \n-  private static final boolean VERBOSE = true;\n+  private static final boolean VERBOSE = false;\n \n   final SAXParser parser;\n \n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 35, "bugNodeStartChar": 1112, "bugNodeLength": 14, "fixLineNum": 35, "fixNodeStartChar": 1112, "fixNodeLength": 15, "sourceBeforeFix": "VERBOSE=true", "sourceAfterFix": "VERBOSE=false"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "049530d90ae7f619525a7f523132552961146fbc", "fixCommitParentSHA1": "fc62a0ce98eaefff5f95d866179e35271fc64320", "bugFilePath": "src/edu/stanford/nlp/math/SloppyMath.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/math/SloppyMath.java b/src/edu/stanford/nlp/math/SloppyMath.java\nindex 0daa3cb..97b2462 100644\n--- a/src/edu/stanford/nlp/math/SloppyMath.java\n+++ b/src/edu/stanford/nlp/math/SloppyMath.java\n@@ -351,7 +351,7 @@\n       if (e == 1) {\n         return b;\n       } else {\n-        return 0; // this is also what you get for e < 0 !\n+        return 1; // this is also what you get for e < 0 !\n       }\n     } else {\n       if (e == 2) {\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 354, "bugNodeStartChar": 11045, "bugNodeLength": 9, "fixLineNum": 354, "fixNodeStartChar": 11045, "fixNodeLength": 9, "sourceBeforeFix": "return 0; ", "sourceAfterFix": "return 1; "}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "fddee1afbed00ca766a88798f968d42a7411a34a", "fixCommitParentSHA1": "1116a0dcc54579c768b30cb5474b4b95d9c00111", "bugFilePath": "src/edu/stanford/nlp/parser/nndep/Classifier.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/nndep/Classifier.java b/src/edu/stanford/nlp/parser/nndep/Classifier.java\nindex 8a563bc..12dc993 100644\n--- a/src/edu/stanford/nlp/parser/nndep/Classifier.java\n+++ b/src/edu/stanford/nlp/parser/nndep/Classifier.java\n@@ -271,7 +271,7 @@\n         double[] gradHidden = new double[config.hiddenSize];\n         for (int nodeIndex : ls) {\n           gradHidden[nodeIndex] = gradHidden3[nodeIndex] * 3 * hidden[nodeIndex] * hidden[nodeIndex];\n-          gradb1[nodeIndex] += gradHidden3[nodeIndex];\n+          gradb1[nodeIndex] += gradHidden[nodeIndex];\n         }\n \n         offset = 0;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 274, "bugNodeStartChar": 9104, "bugNodeLength": 22, "fixLineNum": 274, "fixNodeStartChar": 9104, "fixNodeLength": 21, "sourceBeforeFix": "gradHidden3[nodeIndex]", "sourceAfterFix": "gradHidden[nodeIndex]"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "dc75782a421691bbbb373cc9e226495d4c773876", "fixCommitParentSHA1": "e9d9194376c12133f24a8d5011fbe7eda14550af", "bugFilePath": "itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java", "fixPatch": "diff --git a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\nindex aa12b18..b3be9ed 100644\n--- a/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n+++ b/itest/src/edu/stanford/nlp/pipeline/ProtobufAnnotationSerializerSlowITest.java\n@@ -240,7 +240,7 @@\n \n     // Check size\n     assertTrue(\"\" + compressedProto.length, compressedProto.length < 290000);\n-    assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 1000000);\n+    assertTrue(\"\" + uncompressedProto.length, uncompressedProto.length < 1100000);\n   }\n \n   @Test\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 243, "bugNodeStartChar": 37424, "bugNodeLength": 34, "fixLineNum": 243, "fixNodeStartChar": 37424, "fixNodeLength": 34, "sourceBeforeFix": "uncompressedProto.length < 1000000", "sourceAfterFix": "uncompressedProto.length < 1100000"}, {"bugType": "CHANGE_CALLER_IN_FUNCTION_CALL", "fixCommitSHA1": "a772531235259480a08e539c706b5884fbe6bfd6", "fixCommitParentSHA1": "8bbf9cc98df8cf2de4004bb52439376a1ae4f0e1", "bugFilePath": "src/edu/stanford/nlp/parser/nndep/Classifier.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/nndep/Classifier.java b/src/edu/stanford/nlp/parser/nndep/Classifier.java\nindex f57213d..a1b32e0 100644\n--- a/src/edu/stanford/nlp/parser/nndep/Classifier.java\n+++ b/src/edu/stanford/nlp/parser/nndep/Classifier.java\n@@ -654,7 +654,7 @@\n    */\n   public void preCompute(Set<Integer> toPreCompute) {\n     long startTime = System.currentTimeMillis();\n-    saved = new double[toPreCompute.size()][config.hiddenSize];\n+    saved = new double[preMap.size()][config.hiddenSize];\n     for (int x : toPreCompute) {\n       int mapX = preMap.get(x);\n       int tok = x / config.numTokens;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 657, "bugNodeStartChar": 20652, "bugNodeLength": 19, "fixLineNum": 657, "fixNodeStartChar": 20652, "fixNodeLength": 13, "sourceBeforeFix": "toPreCompute.size()", "sourceAfterFix": "preMap.size()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "a772531235259480a08e539c706b5884fbe6bfd6", "fixCommitParentSHA1": "8bbf9cc98df8cf2de4004bb52439376a1ae4f0e1", "bugFilePath": "src/edu/stanford/nlp/parser/nndep/Classifier.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/nndep/Classifier.java b/src/edu/stanford/nlp/parser/nndep/Classifier.java\nindex f57213d..a1b32e0 100644\n--- a/src/edu/stanford/nlp/parser/nndep/Classifier.java\n+++ b/src/edu/stanford/nlp/parser/nndep/Classifier.java\n@@ -654,7 +654,7 @@\n    */\n   public void preCompute(Set<Integer> toPreCompute) {\n     long startTime = System.currentTimeMillis();\n-    saved = new double[toPreCompute.size()][config.hiddenSize];\n+    saved = new double[preMap.size()][config.hiddenSize];\n     for (int x : toPreCompute) {\n       int mapX = preMap.get(x);\n       int tok = x / config.numTokens;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 657, "bugNodeStartChar": 20652, "bugNodeLength": 19, "fixLineNum": 657, "fixNodeStartChar": 20652, "fixNodeLength": 13, "sourceBeforeFix": "toPreCompute.size()", "sourceAfterFix": "preMap.size()"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "c0d55ea6c6d29478b55deec439643e7b984c0739", "fixCommitParentSHA1": "0f7a46b3d5ad05ff93a7e56fd522c7d7de235534", "bugFilePath": "src/edu/stanford/nlp/parser/nndep/DependencyParser.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/nndep/DependencyParser.java b/src/edu/stanford/nlp/parser/nndep/DependencyParser.java\nindex 2ea6106..f306055 100644\n--- a/src/edu/stanford/nlp/parser/nndep/DependencyParser.java\n+++ b/src/edu/stanford/nlp/parser/nndep/DependencyParser.java\n@@ -995,17 +995,17 @@\n                 ? new BufferedReader(new InputStreamReader(System.in, encoding))\n                 : IOUtils.readerFromString(inputFilename, encoding);\n       } catch (IOException e) {\n-        throw new RuntimeIOException(e);\n+        throw new RuntimeIOException(\"No input file provided (use -parseFile)\", e);\n       }\n \n       String outputFilename = props.getProperty(\"outFile\");\n       PrintWriter output;\n       try {\n-        output = outputFilename.equals(\"-\")\n+        output = outputFilename == null || outputFilename.equals(\"-\")\n             ? IOUtils.encodedOutputStreamPrintWriter(System.out, encoding, true)\n             : IOUtils.getPrintWriter(outputFilename, encoding);\n       } catch (IOException e) {\n-        throw new RuntimeIOException(e);\n+        throw new RuntimeIOException(\"Error opening output file\", e);\n       }\n \n       parser.parseTextFile(input, output);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 998, "bugNodeStartChar": 35078, "bugNodeLength": 25, "fixLineNum": 998, "fixNodeStartChar": 35078, "fixNodeLength": 68, "sourceBeforeFix": "new RuntimeIOException(e)", "sourceAfterFix": "new RuntimeIOException(\"No input file provided (use -parseFile)\",e)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "c0d55ea6c6d29478b55deec439643e7b984c0739", "fixCommitParentSHA1": "0f7a46b3d5ad05ff93a7e56fd522c7d7de235534", "bugFilePath": "src/edu/stanford/nlp/parser/nndep/DependencyParser.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/nndep/DependencyParser.java b/src/edu/stanford/nlp/parser/nndep/DependencyParser.java\nindex 2ea6106..f306055 100644\n--- a/src/edu/stanford/nlp/parser/nndep/DependencyParser.java\n+++ b/src/edu/stanford/nlp/parser/nndep/DependencyParser.java\n@@ -995,17 +995,17 @@\n                 ? new BufferedReader(new InputStreamReader(System.in, encoding))\n                 : IOUtils.readerFromString(inputFilename, encoding);\n       } catch (IOException e) {\n-        throw new RuntimeIOException(e);\n+        throw new RuntimeIOException(\"No input file provided (use -parseFile)\", e);\n       }\n \n       String outputFilename = props.getProperty(\"outFile\");\n       PrintWriter output;\n       try {\n-        output = outputFilename.equals(\"-\")\n+        output = outputFilename == null || outputFilename.equals(\"-\")\n             ? IOUtils.encodedOutputStreamPrintWriter(System.out, encoding, true)\n             : IOUtils.getPrintWriter(outputFilename, encoding);\n       } catch (IOException e) {\n-        throw new RuntimeIOException(e);\n+        throw new RuntimeIOException(\"Error opening output file\", e);\n       }\n \n       parser.parseTextFile(input, output);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 1008, "bugNodeStartChar": 35447, "bugNodeLength": 25, "fixLineNum": 1008, "fixNodeStartChar": 35447, "fixNodeLength": 54, "sourceBeforeFix": "new RuntimeIOException(e)", "sourceAfterFix": "new RuntimeIOException(\"Error opening output file\",e)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "20b4e37925aeacf26a7a596d9989a230b9d5b708", "fixCommitParentSHA1": "25a6414bc4b656f478349e3568a22614e5f5f6b4", "bugFilePath": "src/edu/stanford/nlp/neural/Embedding.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/neural/Embedding.java b/src/edu/stanford/nlp/neural/Embedding.java\nindex 33b6a42..deab3e6 100644\n--- a/src/edu/stanford/nlp/neural/Embedding.java\n+++ b/src/edu/stanford/nlp/neural/Embedding.java\n@@ -100,7 +100,7 @@\n       }\n       // check for end token\n       if(word.equals(\"</s>\")){\n-        word = START_WORD;\n+        word = END_WORD;\n       }\n \n       int dimOfWords = lineSplit.length - 1;\n@@ -152,6 +152,7 @@\n       String word = wordIterator.next();\n \n       // check for unknown token\n+      // FIXME cut and paste code\n     if(word.equals(\"UNKNOWN\") || word.equals(\"UUUNKKK\") || word.equals(\"UNK\") || word.equals(\"*UNKNOWN*\") || word.equals(\"<unk>\")){\n         word = UNKNOWN_WORD;\n       }\n@@ -161,7 +162,7 @@\n       }\n       // check for end token\n       if(word.equals(\"</s>\")){\n-        word = START_WORD;\n+        word = END_WORD;\n       }\n \n       int dimOfWords = lineSplit.length;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 103, "bugNodeStartChar": 3448, "bugNodeLength": 17, "fixLineNum": 103, "fixNodeStartChar": 3448, "fixNodeLength": 15, "sourceBeforeFix": "word=START_WORD", "sourceAfterFix": "word=END_WORD"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "20b4e37925aeacf26a7a596d9989a230b9d5b708", "fixCommitParentSHA1": "25a6414bc4b656f478349e3568a22614e5f5f6b4", "bugFilePath": "src/edu/stanford/nlp/neural/Embedding.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/neural/Embedding.java b/src/edu/stanford/nlp/neural/Embedding.java\nindex 33b6a42..deab3e6 100644\n--- a/src/edu/stanford/nlp/neural/Embedding.java\n+++ b/src/edu/stanford/nlp/neural/Embedding.java\n@@ -100,7 +100,7 @@\n       }\n       // check for end token\n       if(word.equals(\"</s>\")){\n-        word = START_WORD;\n+        word = END_WORD;\n       }\n \n       int dimOfWords = lineSplit.length - 1;\n@@ -152,6 +152,7 @@\n       String word = wordIterator.next();\n \n       // check for unknown token\n+      // FIXME cut and paste code\n     if(word.equals(\"UNKNOWN\") || word.equals(\"UUUNKKK\") || word.equals(\"UNK\") || word.equals(\"*UNKNOWN*\") || word.equals(\"<unk>\")){\n         word = UNKNOWN_WORD;\n       }\n@@ -161,7 +162,7 @@\n       }\n       // check for end token\n       if(word.equals(\"</s>\")){\n-        word = START_WORD;\n+        word = END_WORD;\n       }\n \n       int dimOfWords = lineSplit.length;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 164, "bugNodeStartChar": 5860, "bugNodeLength": 17, "fixLineNum": 164, "fixNodeStartChar": 5860, "fixNodeLength": 15, "sourceBeforeFix": "word=START_WORD", "sourceAfterFix": "word=END_WORD"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "e608cc292c0100c32f7659e10fb1341d68428fc9", "fixCommitParentSHA1": "0c796839f76e0aea4c7bef230c42155f2bea6ac6", "bugFilePath": "src/edu/stanford/nlp/international/spanish/process/SpanishTokenizer.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/international/spanish/process/SpanishTokenizer.java b/src/edu/stanford/nlp/international/spanish/process/SpanishTokenizer.java\nindex bda62b4..9ec7a15 100644\n--- a/src/edu/stanford/nlp/international/spanish/process/SpanishTokenizer.java\n+++ b/src/edu/stanford/nlp/international/spanish/process/SpanishTokenizer.java\n@@ -149,7 +149,7 @@\n \t\tcase \"contigo\":\n \t\tcase \"consigo\":\n \t\tdefault:\n-\t\t\tFirst = word.substring(0, 3);\n+\t\t\tfirst = word.substring(0, 3);\n \t\t\tsecond = word.substring(3, 5);\n \t\t}\n    \n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 152, "bugNodeStartChar": 5231, "bugNodeLength": 28, "fixLineNum": 152, "fixNodeStartChar": 5231, "fixNodeLength": 28, "sourceBeforeFix": "First=word.substring(0,3)", "sourceAfterFix": "first=word.substring(0,3)"}, {"bugType": "CHANGE_NUMERAL", "fixCommitSHA1": "09c8b466c280738cdce89b83e0ee6a92dc02710f", "fixCommitParentSHA1": "bba623ca486d1034253569d8cd3f3c159be27eec", "bugFilePath": "src/edu/stanford/nlp/trees/international/spanish/SpanishTreeNormalizer.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/trees/international/spanish/SpanishTreeNormalizer.java b/src/edu/stanford/nlp/trees/international/spanish/SpanishTreeNormalizer.java\nindex d6caf3c..107369c 100644\n--- a/src/edu/stanford/nlp/trees/international/spanish/SpanishTreeNormalizer.java\n+++ b/src/edu/stanford/nlp/trees/international/spanish/SpanishTreeNormalizer.java\n@@ -179,7 +179,7 @@\n     }\n   }\n \n-  private static final Pattern pQuoted = Pattern.compile(\"\\\".+\\\"\");\n+  private static final Pattern pQuoted = Pattern.compile(\"\\\"(.+)\\\"\");\n \n   /**\n    * Return the (single or multiple) words which make up the given\n@@ -190,7 +190,7 @@\n     if (quoteMatcher.matches()) {\n       String[] ret = new String[3];\n       ret[0] = \"\\\"\";\n-      ret[1] = quoteMatcher.group(0);\n+      ret[1] = quoteMatcher.group(1);\n       ret[2] = \"\\\"\";\n \n       return ret;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 193, "bugNodeStartChar": 5988, "bugNodeLength": 21, "fixLineNum": 193, "fixNodeStartChar": 5988, "fixNodeLength": 21, "sourceBeforeFix": "quoteMatcher.group(0)", "sourceAfterFix": "quoteMatcher.group(1)"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "eeeb1de4d4c7595fab59b51f0c31292e6d5d7fad", "fixCommitParentSHA1": "4835b7e2cc3486f8c34526779935b22865c70d6b", "bugFilePath": "src/edu/stanford/nlp/pipeline/SentenceAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/SentenceAnnotator.java b/src/edu/stanford/nlp/pipeline/SentenceAnnotator.java\nindex 5599ea4..46db206 100644\n--- a/src/edu/stanford/nlp/pipeline/SentenceAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/SentenceAnnotator.java\n@@ -59,7 +59,7 @@\n             } catch (RejectedExecutionException e) {\n               // If we time out, for now, we just throw away all jobs which were running at the time.\n               // Note that in order for this to be useful, the underlying job needs to handle Thread.interrupted()\n-              List<CoreMap> failedSentences = wrapper.shutdownNow();\n+              List<CoreMap> failedSentences = wrapper.joinWithTimeout();\n               for (CoreMap failed : failedSentences) {\n                 doOneFailedSentence(annotation, failed);\n               }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 62, "bugNodeStartChar": 2423, "bugNodeLength": 21, "fixLineNum": 62, "fixNodeStartChar": 2423, "fixNodeLength": 25, "sourceBeforeFix": "wrapper.shutdownNow()", "sourceAfterFix": "wrapper.joinWithTimeout()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "eeeb1de4d4c7595fab59b51f0c31292e6d5d7fad", "fixCommitParentSHA1": "4835b7e2cc3486f8c34526779935b22865c70d6b", "bugFilePath": "src/edu/stanford/nlp/pipeline/SentenceAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/SentenceAnnotator.java b/src/edu/stanford/nlp/pipeline/SentenceAnnotator.java\nindex 5599ea4..46db206 100644\n--- a/src/edu/stanford/nlp/pipeline/SentenceAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/SentenceAnnotator.java\n@@ -59,7 +59,7 @@\n             } catch (RejectedExecutionException e) {\n               // If we time out, for now, we just throw away all jobs which were running at the time.\n               // Note that in order for this to be useful, the underlying job needs to handle Thread.interrupted()\n-              List<CoreMap> failedSentences = wrapper.shutdownNow();\n+              List<CoreMap> failedSentences = wrapper.joinWithTimeout();\n               for (CoreMap failed : failedSentences) {\n                 doOneFailedSentence(annotation, failed);\n               }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 62, "bugNodeStartChar": 2423, "bugNodeLength": 21, "fixLineNum": 62, "fixNodeStartChar": 2423, "fixNodeLength": 25, "sourceBeforeFix": "wrapper.shutdownNow()", "sourceAfterFix": "wrapper.joinWithTimeout()"}, {"bugType": "SWAP_BOOLEAN_LITERAL", "fixCommitSHA1": "70b2b5e86c88c8bf0bb0a096c8952e6cd48b7521", "fixCommitParentSHA1": "af56d37acbe3482b396ec2b5faff4d76d3d8ecc4", "bugFilePath": "src/edu/stanford/nlp/trees/GrammaticalStructure.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/trees/GrammaticalStructure.java b/src/edu/stanford/nlp/trees/GrammaticalStructure.java\nindex f1c12e5..55ca574 100644\n--- a/src/edu/stanford/nlp/trees/GrammaticalStructure.java\n+++ b/src/edu/stanford/nlp/trees/GrammaticalStructure.java\n@@ -253,8 +253,11 @@\n     }\n     if (attach && puncFilter.accept(t.headWordNode().label().value())) {\n       // make faster by first looking for links from parent\n+      // it is necessary to look for paths using all directions\n+      // because sometimes there are edges created from lower nodes to\n+      // nodes higher up\n       TreeGraphNode parent = t.parent().highestNodeWithSameHead();\n-      if (!basicGraph.isEdge(parent, t) && basicGraph.getShortestPath(root, t, true) == null) {\n+      if (!basicGraph.isEdge(parent, t) && basicGraph.getShortestPath(root, t, false) == null) {\n         basicGraph.add(parent, t, GrammaticalRelation.DEPENDENT);\n       }\n     }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 257, "bugNodeStartChar": 10525, "bugNodeLength": 41, "fixLineNum": 257, "fixNodeStartChar": 10525, "fixNodeLength": 42, "sourceBeforeFix": "basicGraph.getShortestPath(root,t,true)", "sourceAfterFix": "basicGraph.getShortestPath(root,t,false)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "4d379e7a16584651192c0f451a7f362d112e27b1", "fixCommitParentSHA1": "5cc04b89b2c37085c5d7809718aeff76810ab5a5", "bugFilePath": "src/edu/stanford/nlp/graph/DirectedMultiGraph.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/graph/DirectedMultiGraph.java b/src/edu/stanford/nlp/graph/DirectedMultiGraph.java\nindex f2118c1..98f4c96 100644\n--- a/src/edu/stanford/nlp/graph/DirectedMultiGraph.java\n+++ b/src/edu/stanford/nlp/graph/DirectedMultiGraph.java\n@@ -138,7 +138,7 @@\n     }\n     boolean foundOut = outgoingEdges.containsKey(source) && outgoingEdges.get(source).containsKey(dest) &&\n         outgoingEdges.get(source).get(dest).remove(data);\n-    boolean foundIn = incomingEdges.containsKey(source) && incomingEdges.get(source).containsKey(dest) &&\n+    boolean foundIn = incomingEdges.containsKey(dest) && incomingEdges.get(dest).containsKey(source) &&\n         incomingEdges.get(dest).get(source).remove(data);\n     if (foundOut && !foundIn) {\n       throw new AssertionError(\"Edge found in outgoing but not incoming\");\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 141, "bugNodeStartChar": 4155, "bugNodeLength": 33, "fixLineNum": 141, "fixNodeStartChar": 4155, "fixNodeLength": 31, "sourceBeforeFix": "incomingEdges.containsKey(source)", "sourceAfterFix": "incomingEdges.containsKey(dest)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "063331b78a8974fa056781a86df1e211c4a6884e", "fixCommitParentSHA1": "c1c1da765e7c3fdec3313421eecd290dc403ddfb", "bugFilePath": "src/edu/stanford/nlp/sentiment/BuildBinarizedDataset.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/sentiment/BuildBinarizedDataset.java b/src/edu/stanford/nlp/sentiment/BuildBinarizedDataset.java\nindex 8d7928b..31690d9 100644\n--- a/src/edu/stanford/nlp/sentiment/BuildBinarizedDataset.java\n+++ b/src/edu/stanford/nlp/sentiment/BuildBinarizedDataset.java\n@@ -201,7 +201,7 @@\n         scorer.forwardPropagateTree(collapsedUnary);\n         setPredictedLabels(collapsedUnary);\n       } else {\n-        setUnknownLabels(binarized, mainLabel);\n+        setUnknownLabels(collapsedUnary, mainLabel);\n       }\n \n       Trees.convertToCoreLabels(collapsedUnary);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 204, "bugNodeStartChar": 7586, "bugNodeLength": 38, "fixLineNum": 204, "fixNodeStartChar": 7586, "fixNodeLength": 43, "sourceBeforeFix": "setUnknownLabels(binarized,mainLabel)", "sourceAfterFix": "setUnknownLabels(collapsedUnary,mainLabel)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "0d252a7fbd2a1b85faa7feb487c39aaa97208209", "fixCommitParentSHA1": "2aa995eac979071ed932f428723e675c67eb5b54", "bugFilePath": "src/edu/stanford/nlp/parser/shiftreduce/Weight.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/shiftreduce/Weight.java b/src/edu/stanford/nlp/parser/shiftreduce/Weight.java\nindex 8475095..bf925c0 100644\n--- a/src/edu/stanford/nlp/parser/shiftreduce/Weight.java\n+++ b/src/edu/stanford/nlp/parser/shiftreduce/Weight.java\n@@ -109,7 +109,7 @@\n       }\n       int index = unpackIndex(i);\n       float score = unpackScore(i);\n-      packed[j] = pack(index, score);\n+      newPacked[j] = pack(index, score);\n       ++j;\n     }\n     packed = newPacked;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 112, "bugNodeStartChar": 2933, "bugNodeLength": 9, "fixLineNum": 112, "fixNodeStartChar": 2933, "fixNodeLength": 12, "sourceBeforeFix": "packed[j]", "sourceAfterFix": "newPacked[j]"}, {"bugType": "CHANGE_CALLER_IN_FUNCTION_CALL", "fixCommitSHA1": "b614342e519c1499a7abc2b88121773db6bc2b35", "fixCommitParentSHA1": "8faab3e3ca83562925aab9befb377c4c4dfe030c", "bugFilePath": "src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java b/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java\nindex 6aee143..9c80680 100644\n--- a/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java\n+++ b/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java\n@@ -75,7 +75,7 @@\n           Transition transition = parser.transitionIndex.get(predictedTransition.object());\n           State newState = transition.apply(state, predictedTransition.score());\n           // System.err.println(\"  Transition: \" + transition + \" (\" + predictedTransition.score() + \")\");\n-          if (bestState == null || newState.score() < bestState.score()) {\n+          if (bestState == null || bestState.score() < newState.score()) {\n             bestState = newState;\n           }\n           beam.add(newState);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 78, "bugNodeStartChar": 2935, "bugNodeLength": 16, "fixLineNum": 78, "fixNodeStartChar": 2935, "fixNodeLength": 17, "sourceBeforeFix": "newState.score()", "sourceAfterFix": "bestState.score()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "b614342e519c1499a7abc2b88121773db6bc2b35", "fixCommitParentSHA1": "8faab3e3ca83562925aab9befb377c4c4dfe030c", "bugFilePath": "src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java b/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java\nindex 6aee143..9c80680 100644\n--- a/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java\n+++ b/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java\n@@ -75,7 +75,7 @@\n           Transition transition = parser.transitionIndex.get(predictedTransition.object());\n           State newState = transition.apply(state, predictedTransition.score());\n           // System.err.println(\"  Transition: \" + transition + \" (\" + predictedTransition.score() + \")\");\n-          if (bestState == null || newState.score() < bestState.score()) {\n+          if (bestState == null || bestState.score() < newState.score()) {\n             bestState = newState;\n           }\n           beam.add(newState);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 78, "bugNodeStartChar": 2935, "bugNodeLength": 16, "fixLineNum": 78, "fixNodeStartChar": 2935, "fixNodeLength": 17, "sourceBeforeFix": "newState.score()", "sourceAfterFix": "bestState.score()"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "f2e361b5fc58c61b73a35dffa6d64312a4a9cddc", "fixCommitParentSHA1": "19c76b985ec00c4158c4fd6b18a722350da03c46", "bugFilePath": "src/edu/stanford/nlp/patterns/surface/GetPatternsFromDataMultiClass.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/patterns/surface/GetPatternsFromDataMultiClass.java b/src/edu/stanford/nlp/patterns/surface/GetPatternsFromDataMultiClass.java\nindex e3bd576..44a45d1 100644\n--- a/src/edu/stanford/nlp/patterns/surface/GetPatternsFromDataMultiClass.java\n+++ b/src/edu/stanford/nlp/patterns/surface/GetPatternsFromDataMultiClass.java\n@@ -997,7 +997,7 @@\n             ConstantsAndVariables.class, PatternScoring.class, String.class,\n             TwoDimensionalCounter.class, TwoDimensionalCounter.class,\n             TwoDimensionalCounter.class, TwoDimensionalCounter.class,\n-            TwoDimensionalCounter.class);\n+            TwoDimensionalCounter.class, String.class);\n         \n         scorePatterns = ctor.newInstance(new Object[] { constVars,\n             constVars.patternScoring, label, patternsandWords4Label,\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 996, "bugNodeStartChar": 41398, "bugNodeLength": 279, "fixLineNum": 996, "fixNodeStartChar": 41398, "fixNodeLength": 293, "sourceBeforeFix": "clazz.getConstructor(ConstantsAndVariables.class,PatternScoring.class,String.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class)", "sourceAfterFix": "clazz.getConstructor(ConstantsAndVariables.class,PatternScoring.class,String.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,String.class)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "f54c1ff9b478d5f00988ad6462d66ed6b6646e1b", "fixCommitParentSHA1": "9cab27d684d8a4714ff0ea17b8eace8160c24f20", "bugFilePath": "src/edu/stanford/nlp/math/ArrayMath.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/math/ArrayMath.java b/src/edu/stanford/nlp/math/ArrayMath.java\nindex cf690dd..97989fa 100644\n--- a/src/edu/stanford/nlp/math/ArrayMath.java\n+++ b/src/edu/stanford/nlp/math/ArrayMath.java\n@@ -280,7 +280,7 @@\n \n   public static void pairwiseAddInPlace(double[] to, double[] from) {\n     if (to.length != from.length) {\n-      throw new RuntimeException();\n+      throw new RuntimeException(\"to length:\" + to.length + \" from length:\" + from.length);\n     }\n     for (int i = 0; i < to.length; i++) {\n       to[i] = to[i] + from[i];\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 283, "bugNodeStartChar": 6888, "bugNodeLength": 22, "fixLineNum": 283, "fixNodeStartChar": 6888, "fixNodeLength": 78, "sourceBeforeFix": "new RuntimeException()", "sourceAfterFix": "new RuntimeException(\"to length:\" + to.length + \" from length:\"+ from.length)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "098119281bf9b903d71a333a0d2bcf087282bbf8", "fixCommitParentSHA1": "85447ceaf34f83db0fb01c21770322b350a7fcbf", "bugFilePath": "src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java b/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java\nindex b3ea82f..0769e8c 100644\n--- a/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java\n+++ b/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java\n@@ -101,7 +101,7 @@\n    * TODO: pass this in rather than create it here if we wind up using\n    * this in more place.  Right now it's only used in testOnTreebank.\n    */\n-  protected Function<List<? extends HasWord>, ArrayList<TaggedWord>> tagger;\n+  protected Function<List<? extends HasWord>, List<TaggedWord>> tagger;\n \n   public EvaluateTreebank(LexicalizedParser parser) {\n     this(parser.getOp(), parser.lex, parser);\n@@ -122,7 +122,7 @@\n         Class[] argsClass = { String.class };\n         Object[] arguments = { op.testOptions.taggerSerializedFile };\n         System.err.printf(\"Loading tagger from serialized file %s ...\\n\",op.testOptions.taggerSerializedFile);\n-        tagger = (Function<List<? extends HasWord>,ArrayList<TaggedWord>>) Class.forName(\"edu.stanford.nlp.tagger.maxent.MaxentTagger\").getConstructor(argsClass).newInstance(arguments);\n+        tagger = (Function<List<? extends HasWord>,List<TaggedWord>>) Class.forName(\"edu.stanford.nlp.tagger.maxent.MaxentTagger\").getConstructor(argsClass).newInstance(arguments);\n       } catch (RuntimeException e) {\n         throw e;\n       } catch (Exception e) {\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 104, "bugNodeStartChar": 3551, "bugNodeLength": 21, "fixLineNum": 104, "fixNodeStartChar": 3551, "fixNodeLength": 16, "sourceBeforeFix": "ArrayList<TaggedWord>", "sourceAfterFix": "List<TaggedWord>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "098119281bf9b903d71a333a0d2bcf087282bbf8", "fixCommitParentSHA1": "85447ceaf34f83db0fb01c21770322b350a7fcbf", "bugFilePath": "src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java b/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java\nindex b3ea82f..0769e8c 100644\n--- a/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java\n+++ b/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java\n@@ -101,7 +101,7 @@\n    * TODO: pass this in rather than create it here if we wind up using\n    * this in more place.  Right now it's only used in testOnTreebank.\n    */\n-  protected Function<List<? extends HasWord>, ArrayList<TaggedWord>> tagger;\n+  protected Function<List<? extends HasWord>, List<TaggedWord>> tagger;\n \n   public EvaluateTreebank(LexicalizedParser parser) {\n     this(parser.getOp(), parser.lex, parser);\n@@ -122,7 +122,7 @@\n         Class[] argsClass = { String.class };\n         Object[] arguments = { op.testOptions.taggerSerializedFile };\n         System.err.printf(\"Loading tagger from serialized file %s ...\\n\",op.testOptions.taggerSerializedFile);\n-        tagger = (Function<List<? extends HasWord>,ArrayList<TaggedWord>>) Class.forName(\"edu.stanford.nlp.tagger.maxent.MaxentTagger\").getConstructor(argsClass).newInstance(arguments);\n+        tagger = (Function<List<? extends HasWord>,List<TaggedWord>>) Class.forName(\"edu.stanford.nlp.tagger.maxent.MaxentTagger\").getConstructor(argsClass).newInstance(arguments);\n       } catch (RuntimeException e) {\n         throw e;\n       } catch (Exception e) {\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 125, "bugNodeStartChar": 4333, "bugNodeLength": 21, "fixLineNum": 125, "fixNodeStartChar": 4333, "fixNodeLength": 16, "sourceBeforeFix": "ArrayList<TaggedWord>", "sourceAfterFix": "List<TaggedWord>"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "0456dbb2f67c9f98090954d9d70cb02ceb4622d3", "fixCommitParentSHA1": "42296477373027abdd09c189e1a6d14a85be4e81", "bugFilePath": "src/edu/stanford/nlp/trees/Tree.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/trees/Tree.java b/src/edu/stanford/nlp/trees/Tree.java\nindex d322e7a..280ec3f 100644\n--- a/src/edu/stanford/nlp/trees/Tree.java\n+++ b/src/edu/stanford/nlp/trees/Tree.java\n@@ -868,7 +868,7 @@\n       return;\n     }\n     pw.print(\"(\");\n-    String nodeString = onlyLabelValue ? nodeString() : nodeString();\n+    String nodeString = onlyLabelValue ? value() : nodeString();\n     pw.print(nodeString);\n     // pw.flush();\n     boolean parentIsNull = label() == null || label().value() == null;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 871, "bugNodeStartChar": 28918, "bugNodeLength": 12, "fixLineNum": 871, "fixNodeStartChar": 28918, "fixNodeLength": 7, "sourceBeforeFix": "nodeString()", "sourceAfterFix": "value()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "0456dbb2f67c9f98090954d9d70cb02ceb4622d3", "fixCommitParentSHA1": "42296477373027abdd09c189e1a6d14a85be4e81", "bugFilePath": "src/edu/stanford/nlp/trees/Tree.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/trees/Tree.java b/src/edu/stanford/nlp/trees/Tree.java\nindex d322e7a..280ec3f 100644\n--- a/src/edu/stanford/nlp/trees/Tree.java\n+++ b/src/edu/stanford/nlp/trees/Tree.java\n@@ -868,7 +868,7 @@\n       return;\n     }\n     pw.print(\"(\");\n-    String nodeString = onlyLabelValue ? nodeString() : nodeString();\n+    String nodeString = onlyLabelValue ? value() : nodeString();\n     pw.print(nodeString);\n     // pw.flush();\n     boolean parentIsNull = label() == null || label().value() == null;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 871, "bugNodeStartChar": 28918, "bugNodeLength": 12, "fixLineNum": 871, "fixNodeStartChar": 28918, "fixNodeLength": 7, "sourceBeforeFix": "nodeString()", "sourceAfterFix": "value()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "45bf206c4d10874716feb5ef11f55a041bd22111", "fixCommitParentSHA1": "6bbd0ececa1678c87872c709297793a84328ca69", "bugFilePath": "src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java b/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java\nindex 385f665..e02b0a0 100644\n--- a/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java\n+++ b/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java\n@@ -46,7 +46,7 @@\n   }\n \n   public static TwoDimensionalMap<String, String, SimpleMatrix> averageBinaryMatrices(List<TwoDimensionalMap<String, String, SimpleMatrix>> maps) {\n-    TwoDimensionalMap<String, String, SimpleMatrix> averages = new TwoDimensionalMap<String, String, SimpleMatrix>();\n+    TwoDimensionalMap<String, String, SimpleMatrix> averages = TwoDimensionalMap.treeMap();\n     for (Pair<String, String> binary : getBinaryMatrixNames(maps)) {\n       int count = 0;\n       SimpleMatrix matrix = null;\n@@ -69,7 +69,7 @@\n   }\n \n   public static Map<String, SimpleMatrix> averageUnaryMatrices(List<Map<String, SimpleMatrix>> maps) {\n-    Map<String, SimpleMatrix> averages = Generics.newHashMap();\n+    Map<String, SimpleMatrix> averages = Generics.newTreeMap();\n     for (String name : getUnaryMatrixNames(maps)) {\n       int count = 0;\n       SimpleMatrix matrix = null;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 72, "bugNodeStartChar": 2742, "bugNodeLength": 21, "fixLineNum": 72, "fixNodeStartChar": 2742, "fixNodeLength": 21, "sourceBeforeFix": "Generics.newHashMap()", "sourceAfterFix": "Generics.newTreeMap()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "45bf206c4d10874716feb5ef11f55a041bd22111", "fixCommitParentSHA1": "6bbd0ececa1678c87872c709297793a84328ca69", "bugFilePath": "src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java b/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java\nindex 385f665..e02b0a0 100644\n--- a/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java\n+++ b/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java\n@@ -46,7 +46,7 @@\n   }\n \n   public static TwoDimensionalMap<String, String, SimpleMatrix> averageBinaryMatrices(List<TwoDimensionalMap<String, String, SimpleMatrix>> maps) {\n-    TwoDimensionalMap<String, String, SimpleMatrix> averages = new TwoDimensionalMap<String, String, SimpleMatrix>();\n+    TwoDimensionalMap<String, String, SimpleMatrix> averages = TwoDimensionalMap.treeMap();\n     for (Pair<String, String> binary : getBinaryMatrixNames(maps)) {\n       int count = 0;\n       SimpleMatrix matrix = null;\n@@ -69,7 +69,7 @@\n   }\n \n   public static Map<String, SimpleMatrix> averageUnaryMatrices(List<Map<String, SimpleMatrix>> maps) {\n-    Map<String, SimpleMatrix> averages = Generics.newHashMap();\n+    Map<String, SimpleMatrix> averages = Generics.newTreeMap();\n     for (String name : getUnaryMatrixNames(maps)) {\n       int count = 0;\n       SimpleMatrix matrix = null;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 72, "bugNodeStartChar": 2742, "bugNodeLength": 21, "fixLineNum": 72, "fixNodeStartChar": 2742, "fixNodeLength": 21, "sourceBeforeFix": "Generics.newHashMap()", "sourceAfterFix": "Generics.newTreeMap()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "54470b2994b692297a41ef87a54e04995ce8c2e2", "fixCommitParentSHA1": "c2356b1bbd92f49c1229ede916f00fec57903112", "bugFilePath": "src/edu/stanford/nlp/util/Execution.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/util/Execution.java b/src/edu/stanford/nlp/util/Execution.java\nindex d2d2eb3..2a134c0 100644\n--- a/src/edu/stanford/nlp/util/Execution.java\n+++ b/src/edu/stanford/nlp/util/Execution.java\n@@ -597,7 +597,7 @@\n       log(FORCE, t);\n       exitCode = 1;\n     }\n-    endTrack(\"main\"); //ends main\n+    endTracksTo(\"main\");  // end main\n     if (exit) {\n       System.exit(exitCode);\n     }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 600, "bugNodeStartChar": 20370, "bugNodeLength": 16, "fixLineNum": 600, "fixNodeStartChar": 20370, "fixNodeLength": 19, "sourceBeforeFix": "endTrack(\"main\")", "sourceAfterFix": "endTracksTo(\"main\")"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "54470b2994b692297a41ef87a54e04995ce8c2e2", "fixCommitParentSHA1": "c2356b1bbd92f49c1229ede916f00fec57903112", "bugFilePath": "src/edu/stanford/nlp/util/Execution.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/util/Execution.java b/src/edu/stanford/nlp/util/Execution.java\nindex d2d2eb3..2a134c0 100644\n--- a/src/edu/stanford/nlp/util/Execution.java\n+++ b/src/edu/stanford/nlp/util/Execution.java\n@@ -597,7 +597,7 @@\n       log(FORCE, t);\n       exitCode = 1;\n     }\n-    endTrack(\"main\"); //ends main\n+    endTracksTo(\"main\");  // end main\n     if (exit) {\n       System.exit(exitCode);\n     }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 600, "bugNodeStartChar": 20370, "bugNodeLength": 16, "fixLineNum": 600, "fixNodeStartChar": 20370, "fixNodeLength": 19, "sourceBeforeFix": "endTrack(\"main\")", "sourceAfterFix": "endTracksTo(\"main\")"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "cb740fb45277664eb13dd1e4cae4c98485f88174", "fixCommitParentSHA1": "0fd04fda9ee52cc3950bdd17cb76ae60fb6165fc", "bugFilePath": "src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java b/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\nindex b9e6eb6..2f0f37d 100644\n--- a/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\n+++ b/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\n@@ -232,7 +232,7 @@\n     SimpleMatrix fullVector = RNNUtils.concatenate(leftVector, rightVector);\n     for (int slice = 0; slice < size; ++slice) {\n       SimpleMatrix scaledFullVector = fullVector.scale(deltaFull.get(slice));\n-      deltaTensor = deltaTensor.plus(Wt.getSlice(slice).mult(Wt.getSlice(slice).transpose()).mult(scaledFullVector));\n+      deltaTensor = deltaTensor.plus(Wt.getSlice(slice).plus(Wt.getSlice(slice).transpose()).mult(scaledFullVector));\n     }\n     return deltaTensor.plus(WTDeltaNoBias);\n   }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 235, "bugNodeStartChar": 11518, "bugNodeLength": 55, "fixLineNum": 235, "fixNodeStartChar": 11518, "fixNodeLength": 55, "sourceBeforeFix": "Wt.getSlice(slice).mult(Wt.getSlice(slice).transpose())", "sourceAfterFix": "Wt.getSlice(slice).plus(Wt.getSlice(slice).transpose())"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "cb740fb45277664eb13dd1e4cae4c98485f88174", "fixCommitParentSHA1": "0fd04fda9ee52cc3950bdd17cb76ae60fb6165fc", "bugFilePath": "src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java b/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\nindex b9e6eb6..2f0f37d 100644\n--- a/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\n+++ b/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\n@@ -232,7 +232,7 @@\n     SimpleMatrix fullVector = RNNUtils.concatenate(leftVector, rightVector);\n     for (int slice = 0; slice < size; ++slice) {\n       SimpleMatrix scaledFullVector = fullVector.scale(deltaFull.get(slice));\n-      deltaTensor = deltaTensor.plus(Wt.getSlice(slice).mult(Wt.getSlice(slice).transpose()).mult(scaledFullVector));\n+      deltaTensor = deltaTensor.plus(Wt.getSlice(slice).plus(Wt.getSlice(slice).transpose()).mult(scaledFullVector));\n     }\n     return deltaTensor.plus(WTDeltaNoBias);\n   }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 235, "bugNodeStartChar": 11518, "bugNodeLength": 55, "fixLineNum": 235, "fixNodeStartChar": 11518, "fixNodeLength": 55, "sourceBeforeFix": "Wt.getSlice(slice).mult(Wt.getSlice(slice).transpose())", "sourceAfterFix": "Wt.getSlice(slice).plus(Wt.getSlice(slice).transpose())"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "0c2a092d9e9116507ccd2dd8e66fd436e0fb025f", "fixCommitParentSHA1": "368b2ddb2ef055aa61952705fe967e8aae12f3a2", "bugFilePath": "src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java b/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\nindex ca4bed3..ed97ea3 100644\n--- a/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\n+++ b/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\n@@ -27,7 +27,7 @@\n \n   @Override\n   public Annotation createFromFile(File file) throws IOException {\n-    return createFromFile(file.getAbsoluteFile());\n+    return createFromFile(file.getAbsolutePath());\n   }\n \n   @Override\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 30, "bugNodeStartChar": 899, "bugNodeLength": 22, "fixLineNum": 30, "fixNodeStartChar": 899, "fixNodeLength": 22, "sourceBeforeFix": "file.getAbsoluteFile()", "sourceAfterFix": "file.getAbsolutePath()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "0c2a092d9e9116507ccd2dd8e66fd436e0fb025f", "fixCommitParentSHA1": "368b2ddb2ef055aa61952705fe967e8aae12f3a2", "bugFilePath": "src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java b/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\nindex ca4bed3..ed97ea3 100644\n--- a/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\n+++ b/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\n@@ -27,7 +27,7 @@\n \n   @Override\n   public Annotation createFromFile(File file) throws IOException {\n-    return createFromFile(file.getAbsoluteFile());\n+    return createFromFile(file.getAbsolutePath());\n   }\n \n   @Override\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 30, "bugNodeStartChar": 899, "bugNodeLength": 22, "fixLineNum": 30, "fixNodeStartChar": 899, "fixNodeLength": 22, "sourceBeforeFix": "file.getAbsoluteFile()", "sourceAfterFix": "file.getAbsolutePath()"}, {"bugType": "CHANGE_OPERATOR", "fixCommitSHA1": "db5a771e90739dd0e121f45aff09f383a225113b", "fixCommitParentSHA1": "4d0a69b59e8a20deb91bddaaeb780822494ff8c0", "bugFilePath": "src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java b/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\nindex 7df4f7d..16c9a06 100644\n--- a/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\n@@ -247,7 +247,7 @@\n       }\n     }\n     boolean overwriteOriginalNer = false;\n-    if (prevNerEndIndex != (start-1) || nextNerStartIndex != end) {\n+    if (prevNerEndIndex != (start-1) && nextNerStartIndex != end) {\n       // Cutting across already recognized NEs don't disturb\n     } else if (startNer == null) {\n       // No old ner, okay to replace\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 250, "bugNodeStartChar": 9757, "bugNodeLength": 56, "fixLineNum": 250, "fixNodeStartChar": 9757, "fixNodeLength": 56, "sourceBeforeFix": "prevNerEndIndex != (start - 1) || nextNerStartIndex != end", "sourceAfterFix": "prevNerEndIndex != (start - 1) && nextNerStartIndex != end"}, {"bugType": "CHANGE_OPERATOR", "fixCommitSHA1": "c882d23367e3d5dbec8574b8fef5d7be176e76aa", "fixCommitParentSHA1": "cc20278a29d3d034b158a759e243d3735e851417", "bugFilePath": "src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java b/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\nindex 16c9a06..7df4f7d 100644\n--- a/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\n@@ -247,7 +247,7 @@\n       }\n     }\n     boolean overwriteOriginalNer = false;\n-    if (prevNerEndIndex != (start-1) && nextNerStartIndex != end) {\n+    if (prevNerEndIndex != (start-1) || nextNerStartIndex != end) {\n       // Cutting across already recognized NEs don't disturb\n     } else if (startNer == null) {\n       // No old ner, okay to replace\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 250, "bugNodeStartChar": 9757, "bugNodeLength": 56, "fixLineNum": 250, "fixNodeStartChar": 9757, "fixNodeLength": 56, "sourceBeforeFix": "prevNerEndIndex != (start - 1) && nextNerStartIndex != end", "sourceAfterFix": "prevNerEndIndex != (start - 1) || nextNerStartIndex != end"}, {"bugType": "CHANGE_OPERAND", "fixCommitSHA1": "55c5f8284269d599b48d8b77bf5b7f45f449b185", "fixCommitParentSHA1": "337c8a9b65e5ed70dec4010a75fa5a77501e5ccc", "bugFilePath": "src/edu/stanford/nlp/tagger/io/TSVTaggedFileReader.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/tagger/io/TSVTaggedFileReader.java b/src/edu/stanford/nlp/tagger/io/TSVTaggedFileReader.java\nindex 961a0a6..5c68fcc 100644\n--- a/src/edu/stanford/nlp/tagger/io/TSVTaggedFileReader.java\n+++ b/src/edu/stanford/nlp/tagger/io/TSVTaggedFileReader.java\n@@ -73,7 +73,7 @@\n     next = new ArrayList<TaggedWord>();\r\n     while (line != null && !line.trim().equals(\"\")) {\r\n       String[] pieces = line.split(\"\\t\");\r\n-      if (pieces.length <= wordColumn || pieces.length <= wordColumn) {\r\n+      if (pieces.length <= wordColumn || pieces.length <= tagColumn) {\r\n         throw new IllegalArgumentException(\"File \" + filename + \" line #\" + \r\n                                            linesRead + \" too short\");\r\n       }\r\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 76, "bugNodeStartChar": 2348, "bugNodeLength": 27, "fixLineNum": 76, "fixNodeStartChar": 2273, "fixNodeLength": 26, "sourceBeforeFix": "pieces.length <= wordColumn", "sourceAfterFix": "pieces.length <= tagColumn"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "55c5f8284269d599b48d8b77bf5b7f45f449b185", "fixCommitParentSHA1": "337c8a9b65e5ed70dec4010a75fa5a77501e5ccc", "bugFilePath": "src/edu/stanford/nlp/tagger/io/TSVTaggedFileReader.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/tagger/io/TSVTaggedFileReader.java b/src/edu/stanford/nlp/tagger/io/TSVTaggedFileReader.java\nindex 961a0a6..5c68fcc 100644\n--- a/src/edu/stanford/nlp/tagger/io/TSVTaggedFileReader.java\n+++ b/src/edu/stanford/nlp/tagger/io/TSVTaggedFileReader.java\n@@ -73,7 +73,7 @@\n     next = new ArrayList<TaggedWord>();\r\n     while (line != null && !line.trim().equals(\"\")) {\r\n       String[] pieces = line.split(\"\\t\");\r\n-      if (pieces.length <= wordColumn || pieces.length <= wordColumn) {\r\n+      if (pieces.length <= wordColumn || pieces.length <= tagColumn) {\r\n         throw new IllegalArgumentException(\"File \" + filename + \" line #\" + \r\n                                            linesRead + \" too short\");\r\n       }\r\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 76, "bugNodeStartChar": 2348, "bugNodeLength": 27, "fixLineNum": 76, "fixNodeStartChar": 2273, "fixNodeLength": 26, "sourceBeforeFix": "pieces.length <= wordColumn", "sourceAfterFix": "pieces.length <= tagColumn"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "a72db2b67eacd746950b2880da987bf5f18624ab", "fixCommitParentSHA1": "c623959bea62ff15dbd8f46eef7543f03df4967a", "bugFilePath": "src/edu/stanford/nlp/sentiment/BuildBinarizedDataset.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/sentiment/BuildBinarizedDataset.java b/src/edu/stanford/nlp/sentiment/BuildBinarizedDataset.java\nindex 8d7928b..31690d9 100644\n--- a/src/edu/stanford/nlp/sentiment/BuildBinarizedDataset.java\n+++ b/src/edu/stanford/nlp/sentiment/BuildBinarizedDataset.java\n@@ -201,7 +201,7 @@\n         scorer.forwardPropagateTree(collapsedUnary);\n         setPredictedLabels(collapsedUnary);\n       } else {\n-        setUnknownLabels(binarized, mainLabel);\n+        setUnknownLabels(collapsedUnary, mainLabel);\n       }\n \n       Trees.convertToCoreLabels(collapsedUnary);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 204, "bugNodeStartChar": 7586, "bugNodeLength": 38, "fixLineNum": 204, "fixNodeStartChar": 7586, "fixNodeLength": 43, "sourceBeforeFix": "setUnknownLabels(binarized,mainLabel)", "sourceAfterFix": "setUnknownLabels(collapsedUnary,mainLabel)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "f457f86528e2f9ba92efa677f0e310a294783f61", "fixCommitParentSHA1": "6f311c6326640d95b0951e0bde385fabb4a170fe", "bugFilePath": "src/edu/stanford/nlp/parser/shiftreduce/Weight.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/shiftreduce/Weight.java b/src/edu/stanford/nlp/parser/shiftreduce/Weight.java\nindex 8475095..bf925c0 100644\n--- a/src/edu/stanford/nlp/parser/shiftreduce/Weight.java\n+++ b/src/edu/stanford/nlp/parser/shiftreduce/Weight.java\n@@ -109,7 +109,7 @@\n       }\n       int index = unpackIndex(i);\n       float score = unpackScore(i);\n-      packed[j] = pack(index, score);\n+      newPacked[j] = pack(index, score);\n       ++j;\n     }\n     packed = newPacked;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 112, "bugNodeStartChar": 2933, "bugNodeLength": 9, "fixLineNum": 112, "fixNodeStartChar": 2933, "fixNodeLength": 12, "sourceBeforeFix": "packed[j]", "sourceAfterFix": "newPacked[j]"}, {"bugType": "CHANGE_CALLER_IN_FUNCTION_CALL", "fixCommitSHA1": "5be7bae54f193a7a9c6adf15e323af1e187a7592", "fixCommitParentSHA1": "9191877f3b292f844374ca02134faa199cf89306", "bugFilePath": "src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java b/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java\nindex 6aee143..9c80680 100644\n--- a/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java\n+++ b/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java\n@@ -75,7 +75,7 @@\n           Transition transition = parser.transitionIndex.get(predictedTransition.object());\n           State newState = transition.apply(state, predictedTransition.score());\n           // System.err.println(\"  Transition: \" + transition + \" (\" + predictedTransition.score() + \")\");\n-          if (bestState == null || newState.score() < bestState.score()) {\n+          if (bestState == null || bestState.score() < newState.score()) {\n             bestState = newState;\n           }\n           beam.add(newState);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 78, "bugNodeStartChar": 2935, "bugNodeLength": 16, "fixLineNum": 78, "fixNodeStartChar": 2935, "fixNodeLength": 17, "sourceBeforeFix": "newState.score()", "sourceAfterFix": "bestState.score()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "5be7bae54f193a7a9c6adf15e323af1e187a7592", "fixCommitParentSHA1": "9191877f3b292f844374ca02134faa199cf89306", "bugFilePath": "src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java b/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java\nindex 6aee143..9c80680 100644\n--- a/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java\n+++ b/src/edu/stanford/nlp/parser/shiftreduce/ShiftReduceParserQuery.java\n@@ -75,7 +75,7 @@\n           Transition transition = parser.transitionIndex.get(predictedTransition.object());\n           State newState = transition.apply(state, predictedTransition.score());\n           // System.err.println(\"  Transition: \" + transition + \" (\" + predictedTransition.score() + \")\");\n-          if (bestState == null || newState.score() < bestState.score()) {\n+          if (bestState == null || bestState.score() < newState.score()) {\n             bestState = newState;\n           }\n           beam.add(newState);\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 78, "bugNodeStartChar": 2935, "bugNodeLength": 16, "fixLineNum": 78, "fixNodeStartChar": 2935, "fixNodeLength": 17, "sourceBeforeFix": "newState.score()", "sourceAfterFix": "bestState.score()"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "b85b4c49d7ff48478a0ab8d4480e7ec4315190f8", "fixCommitParentSHA1": "aab8c498e0c10a41b8126ae314af77ab8e95cbe7", "bugFilePath": "src/edu/stanford/nlp/patterns/surface/GetPatternsFromDataMultiClass.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/patterns/surface/GetPatternsFromDataMultiClass.java b/src/edu/stanford/nlp/patterns/surface/GetPatternsFromDataMultiClass.java\nindex e3bd576..44a45d1 100644\n--- a/src/edu/stanford/nlp/patterns/surface/GetPatternsFromDataMultiClass.java\n+++ b/src/edu/stanford/nlp/patterns/surface/GetPatternsFromDataMultiClass.java\n@@ -997,7 +997,7 @@\n             ConstantsAndVariables.class, PatternScoring.class, String.class,\n             TwoDimensionalCounter.class, TwoDimensionalCounter.class,\n             TwoDimensionalCounter.class, TwoDimensionalCounter.class,\n-            TwoDimensionalCounter.class);\n+            TwoDimensionalCounter.class, String.class);\n         \n         scorePatterns = ctor.newInstance(new Object[] { constVars,\n             constVars.patternScoring, label, patternsandWords4Label,\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 996, "bugNodeStartChar": 41398, "bugNodeLength": 279, "fixLineNum": 996, "fixNodeStartChar": 41398, "fixNodeLength": 293, "sourceBeforeFix": "clazz.getConstructor(ConstantsAndVariables.class,PatternScoring.class,String.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class)", "sourceAfterFix": "clazz.getConstructor(ConstantsAndVariables.class,PatternScoring.class,String.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,TwoDimensionalCounter.class,String.class)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "396216e7c45eb13cb0165b9abfa75cf8ebbe97f3", "fixCommitParentSHA1": "08b7fb0ba89fd8967be09a690f9572cee401eb08", "bugFilePath": "src/edu/stanford/nlp/math/ArrayMath.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/math/ArrayMath.java b/src/edu/stanford/nlp/math/ArrayMath.java\nindex cf690dd..97989fa 100644\n--- a/src/edu/stanford/nlp/math/ArrayMath.java\n+++ b/src/edu/stanford/nlp/math/ArrayMath.java\n@@ -280,7 +280,7 @@\n \n   public static void pairwiseAddInPlace(double[] to, double[] from) {\n     if (to.length != from.length) {\n-      throw new RuntimeException();\n+      throw new RuntimeException(\"to length:\" + to.length + \" from length:\" + from.length);\n     }\n     for (int i = 0; i < to.length; i++) {\n       to[i] = to[i] + from[i];\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 283, "bugNodeStartChar": 6888, "bugNodeLength": 22, "fixLineNum": 283, "fixNodeStartChar": 6888, "fixNodeLength": 78, "sourceBeforeFix": "new RuntimeException()", "sourceAfterFix": "new RuntimeException(\"to length:\" + to.length + \" from length:\"+ from.length)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "fd37bb87cc8aa1cf713c2f4ddfd2ec95be096d70", "fixCommitParentSHA1": "8ae415826ddf76a1d6044e90e0acb94e9e06a9eb", "bugFilePath": "src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java b/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java\nindex b3ea82f..0769e8c 100644\n--- a/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java\n+++ b/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java\n@@ -101,7 +101,7 @@\n    * TODO: pass this in rather than create it here if we wind up using\n    * this in more place.  Right now it's only used in testOnTreebank.\n    */\n-  protected Function<List<? extends HasWord>, ArrayList<TaggedWord>> tagger;\n+  protected Function<List<? extends HasWord>, List<TaggedWord>> tagger;\n \n   public EvaluateTreebank(LexicalizedParser parser) {\n     this(parser.getOp(), parser.lex, parser);\n@@ -122,7 +122,7 @@\n         Class[] argsClass = { String.class };\n         Object[] arguments = { op.testOptions.taggerSerializedFile };\n         System.err.printf(\"Loading tagger from serialized file %s ...\\n\",op.testOptions.taggerSerializedFile);\n-        tagger = (Function<List<? extends HasWord>,ArrayList<TaggedWord>>) Class.forName(\"edu.stanford.nlp.tagger.maxent.MaxentTagger\").getConstructor(argsClass).newInstance(arguments);\n+        tagger = (Function<List<? extends HasWord>,List<TaggedWord>>) Class.forName(\"edu.stanford.nlp.tagger.maxent.MaxentTagger\").getConstructor(argsClass).newInstance(arguments);\n       } catch (RuntimeException e) {\n         throw e;\n       } catch (Exception e) {\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 104, "bugNodeStartChar": 3551, "bugNodeLength": 21, "fixLineNum": 104, "fixNodeStartChar": 3551, "fixNodeLength": 16, "sourceBeforeFix": "ArrayList<TaggedWord>", "sourceAfterFix": "List<TaggedWord>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "fd37bb87cc8aa1cf713c2f4ddfd2ec95be096d70", "fixCommitParentSHA1": "8ae415826ddf76a1d6044e90e0acb94e9e06a9eb", "bugFilePath": "src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java b/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java\nindex b3ea82f..0769e8c 100644\n--- a/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java\n+++ b/src/edu/stanford/nlp/parser/lexparser/EvaluateTreebank.java\n@@ -101,7 +101,7 @@\n    * TODO: pass this in rather than create it here if we wind up using\n    * this in more place.  Right now it's only used in testOnTreebank.\n    */\n-  protected Function<List<? extends HasWord>, ArrayList<TaggedWord>> tagger;\n+  protected Function<List<? extends HasWord>, List<TaggedWord>> tagger;\n \n   public EvaluateTreebank(LexicalizedParser parser) {\n     this(parser.getOp(), parser.lex, parser);\n@@ -122,7 +122,7 @@\n         Class[] argsClass = { String.class };\n         Object[] arguments = { op.testOptions.taggerSerializedFile };\n         System.err.printf(\"Loading tagger from serialized file %s ...\\n\",op.testOptions.taggerSerializedFile);\n-        tagger = (Function<List<? extends HasWord>,ArrayList<TaggedWord>>) Class.forName(\"edu.stanford.nlp.tagger.maxent.MaxentTagger\").getConstructor(argsClass).newInstance(arguments);\n+        tagger = (Function<List<? extends HasWord>,List<TaggedWord>>) Class.forName(\"edu.stanford.nlp.tagger.maxent.MaxentTagger\").getConstructor(argsClass).newInstance(arguments);\n       } catch (RuntimeException e) {\n         throw e;\n       } catch (Exception e) {\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 125, "bugNodeStartChar": 4333, "bugNodeLength": 21, "fixLineNum": 125, "fixNodeStartChar": 4333, "fixNodeLength": 16, "sourceBeforeFix": "ArrayList<TaggedWord>", "sourceAfterFix": "List<TaggedWord>"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "68cdbf73818c9d76ccad723dd4e86651a7479954", "fixCommitParentSHA1": "de2350af32045c031c3852f5e0232b5e5b40f14f", "bugFilePath": "src/edu/stanford/nlp/graph/DirectedMultiGraph.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/graph/DirectedMultiGraph.java b/src/edu/stanford/nlp/graph/DirectedMultiGraph.java\nindex f2118c1..98f4c96 100644\n--- a/src/edu/stanford/nlp/graph/DirectedMultiGraph.java\n+++ b/src/edu/stanford/nlp/graph/DirectedMultiGraph.java\n@@ -138,7 +138,7 @@\n     }\n     boolean foundOut = outgoingEdges.containsKey(source) && outgoingEdges.get(source).containsKey(dest) &&\n         outgoingEdges.get(source).get(dest).remove(data);\n-    boolean foundIn = incomingEdges.containsKey(source) && incomingEdges.get(source).containsKey(dest) &&\n+    boolean foundIn = incomingEdges.containsKey(dest) && incomingEdges.get(dest).containsKey(source) &&\n         incomingEdges.get(dest).get(source).remove(data);\n     if (foundOut && !foundIn) {\n       throw new AssertionError(\"Edge found in outgoing but not incoming\");\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 141, "bugNodeStartChar": 4155, "bugNodeLength": 33, "fixLineNum": 141, "fixNodeStartChar": 4155, "fixNodeLength": 31, "sourceBeforeFix": "incomingEdges.containsKey(source)", "sourceAfterFix": "incomingEdges.containsKey(dest)"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "9b6abcd40b6083ddafb2e6ce2a730a52a6156b47", "fixCommitParentSHA1": "0db76633548402d60fec5eb9165c78b8723f56bf", "bugFilePath": "src/edu/stanford/nlp/trees/Tree.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/trees/Tree.java b/src/edu/stanford/nlp/trees/Tree.java\nindex d322e7a..280ec3f 100644\n--- a/src/edu/stanford/nlp/trees/Tree.java\n+++ b/src/edu/stanford/nlp/trees/Tree.java\n@@ -868,7 +868,7 @@\n       return;\n     }\n     pw.print(\"(\");\n-    String nodeString = onlyLabelValue ? nodeString() : nodeString();\n+    String nodeString = onlyLabelValue ? value() : nodeString();\n     pw.print(nodeString);\n     // pw.flush();\n     boolean parentIsNull = label() == null || label().value() == null;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 871, "bugNodeStartChar": 28918, "bugNodeLength": 12, "fixLineNum": 871, "fixNodeStartChar": 28918, "fixNodeLength": 7, "sourceBeforeFix": "nodeString()", "sourceAfterFix": "value()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "9b6abcd40b6083ddafb2e6ce2a730a52a6156b47", "fixCommitParentSHA1": "0db76633548402d60fec5eb9165c78b8723f56bf", "bugFilePath": "src/edu/stanford/nlp/trees/Tree.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/trees/Tree.java b/src/edu/stanford/nlp/trees/Tree.java\nindex d322e7a..280ec3f 100644\n--- a/src/edu/stanford/nlp/trees/Tree.java\n+++ b/src/edu/stanford/nlp/trees/Tree.java\n@@ -868,7 +868,7 @@\n       return;\n     }\n     pw.print(\"(\");\n-    String nodeString = onlyLabelValue ? nodeString() : nodeString();\n+    String nodeString = onlyLabelValue ? value() : nodeString();\n     pw.print(nodeString);\n     // pw.flush();\n     boolean parentIsNull = label() == null || label().value() == null;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 871, "bugNodeStartChar": 28918, "bugNodeLength": 12, "fixLineNum": 871, "fixNodeStartChar": 28918, "fixNodeLength": 7, "sourceBeforeFix": "nodeString()", "sourceAfterFix": "value()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "4a9d7457934350576e9e37a7f253066f4a5d3cce", "fixCommitParentSHA1": "a3bc9012c4e13cd6cd72f0e23648fc210e060cd2", "bugFilePath": "src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java b/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java\nindex 385f665..e02b0a0 100644\n--- a/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java\n+++ b/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java\n@@ -46,7 +46,7 @@\n   }\n \n   public static TwoDimensionalMap<String, String, SimpleMatrix> averageBinaryMatrices(List<TwoDimensionalMap<String, String, SimpleMatrix>> maps) {\n-    TwoDimensionalMap<String, String, SimpleMatrix> averages = new TwoDimensionalMap<String, String, SimpleMatrix>();\n+    TwoDimensionalMap<String, String, SimpleMatrix> averages = TwoDimensionalMap.treeMap();\n     for (Pair<String, String> binary : getBinaryMatrixNames(maps)) {\n       int count = 0;\n       SimpleMatrix matrix = null;\n@@ -69,7 +69,7 @@\n   }\n \n   public static Map<String, SimpleMatrix> averageUnaryMatrices(List<Map<String, SimpleMatrix>> maps) {\n-    Map<String, SimpleMatrix> averages = Generics.newHashMap();\n+    Map<String, SimpleMatrix> averages = Generics.newTreeMap();\n     for (String name : getUnaryMatrixNames(maps)) {\n       int count = 0;\n       SimpleMatrix matrix = null;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 72, "bugNodeStartChar": 2742, "bugNodeLength": 21, "fixLineNum": 72, "fixNodeStartChar": 2742, "fixNodeLength": 21, "sourceBeforeFix": "Generics.newHashMap()", "sourceAfterFix": "Generics.newTreeMap()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "4a9d7457934350576e9e37a7f253066f4a5d3cce", "fixCommitParentSHA1": "a3bc9012c4e13cd6cd72f0e23648fc210e060cd2", "bugFilePath": "src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java b/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java\nindex 385f665..e02b0a0 100644\n--- a/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java\n+++ b/src/edu/stanford/nlp/parser/dvparser/AverageDVModels.java\n@@ -46,7 +46,7 @@\n   }\n \n   public static TwoDimensionalMap<String, String, SimpleMatrix> averageBinaryMatrices(List<TwoDimensionalMap<String, String, SimpleMatrix>> maps) {\n-    TwoDimensionalMap<String, String, SimpleMatrix> averages = new TwoDimensionalMap<String, String, SimpleMatrix>();\n+    TwoDimensionalMap<String, String, SimpleMatrix> averages = TwoDimensionalMap.treeMap();\n     for (Pair<String, String> binary : getBinaryMatrixNames(maps)) {\n       int count = 0;\n       SimpleMatrix matrix = null;\n@@ -69,7 +69,7 @@\n   }\n \n   public static Map<String, SimpleMatrix> averageUnaryMatrices(List<Map<String, SimpleMatrix>> maps) {\n-    Map<String, SimpleMatrix> averages = Generics.newHashMap();\n+    Map<String, SimpleMatrix> averages = Generics.newTreeMap();\n     for (String name : getUnaryMatrixNames(maps)) {\n       int count = 0;\n       SimpleMatrix matrix = null;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 72, "bugNodeStartChar": 2742, "bugNodeLength": 21, "fixLineNum": 72, "fixNodeStartChar": 2742, "fixNodeLength": 21, "sourceBeforeFix": "Generics.newHashMap()", "sourceAfterFix": "Generics.newTreeMap()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "4aecd0d356c9990ebf8039b4b412fa08ca0c9353", "fixCommitParentSHA1": "b880e2d6d4b3b43fd235781c5758eb8fe4eae209", "bugFilePath": "src/edu/stanford/nlp/util/Execution.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/util/Execution.java b/src/edu/stanford/nlp/util/Execution.java\nindex d2d2eb3..2a134c0 100644\n--- a/src/edu/stanford/nlp/util/Execution.java\n+++ b/src/edu/stanford/nlp/util/Execution.java\n@@ -597,7 +597,7 @@\n       log(FORCE, t);\n       exitCode = 1;\n     }\n-    endTrack(\"main\"); //ends main\n+    endTracksTo(\"main\");  // end main\n     if (exit) {\n       System.exit(exitCode);\n     }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 600, "bugNodeStartChar": 20370, "bugNodeLength": 16, "fixLineNum": 600, "fixNodeStartChar": 20370, "fixNodeLength": 19, "sourceBeforeFix": "endTrack(\"main\")", "sourceAfterFix": "endTracksTo(\"main\")"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "4aecd0d356c9990ebf8039b4b412fa08ca0c9353", "fixCommitParentSHA1": "b880e2d6d4b3b43fd235781c5758eb8fe4eae209", "bugFilePath": "src/edu/stanford/nlp/util/Execution.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/util/Execution.java b/src/edu/stanford/nlp/util/Execution.java\nindex d2d2eb3..2a134c0 100644\n--- a/src/edu/stanford/nlp/util/Execution.java\n+++ b/src/edu/stanford/nlp/util/Execution.java\n@@ -597,7 +597,7 @@\n       log(FORCE, t);\n       exitCode = 1;\n     }\n-    endTrack(\"main\"); //ends main\n+    endTracksTo(\"main\");  // end main\n     if (exit) {\n       System.exit(exitCode);\n     }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 600, "bugNodeStartChar": 20370, "bugNodeLength": 16, "fixLineNum": 600, "fixNodeStartChar": 20370, "fixNodeLength": 19, "sourceBeforeFix": "endTrack(\"main\")", "sourceAfterFix": "endTracksTo(\"main\")"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "e39c5ae811f45bcf70833c0d855686864a86a42b", "fixCommitParentSHA1": "505aa6478049ffe6b2c9b6b16afcb9c6451e54de", "bugFilePath": "src/edu/stanford/nlp/util/Execution.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/util/Execution.java b/src/edu/stanford/nlp/util/Execution.java\nindex d2d2eb3..2a134c0 100644\n--- a/src/edu/stanford/nlp/util/Execution.java\n+++ b/src/edu/stanford/nlp/util/Execution.java\n@@ -597,7 +597,7 @@\n       log(FORCE, t);\n       exitCode = 1;\n     }\n-    endTrack(\"main\"); //ends main\n+    endTracksTo(\"main\");  // end main\n     if (exit) {\n       System.exit(exitCode);\n     }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 600, "bugNodeStartChar": 20370, "bugNodeLength": 16, "fixLineNum": 600, "fixNodeStartChar": 20370, "fixNodeLength": 19, "sourceBeforeFix": "endTrack(\"main\")", "sourceAfterFix": "endTracksTo(\"main\")"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "e39c5ae811f45bcf70833c0d855686864a86a42b", "fixCommitParentSHA1": "505aa6478049ffe6b2c9b6b16afcb9c6451e54de", "bugFilePath": "src/edu/stanford/nlp/util/Execution.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/util/Execution.java b/src/edu/stanford/nlp/util/Execution.java\nindex d2d2eb3..2a134c0 100644\n--- a/src/edu/stanford/nlp/util/Execution.java\n+++ b/src/edu/stanford/nlp/util/Execution.java\n@@ -597,7 +597,7 @@\n       log(FORCE, t);\n       exitCode = 1;\n     }\n-    endTrack(\"main\"); //ends main\n+    endTracksTo(\"main\");  // end main\n     if (exit) {\n       System.exit(exitCode);\n     }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 600, "bugNodeStartChar": 20370, "bugNodeLength": 16, "fixLineNum": 600, "fixNodeStartChar": 20370, "fixNodeLength": 19, "sourceBeforeFix": "endTrack(\"main\")", "sourceAfterFix": "endTracksTo(\"main\")"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "aa0714dac503e8245e19db415d50b9f17ebdeff9", "fixCommitParentSHA1": "0b57e784358d0f004ff3eff9a5367f483faf024d", "bugFilePath": "src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java b/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\nindex b9e6eb6..2f0f37d 100644\n--- a/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\n+++ b/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\n@@ -232,7 +232,7 @@\n     SimpleMatrix fullVector = RNNUtils.concatenate(leftVector, rightVector);\n     for (int slice = 0; slice < size; ++slice) {\n       SimpleMatrix scaledFullVector = fullVector.scale(deltaFull.get(slice));\n-      deltaTensor = deltaTensor.plus(Wt.getSlice(slice).mult(Wt.getSlice(slice).transpose()).mult(scaledFullVector));\n+      deltaTensor = deltaTensor.plus(Wt.getSlice(slice).plus(Wt.getSlice(slice).transpose()).mult(scaledFullVector));\n     }\n     return deltaTensor.plus(WTDeltaNoBias);\n   }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 235, "bugNodeStartChar": 11518, "bugNodeLength": 55, "fixLineNum": 235, "fixNodeStartChar": 11518, "fixNodeLength": 55, "sourceBeforeFix": "Wt.getSlice(slice).mult(Wt.getSlice(slice).transpose())", "sourceAfterFix": "Wt.getSlice(slice).plus(Wt.getSlice(slice).transpose())"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "aa0714dac503e8245e19db415d50b9f17ebdeff9", "fixCommitParentSHA1": "0b57e784358d0f004ff3eff9a5367f483faf024d", "bugFilePath": "src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java b/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\nindex b9e6eb6..2f0f37d 100644\n--- a/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\n+++ b/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\n@@ -232,7 +232,7 @@\n     SimpleMatrix fullVector = RNNUtils.concatenate(leftVector, rightVector);\n     for (int slice = 0; slice < size; ++slice) {\n       SimpleMatrix scaledFullVector = fullVector.scale(deltaFull.get(slice));\n-      deltaTensor = deltaTensor.plus(Wt.getSlice(slice).mult(Wt.getSlice(slice).transpose()).mult(scaledFullVector));\n+      deltaTensor = deltaTensor.plus(Wt.getSlice(slice).plus(Wt.getSlice(slice).transpose()).mult(scaledFullVector));\n     }\n     return deltaTensor.plus(WTDeltaNoBias);\n   }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 235, "bugNodeStartChar": 11518, "bugNodeLength": 55, "fixLineNum": 235, "fixNodeStartChar": 11518, "fixNodeLength": 55, "sourceBeforeFix": "Wt.getSlice(slice).mult(Wt.getSlice(slice).transpose())", "sourceAfterFix": "Wt.getSlice(slice).plus(Wt.getSlice(slice).transpose())"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "14ba691b78e51a8e9cbb97dfd39a79c2f3acfccc", "fixCommitParentSHA1": "6787c557aff48b810b88680f30aeaec51e31cecf", "bugFilePath": "src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java b/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\nindex b9e6eb6..2f0f37d 100644\n--- a/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\n+++ b/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\n@@ -232,7 +232,7 @@\n     SimpleMatrix fullVector = RNNUtils.concatenate(leftVector, rightVector);\n     for (int slice = 0; slice < size; ++slice) {\n       SimpleMatrix scaledFullVector = fullVector.scale(deltaFull.get(slice));\n-      deltaTensor = deltaTensor.plus(Wt.getSlice(slice).mult(Wt.getSlice(slice).transpose()).mult(scaledFullVector));\n+      deltaTensor = deltaTensor.plus(Wt.getSlice(slice).plus(Wt.getSlice(slice).transpose()).mult(scaledFullVector));\n     }\n     return deltaTensor.plus(WTDeltaNoBias);\n   }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 235, "bugNodeStartChar": 11518, "bugNodeLength": 55, "fixLineNum": 235, "fixNodeStartChar": 11518, "fixNodeLength": 55, "sourceBeforeFix": "Wt.getSlice(slice).mult(Wt.getSlice(slice).transpose())", "sourceAfterFix": "Wt.getSlice(slice).plus(Wt.getSlice(slice).transpose())"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "14ba691b78e51a8e9cbb97dfd39a79c2f3acfccc", "fixCommitParentSHA1": "6787c557aff48b810b88680f30aeaec51e31cecf", "bugFilePath": "src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java b/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\nindex b9e6eb6..2f0f37d 100644\n--- a/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\n+++ b/src/edu/stanford/nlp/sentiment/SentimentCostAndGradient.java\n@@ -232,7 +232,7 @@\n     SimpleMatrix fullVector = RNNUtils.concatenate(leftVector, rightVector);\n     for (int slice = 0; slice < size; ++slice) {\n       SimpleMatrix scaledFullVector = fullVector.scale(deltaFull.get(slice));\n-      deltaTensor = deltaTensor.plus(Wt.getSlice(slice).mult(Wt.getSlice(slice).transpose()).mult(scaledFullVector));\n+      deltaTensor = deltaTensor.plus(Wt.getSlice(slice).plus(Wt.getSlice(slice).transpose()).mult(scaledFullVector));\n     }\n     return deltaTensor.plus(WTDeltaNoBias);\n   }\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 235, "bugNodeStartChar": 11518, "bugNodeLength": 55, "fixLineNum": 235, "fixNodeStartChar": 11518, "fixNodeLength": 55, "sourceBeforeFix": "Wt.getSlice(slice).mult(Wt.getSlice(slice).transpose())", "sourceAfterFix": "Wt.getSlice(slice).plus(Wt.getSlice(slice).transpose())"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "8aa6656b6b06d0a111a2634af080ba77cc423140", "fixCommitParentSHA1": "5ea5ef58e2e390bfe9cca24f256de8143826d26e", "bugFilePath": "src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java b/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\nindex ca4bed3..ed97ea3 100644\n--- a/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\n+++ b/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\n@@ -27,7 +27,7 @@\n \n   @Override\n   public Annotation createFromFile(File file) throws IOException {\n-    return createFromFile(file.getAbsoluteFile());\n+    return createFromFile(file.getAbsolutePath());\n   }\n \n   @Override\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 30, "bugNodeStartChar": 899, "bugNodeLength": 22, "fixLineNum": 30, "fixNodeStartChar": 899, "fixNodeLength": 22, "sourceBeforeFix": "file.getAbsoluteFile()", "sourceAfterFix": "file.getAbsolutePath()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "8aa6656b6b06d0a111a2634af080ba77cc423140", "fixCommitParentSHA1": "5ea5ef58e2e390bfe9cca24f256de8143826d26e", "bugFilePath": "src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java b/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\nindex ca4bed3..ed97ea3 100644\n--- a/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\n+++ b/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\n@@ -27,7 +27,7 @@\n \n   @Override\n   public Annotation createFromFile(File file) throws IOException {\n-    return createFromFile(file.getAbsoluteFile());\n+    return createFromFile(file.getAbsolutePath());\n   }\n \n   @Override\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 30, "bugNodeStartChar": 899, "bugNodeLength": 22, "fixLineNum": 30, "fixNodeStartChar": 899, "fixNodeLength": 22, "sourceBeforeFix": "file.getAbsoluteFile()", "sourceAfterFix": "file.getAbsolutePath()"}, {"bugType": "DIFFERENT_METHOD_SAME_ARGS", "fixCommitSHA1": "efa2a798c755c014639a5d19e665ccacf1f032e0", "fixCommitParentSHA1": "3f5be79ffd8b5472f35442a34e6e0d4eac13c516", "bugFilePath": "src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java b/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\nindex ca4bed3..ed97ea3 100644\n--- a/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\n+++ b/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\n@@ -27,7 +27,7 @@\n \n   @Override\n   public Annotation createFromFile(File file) throws IOException {\n-    return createFromFile(file.getAbsoluteFile());\n+    return createFromFile(file.getAbsolutePath());\n   }\n \n   @Override\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 30, "bugNodeStartChar": 899, "bugNodeLength": 22, "fixLineNum": 30, "fixNodeStartChar": 899, "fixNodeLength": 22, "sourceBeforeFix": "file.getAbsoluteFile()", "sourceAfterFix": "file.getAbsolutePath()"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "efa2a798c755c014639a5d19e665ccacf1f032e0", "fixCommitParentSHA1": "3f5be79ffd8b5472f35442a34e6e0d4eac13c516", "bugFilePath": "src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java b/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\nindex ca4bed3..ed97ea3 100644\n--- a/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\n+++ b/src/edu/stanford/nlp/pipeline/AbstractInputStreamAnnotationCreator.java\n@@ -27,7 +27,7 @@\n \n   @Override\n   public Annotation createFromFile(File file) throws IOException {\n-    return createFromFile(file.getAbsoluteFile());\n+    return createFromFile(file.getAbsolutePath());\n   }\n \n   @Override\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 30, "bugNodeStartChar": 899, "bugNodeLength": 22, "fixLineNum": 30, "fixNodeStartChar": 899, "fixNodeLength": 22, "sourceBeforeFix": "file.getAbsoluteFile()", "sourceAfterFix": "file.getAbsolutePath()"}, {"bugType": "CHANGE_OPERATOR", "fixCommitSHA1": "f827d5c9909e01054e60374ee6016b569ca88825", "fixCommitParentSHA1": "bc5bd9d0a8e8afe7ca7882f93cb7899a8c1d533a", "bugFilePath": "src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java b/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\nindex 7df4f7d..16c9a06 100644\n--- a/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\n@@ -247,7 +247,7 @@\n       }\n     }\n     boolean overwriteOriginalNer = false;\n-    if (prevNerEndIndex != (start-1) || nextNerStartIndex != end) {\n+    if (prevNerEndIndex != (start-1) && nextNerStartIndex != end) {\n       // Cutting across already recognized NEs don't disturb\n     } else if (startNer == null) {\n       // No old ner, okay to replace\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 250, "bugNodeStartChar": 9757, "bugNodeLength": 56, "fixLineNum": 250, "fixNodeStartChar": 9757, "fixNodeLength": 56, "sourceBeforeFix": "prevNerEndIndex != (start - 1) || nextNerStartIndex != end", "sourceAfterFix": "prevNerEndIndex != (start - 1) && nextNerStartIndex != end"}, {"bugType": "CHANGE_OPERATOR", "fixCommitSHA1": "6692574f6a113ff93038c6abad9b2b1420499f82", "fixCommitParentSHA1": "30093005fb18a842fbfcd4f90e39d3f413a3c989", "bugFilePath": "src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java b/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\nindex 7df4f7d..16c9a06 100644\n--- a/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\n@@ -247,7 +247,7 @@\n       }\n     }\n     boolean overwriteOriginalNer = false;\n-    if (prevNerEndIndex != (start-1) || nextNerStartIndex != end) {\n+    if (prevNerEndIndex != (start-1) && nextNerStartIndex != end) {\n       // Cutting across already recognized NEs don't disturb\n     } else if (startNer == null) {\n       // No old ner, okay to replace\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 250, "bugNodeStartChar": 9757, "bugNodeLength": 56, "fixLineNum": 250, "fixNodeStartChar": 9757, "fixNodeLength": 56, "sourceBeforeFix": "prevNerEndIndex != (start - 1) || nextNerStartIndex != end", "sourceAfterFix": "prevNerEndIndex != (start - 1) && nextNerStartIndex != end"}, {"bugType": "CHANGE_OPERATOR", "fixCommitSHA1": "afb86b272431b41ee7360af0f40f504ba3aeebc9", "fixCommitParentSHA1": "2e8190f7df16faed538e4562e0afb7b223512fe8", "bugFilePath": "src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java b/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\nindex 16c9a06..7df4f7d 100644\n--- a/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\n@@ -247,7 +247,7 @@\n       }\n     }\n     boolean overwriteOriginalNer = false;\n-    if (prevNerEndIndex != (start-1) && nextNerStartIndex != end) {\n+    if (prevNerEndIndex != (start-1) || nextNerStartIndex != end) {\n       // Cutting across already recognized NEs don't disturb\n     } else if (startNer == null) {\n       // No old ner, okay to replace\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 250, "bugNodeStartChar": 9757, "bugNodeLength": 56, "fixLineNum": 250, "fixNodeStartChar": 9757, "fixNodeLength": 56, "sourceBeforeFix": "prevNerEndIndex != (start - 1) && nextNerStartIndex != end", "sourceAfterFix": "prevNerEndIndex != (start - 1) || nextNerStartIndex != end"}, {"bugType": "CHANGE_OPERATOR", "fixCommitSHA1": "ded560ca5d41f7ea8a99d917fa63d59323ed9766", "fixCommitParentSHA1": "31c32d77905ba0e9e17772a8306f47031f21eafd", "bugFilePath": "src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java b/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\nindex 16c9a06..7df4f7d 100644\n--- a/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\n+++ b/src/edu/stanford/nlp/pipeline/TokensRegexNERAnnotator.java\n@@ -247,7 +247,7 @@\n       }\n     }\n     boolean overwriteOriginalNer = false;\n-    if (prevNerEndIndex != (start-1) && nextNerStartIndex != end) {\n+    if (prevNerEndIndex != (start-1) || nextNerStartIndex != end) {\n       // Cutting across already recognized NEs don't disturb\n     } else if (startNer == null) {\n       // No old ner, okay to replace\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 250, "bugNodeStartChar": 9757, "bugNodeLength": 56, "fixLineNum": 250, "fixNodeStartChar": 9757, "fixNodeLength": 56, "sourceBeforeFix": "prevNerEndIndex != (start - 1) && nextNerStartIndex != end", "sourceAfterFix": "prevNerEndIndex != (start - 1) || nextNerStartIndex != end"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "fdb2c862f1bb03a84a9ded88d853379ba917f253", "fixCommitParentSHA1": "3641e8d707e006e6d25cba9ff7af5a0599447fbd", "bugFilePath": "src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java b/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\nindex 25e8792..3680c4e 100644\n--- a/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\n+++ b/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\n@@ -1528,7 +1528,7 @@\n \n       empiricalCountsForADoc(eHat4Update, ind);\n       // TODO(mengqiu) broken, does not do E calculation\n-      expectedCountsForADoc(weights, ind);\n+      expectedCountsForADoc(weights, e4Update, ind);\n \n       /* the commented out code below is to iterate over the batch docs instead of iterating over all\n          parameters at the end, which is more efficient; but it would also require us to clearUpdateEs()\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 1531, "bugNodeStartChar": 63179, "bugNodeLength": 35, "fixLineNum": 1531, "fixNodeStartChar": 63179, "fixNodeLength": 45, "sourceBeforeFix": "expectedCountsForADoc(weights,ind)", "sourceAfterFix": "expectedCountsForADoc(weights,e4Update,ind)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "fdb2c862f1bb03a84a9ded88d853379ba917f253", "fixCommitParentSHA1": "3641e8d707e006e6d25cba9ff7af5a0599447fbd", "bugFilePath": "src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java b/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\nindex 25e8792..3680c4e 100644\n--- a/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\n+++ b/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\n@@ -1528,7 +1528,7 @@\n \n       empiricalCountsForADoc(eHat4Update, ind);\n       // TODO(mengqiu) broken, does not do E calculation\n-      expectedCountsForADoc(weights, ind);\n+      expectedCountsForADoc(weights, e4Update, ind);\n \n       /* the commented out code below is to iterate over the batch docs instead of iterating over all\n          parameters at the end, which is more efficient; but it would also require us to clearUpdateEs()\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 1531, "bugNodeStartChar": 63179, "bugNodeLength": 35, "fixLineNum": 1531, "fixNodeStartChar": 63179, "fixNodeLength": 45, "sourceBeforeFix": "expectedCountsForADoc(weights,ind)", "sourceAfterFix": "expectedCountsForADoc(weights,e4Update,ind)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "a2f57d22e2e88c8b51ef24980d5ed2ac5df507e0", "fixCommitParentSHA1": "55ee6d86000363468e627480d6bafd881ad6043c", "bugFilePath": "src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java b/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\nindex 25e8792..3680c4e 100644\n--- a/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\n+++ b/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\n@@ -1528,7 +1528,7 @@\n \n       empiricalCountsForADoc(eHat4Update, ind);\n       // TODO(mengqiu) broken, does not do E calculation\n-      expectedCountsForADoc(weights, ind);\n+      expectedCountsForADoc(weights, e4Update, ind);\n \n       /* the commented out code below is to iterate over the batch docs instead of iterating over all\n          parameters at the end, which is more efficient; but it would also require us to clearUpdateEs()\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 1531, "bugNodeStartChar": 63179, "bugNodeLength": 35, "fixLineNum": 1531, "fixNodeStartChar": 63179, "fixNodeLength": 45, "sourceBeforeFix": "expectedCountsForADoc(weights,ind)", "sourceAfterFix": "expectedCountsForADoc(weights,e4Update,ind)"}, {"bugType": "CHANGE_IDENTIFIER", "fixCommitSHA1": "a2f57d22e2e88c8b51ef24980d5ed2ac5df507e0", "fixCommitParentSHA1": "55ee6d86000363468e627480d6bafd881ad6043c", "bugFilePath": "src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java b/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\nindex 25e8792..3680c4e 100644\n--- a/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\n+++ b/src/edu/stanford/nlp/ie/crf/CRFLogConditionalObjectiveFunction.java\n@@ -1528,7 +1528,7 @@\n \n       empiricalCountsForADoc(eHat4Update, ind);\n       // TODO(mengqiu) broken, does not do E calculation\n-      expectedCountsForADoc(weights, ind);\n+      expectedCountsForADoc(weights, e4Update, ind);\n \n       /* the commented out code below is to iterate over the batch docs instead of iterating over all\n          parameters at the end, which is more efficient; but it would also require us to clearUpdateEs()\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 1531, "bugNodeStartChar": 63179, "bugNodeLength": 35, "fixLineNum": 1531, "fixNodeStartChar": 63179, "fixNodeLength": 45, "sourceBeforeFix": "expectedCountsForADoc(weights,ind)", "sourceAfterFix": "expectedCountsForADoc(weights,e4Update,ind)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "9e5b20f7a09ea4fcc390cb4badcf7ffced5d96f9", "fixCommitParentSHA1": "833986dac5d4a1bd68f46cfc4d0c014a78fa6139", "bugFilePath": "src/edu/stanford/nlp/sequences/PlainTextDocumentReaderAndWriter.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/sequences/PlainTextDocumentReaderAndWriter.java b/src/edu/stanford/nlp/sequences/PlainTextDocumentReaderAndWriter.java\nindex 9a8ff17..74f8737 100644\n--- a/src/edu/stanford/nlp/sequences/PlainTextDocumentReaderAndWriter.java\n+++ b/src/edu/stanford/nlp/sequences/PlainTextDocumentReaderAndWriter.java\n@@ -75,7 +75,7 @@\n   }\n \n   private static final Pattern sgml = Pattern.compile(\"<[^>]*>\");\n-  private final WordToSentenceProcessor<IN> wts = new WordToSentenceProcessor<IN>();\n+  private final WordToSentenceProcessor<IN> wts = new WordToSentenceProcessor<IN>(WordToSentenceProcessor.NewlineIsSentenceBreak.ALWAYS);\n \n   private SeqClassifierFlags flags; // = null;\n   private TokenizerFactory<IN> tokenizerFactory;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 78, "bugNodeStartChar": 2674, "bugNodeLength": 33, "fixLineNum": 78, "fixNodeStartChar": 2674, "fixNodeLength": 86, "sourceBeforeFix": "new WordToSentenceProcessor<IN>()", "sourceAfterFix": "new WordToSentenceProcessor<IN>(WordToSentenceProcessor.NewlineIsSentenceBreak.ALWAYS)"}, {"bugType": "OVERLOAD_METHOD_MORE_ARGS", "fixCommitSHA1": "92f8bef51dfa0a81f9bfd88b9790cb04de0eb413", "fixCommitParentSHA1": "d4d0f4bd8227dfc5ffb48dc420ac90c193030467", "bugFilePath": "src/edu/stanford/nlp/sequences/PlainTextDocumentReaderAndWriter.java", "fixPatch": "diff --git a/src/edu/stanford/nlp/sequences/PlainTextDocumentReaderAndWriter.java b/src/edu/stanford/nlp/sequences/PlainTextDocumentReaderAndWriter.java\nindex 9a8ff17..74f8737 100644\n--- a/src/edu/stanford/nlp/sequences/PlainTextDocumentReaderAndWriter.java\n+++ b/src/edu/stanford/nlp/sequences/PlainTextDocumentReaderAndWriter.java\n@@ -75,7 +75,7 @@\n   }\n \n   private static final Pattern sgml = Pattern.compile(\"<[^>]*>\");\n-  private final WordToSentenceProcessor<IN> wts = new WordToSentenceProcessor<IN>();\n+  private final WordToSentenceProcessor<IN> wts = new WordToSentenceProcessor<IN>(WordToSentenceProcessor.NewlineIsSentenceBreak.ALWAYS);\n \n   private SeqClassifierFlags flags; // = null;\n   private TokenizerFactory<IN> tokenizerFactory;\n", "projectName": "stanfordnlp.CoreNLP", "bugLineNum": 78, "bugNodeStartChar": 2674, "bugNodeLength": 33, "fixLineNum": 78, "fixNodeStartChar": 2674, "fixNodeLength": 86, "sourceBeforeFix": "new WordToSentenceProcessor<IN>()", "sourceAfterFix": "new WordToSentenceProcessor<IN>(WordToSentenceProcessor.NewlineIsSentenceBreak.ALWAYS)"}]