[{"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "d08bf8738680178b111ce07d9b4593464125fe3a", "fixCommitParentSHA1": "16c0b7482d717dc4325ce0d81b74c7509bf0af13", "bugFilePath": "src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java", "fixPatch": "diff --git a/src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java b/src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java\nindex e2239be..46435f5 100644\n--- a/src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java\n+++ b/src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java\n@@ -302,15 +302,20 @@\n   }\n \n   /**\n-   * Determine whether links in the given page should be added to the queue for crawling.\n+   * Determine whether links found at the given URL should be added to the queue for crawling.\n    * By default this method returns true always, but classes that extend WebCrawler can\n    * override it in order to implement particular policies about which pages should be\n    * mined for outgoing links and which should not.\n    *\n-   * @param page the page currently being visited\n+   * If links from the URL are not being followed, then we are not operating as\n+   * a web crawler and need not check robots.txt before fetching the single URL.\n+   * (see definition at http://www.robotstxt.org/faq/what.html).  Thus URLs that\n+   * return false from this method will not be subject to robots.txt filtering.\n+   *\n+   * @param url the URL of the page under consideration\n    * @return true if outgoing links from this page should be added to the queue.\n    */\n-  protected boolean shouldFollowLinksInPage(Page page) {\n+  protected boolean shouldFollowLinksIn(WebURL url) {\n     return true;\n   }\n \n@@ -368,7 +373,7 @@\n             webURL.setDocid(-1);\n             webURL.setAnchor(curURL.getAnchor());\n             if (shouldVisit(page, webURL)) {\n-              if (robotstxtServer.allows(webURL)) {\n+              if (!shouldFollowLinksIn(webURL) || robotstxtServer.allows(webURL)) {\n                 webURL.setDocid(docIdServer.getNewDocID(movedToUrl));\n                 frontier.schedule(webURL);\n               } else {\n@@ -401,7 +406,7 @@\n \n         parser.parse(page, curURL.getURL());\n \n-        if (shouldFollowLinksInPage(page)) {\n+        if (shouldFollowLinksIn(page.getWebURL())) {\n           ParseData parseData = page.getParseData();\n           List<WebURL> toSchedule = new ArrayList<>();\n           int maxCrawlDepth = myController.getConfig().getMaxDepthOfCrawling();\n@@ -418,7 +423,7 @@\n               webURL.setDepth((short) (curURL.getDepth() + 1));\n               if ((maxCrawlDepth == -1) || (curURL.getDepth() < maxCrawlDepth)) {\n                 if (shouldVisit(page, webURL)) {\n-                  if (robotstxtServer.allows(webURL)) {\n+                  if (!shouldFollowLinksIn(webURL) || robotstxtServer.allows(webURL)) {\n                     webURL.setDocid(docIdServer.getNewDocID(webURL.getURL()));\n                     toSchedule.add(webURL);\n                   } else {\n", "projectName": "yasserg.crawler4j", "bugLineNum": 371, "bugNodeStartChar": 13690, "bugNodeLength": 30, "fixLineNum": 371, "fixNodeStartChar": 13690, "fixNodeLength": 62, "sourceBeforeFix": "robotstxtServer.allows(webURL)", "sourceAfterFix": "!shouldFollowLinksIn(webURL) || robotstxtServer.allows(webURL)"}, {"bugType": "LESS_SPECIFIC_IF", "fixCommitSHA1": "d08bf8738680178b111ce07d9b4593464125fe3a", "fixCommitParentSHA1": "16c0b7482d717dc4325ce0d81b74c7509bf0af13", "bugFilePath": "src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java", "fixPatch": "diff --git a/src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java b/src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java\nindex e2239be..46435f5 100644\n--- a/src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java\n+++ b/src/main/java/edu/uci/ics/crawler4j/crawler/WebCrawler.java\n@@ -302,15 +302,20 @@\n   }\n \n   /**\n-   * Determine whether links in the given page should be added to the queue for crawling.\n+   * Determine whether links found at the given URL should be added to the queue for crawling.\n    * By default this method returns true always, but classes that extend WebCrawler can\n    * override it in order to implement particular policies about which pages should be\n    * mined for outgoing links and which should not.\n    *\n-   * @param page the page currently being visited\n+   * If links from the URL are not being followed, then we are not operating as\n+   * a web crawler and need not check robots.txt before fetching the single URL.\n+   * (see definition at http://www.robotstxt.org/faq/what.html).  Thus URLs that\n+   * return false from this method will not be subject to robots.txt filtering.\n+   *\n+   * @param url the URL of the page under consideration\n    * @return true if outgoing links from this page should be added to the queue.\n    */\n-  protected boolean shouldFollowLinksInPage(Page page) {\n+  protected boolean shouldFollowLinksIn(WebURL url) {\n     return true;\n   }\n \n@@ -368,7 +373,7 @@\n             webURL.setDocid(-1);\n             webURL.setAnchor(curURL.getAnchor());\n             if (shouldVisit(page, webURL)) {\n-              if (robotstxtServer.allows(webURL)) {\n+              if (!shouldFollowLinksIn(webURL) || robotstxtServer.allows(webURL)) {\n                 webURL.setDocid(docIdServer.getNewDocID(movedToUrl));\n                 frontier.schedule(webURL);\n               } else {\n@@ -401,7 +406,7 @@\n \n         parser.parse(page, curURL.getURL());\n \n-        if (shouldFollowLinksInPage(page)) {\n+        if (shouldFollowLinksIn(page.getWebURL())) {\n           ParseData parseData = page.getParseData();\n           List<WebURL> toSchedule = new ArrayList<>();\n           int maxCrawlDepth = myController.getConfig().getMaxDepthOfCrawling();\n@@ -418,7 +423,7 @@\n               webURL.setDepth((short) (curURL.getDepth() + 1));\n               if ((maxCrawlDepth == -1) || (curURL.getDepth() < maxCrawlDepth)) {\n                 if (shouldVisit(page, webURL)) {\n-                  if (robotstxtServer.allows(webURL)) {\n+                  if (!shouldFollowLinksIn(webURL) || robotstxtServer.allows(webURL)) {\n                     webURL.setDocid(docIdServer.getNewDocID(webURL.getURL()));\n                     toSchedule.add(webURL);\n                   } else {\n", "projectName": "yasserg.crawler4j", "bugLineNum": 421, "bugNodeStartChar": 16198, "bugNodeLength": 30, "fixLineNum": 421, "fixNodeStartChar": 16198, "fixNodeLength": 62, "sourceBeforeFix": "robotstxtServer.allows(webURL)", "sourceAfterFix": "!shouldFollowLinksIn(webURL) || robotstxtServer.allows(webURL)"}]